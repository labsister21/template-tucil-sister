site reliability engineering site reliability engineering foreword google s story story scaling one great success stories computing industry marking shift towards itcentric business google one first companies define businessit alignment meant practice went inform concept devops wider community book written broad crosssection people made transition reality google grew time traditional role system administrator transformed questioned system administration say ca nt afford hold tradition authority think anew nt time wait everyone else catch introduction principles network system administration bur claimed system administration form humancomputer engineering strongly rejected reviewers said yet stage call engineering time felt field become lost trapped wizard culture could see way forward google drew line silicon forcing fate revised role called sre site reliability engineer friends among first new generation engineer formalized using software automation initially fiercely secretive happened inside outside google different google s experience unique time information methods flowed directions book shows willingness let sre thinking come shadows see google built legendary infrastructure also studied learned changed mind tools technologies along way face daunting challenges open spirit tribal nature culture often entrenches practitioners dogmatic positions hold industry back google overcame inertia book collection essays one company single common vision fact contributions aligned around single company s goal makes special common themes common characters software systems reappear several chapters see choices different perspectives know correlate resolve competing interests articles rigorous academic pieces personal accounts written pride variety personal styles perspective individual skill sets written bravely intellectual honesty refreshing uncommon industry literature claim never always others philosophical tentative reflecting variety personalities within culture plays role story turn read humility observers part journey information myriad conflicting challenges many questions real legacy volume nt x d done look back years come comparing ideas reasoning measure thoughts experiences impressive thing book existence today hear brazen culture show code culture ask questions grown around open source community rather expertise championed google company dared think problems first principles employ top talent high proportion phds tools components processes working alongside chains software people data nothing tells us solve problems universally point stories like far valuable code designs resulted implementations ephemeral documented reasoning priceless rarely access kind insight story one company fact many overlapping stories shows us scaling far photographic enlargement textbook computer architecture scaling business process rather machinery lesson alone worth weight electronic paper engage much selfcritical review world much reinvention repetition many years usenix lisa conference community discussing infrastructure plus conferences operating systems different today yet book still feels like rare offering detailed documentation google step watershed epoch tale copyingthough perhaps emulating inspire next step us unique intellectual honesty pages expressing leadership humility stories hopes fears successes failures salute courage authors editors allowing candor party handson experiences also benefit lessons learned inside cocoon mark burgess preface software engineering common children labor birth painful difficult labor birth actually spend effort yet software engineering discipline spends much time talking first period opposed second despite estimates total costs system incurred birth popular industry model conceives deployed operational software stabilized production therefore needing much less attention software engineers wrong lens see software engineering tends focus designing building software systems must another discipline focuses whole lifecycle software objects inception deployment operation refinement eventual peaceful decommissioning discipline usesand needs use a wide range skills separate concerns kinds engineers today answer discipline google calls site reliability engineering exactly site reliability engineering sre admit particularly clear name dopretty much every site reliability engineer google gets asked exactly actually regular basis unpacking term little first foremost sres engineers apply principles computer science engineering design development computing systems generally large distributed ones sometimes task writing software systems alongside product development counterparts sometimes task building additional pieces systems need like backups load balancing ideally reused across systems sometimes task figuring apply existing solutions new problems next focus system reliability ben treynor sloss google vp operations originator term sre claims reliability fundamental feature product system useful nobody use reliability critical sres focused finding ways improve design operation systems make scalable reliable efficient however expend effort direction point systems reliable enough instead invest efforts adding features building new products finally sres focused operating services built atop distributed computing systems whether services planetscale storage email hundreds millions users google began web search site name originally referred sre role keeping googlecom website running though run many services many websitesfrom internal infrastructure bigtable products external developers google cloud platform although represented sre broad discipline surprise arose fastmoving world web services perhaps origin owes something peculiarities infrastructure equally surprise postdeployment characteristics software could choose devote special attention reliability one regard primary domain web services process improving changing serverside software comparatively contained managing change tightly coupled failures kinds natural platform approach might emerge despite arising google web community generally think discipline lessons applicable communities organizations book attempt explain things organizations might make use learned better define role term means end organized book general principles specific practices separated possible appropriate discuss particular topic googlespecific information trust reader indulge us afraid draw useful conclusions environment also provided orienting materiala description google production environment mapping internal software publicly available softwarewhich help contextualize saying make directly usable ultimately course reliabilityoriented software systems engineering inherently good however acknowledge smaller organizations may wondering best use experience represented much like security earlier care reliability better implies even though small organization many pressing concerns software choices make may differ google made still worth putting lightweight reliability support place early less costly expand structure later introduce one present management contains number best practices training communication meetings found work well us many immediately usable organization sizes startup multinational probably already someone organization sre work without necessarily called name recognized another way get started path improving reliability organization formally recognize work find people foster doreward people stand cusp one way looking world another one like newton sometimes called world first physicist world last alchemist taking historical view looking back might first sre like think margaret hamilton working apollo program loan mit significant traits first sre words part culture learn everyone everything including one would least expect case point young daughter lauren came work one day team running mission scenarios hybrid simulation computer young children lauren went exploring caused mission crash selecting dsky keys unexpected way alerting team would happen prelaunch program p inadvertently selected real astronaut real mission real midcourse launching p inadvertently real mission would major problem wipes navigation data computer equipped pilot craft navigation data sre instincts margaret submitted program change request add special error checking code onboard flight software case astronaut accident happen select p flight move considered unnecessary higherups nasa course could never happen instead adding error checking code margaret updated mission specifications documentation say equivalent select p flight apparently update amusing many project told many times astronauts would make mistakesafter trained perfect well margaret suggested safeguard considered unnecessary next mission apollo days specifications update midcourse fourth day flight astronauts jim lovell william anders frank borman board jim lovell selected p mistakeas happens christmas daycreating much havoc involved critical problem absence workaround navigation data meant astronauts never coming home thankfully documentation update explicitly called possibility invaluable figuring upload usable data recover mission much time spare margaret says thorough understanding operate systems enough prevent human errors change request add error detection recovery software prelaunch program p approved shortly afterwards although apollo incident occurred decades ago much preceding paragraphs directly relevant engineers lives today much continue directly relevant future accordingly systems look groups work organizations building please bear sre way mind thoroughness dedication belief value preparation documentation awareness could go wrong coupled strong desire prevent welcome emerging profession read book book series essays written members alumni google site reliability engineering organization much like conference proceedings like standard book author small number authors chapter intended read part coherent whole good deal gained reading whatever subject particularly interests articles support inform text reference follow accordingly need read particular order though suggest least starting chapters production environment google viewpoint sre embracing risk describe google production environment outline sre approaches risk respectively risk many ways key quality profession reading covertocover course also useful possible chapters grouped thematically principles principles practices practices management management small introduction highlights individual pieces references articles published google sres covering specific topics detail additionally companion website book https gcosrebook number helpful resources hope least useful interesting putting together us editors conventions used book following typographical conventions used book italic indicates new terms urls email addresses filenames file extensions constant width used program listings well within paragraphs refer program elements variable function names databases data types environment variables statements keywords constant width bold shows commands text typed literally user constant width italic shows text replaced usersupplied values values determined context tip element signifies tip suggestion note element signifies general note warning element indicates warning caution using code examples supplemental material available https gcosrebook book help get job done general example code offered book may use programs documentation need contact us permission unless reproducing significant portion code example writing program uses several chunks code book require permission selling distributing cdrom examples reilly books require permission answering question citing book quoting example code require permission incorporating significant amount example code book product documentation require permission appreciate require attribution attribution usually includes title author publisher isbn example site reliability engineering edited betsy beyer chris jones jennifer petoff niall richard murphy reilly copyright google inc feel use code examples falls outside fair use permission given feel free contact us permissions oreillycom safari books online note safari books online ondemand digital library delivers expert content book video form world leading authors technology business technology professionals software developers web designers business creative professionals use safari books online primary resource research problem solving learning certification training safari books online offers range plans pricing enterprise government education individuals members access thousands books training videos prepublication manuscripts one fully searchable database publishers like reilly media prentice hall professional addisonwesley professional microsoft press sams que peachpit press focal press cisco press john wiley sons syngress morgan kaufmann ibm redbooks packt adobe press ft press apress manning new riders mcgrawhill jones bartlett course technology hundreds information safari books online please visit us online contact us please address comments questions concerning book publisher reilly media inc gravenstein highway north sebastopol ca united states canada international local fax web page book list errata examples additional information access page http bitlysitereliabilityengineering comment ask technical questions book send email bookquestions oreillycom information books courses conferences news see website http wwworeillycom find us facebook http facebookcomoreilly follow us twitter http twittercomoreillymedia watch us youtube http wwwyoutubecomoreillymedia acknowledgments book would possible without tireless efforts authors technical writers also like thank following internal reviewers providing especially valuable feedback alex matey dermot duffy jc van winkel john t reese michael reilly steve carstensen todd underwood ben lutch ben treynor sloss book sponsors within google belief project sharing learned running largescale services essential making book happen like send special thanks rik farrow editor login partnering us number contributions prepublication via usenix authors specifically acknowledged chapter like take time recognize contributed chapter providing thoughtful input discussion review embracing risk abe rahey ben treynor sloss brian stoler dave connor david besbris jill alvidrez mike curtis nancy chang tammy capistrant tom limoncelli eliminating toil cody smith george sadlier laurence berland marc alvidrez patrick stahlberg peter duff pim van pelt ryan anderson sabrina farmer seth hettich monitoring distributed systems mike curtis jamie wilkinson seth hettich release engineering david schnur jt goldstone marc alvidrez marcus larareinhold noah maxwell peter dinges sumitran raghunathan yutong cho simplicity ryan anderson practical alerting timeseries data jules anderson max luebbe mikel mcdaniel raul vera seth hettich oncall andrew stribblehill richard woodbury effective troubleshooting charles stephen gunn john hedditch peter nuttall rob ewaschuk sam greenfield emergency response jelena oertel kripa krishnan sergio salvi tim craig managing incidents amy zhou carla geisser grainne sheerin hildo biersma jelena oertel perry lorier rune kristian viken postmortem culture learning failure dan wu heather sherman jared brick mike louer štěpán davidovič tim craig tracking outages andrew stribblehill richard woodbury testing reliability isaac clerencia marc alvidrez software engineering sre ulric longyear load balancing frontend debashish chatterjee perry lorier chapters load balancing datacenter handling overload adam fletcher christoph pfisterer lukáš ježek manjot pahwa micha riser noah fiedel pavel herrmann paweł zuzelski perry lorier ralf wildenhues tudorioan salomie witold baryluk addressing cascading failures mike curtis ryan anderson managing critical state distributed consensus reliability ananth shrinivas mike burrows distributed periodic scheduling cron ben fried derek jackson gabe krabbe laura nolan seth hettich data processing pipelines abdulrahman salem alex perry arnar mar hrafnkelsson dieter pearcey dylan curley eivind eklund eric veach graham poulter ingvar mattsson john looney ken grant michelle duffy mike hochberg robinson data integrity read wrote corey vickrey dan ardelean disney luangsisongkham gordon prioreschi kristina bennett liang lin michael kelly sergey ivanyuk reliable product launches scale vivek rau accelerating sres oncall beyond melissa binde perry lorier preston yoshioka dealing interrupts ben lutch carla geisser dzevad trumic john turek matt brown embedding sre recover operational overload charles stephen gunn chris heiser max luebbe sam greenfield communication collaboration sre alex kehlenbeck jeromy carriere joel becker sowmya vijayaraghavan trevor mattsonhamilton evolving sre engagement model seth hettich lessons learned industries adrian hilton brad kratochvil charles ballowe dan sheridan eddie kennedy erik gross gus hartmann jackson stone jeff stevenson john li kevin greer matt toia michael haynie mike doherty peter dahl ron heiby also grateful following contributors either provided significant material excellent job reviewing agreed interviewed supplied significant expertise resources otherwise excellent effect work abe hassan adam rogoyski alex hidalgo amaya booker andrew fikes andrew hurst ariel goh ashleigh rentz ayman hourieh barclay osborn ben appleton ben love ben winslow bernhard beck bill duane bill patry blair zajac bob gruber brian gustafson bruce murphy buck clay cedric cellier chiho saito chris carlon christopher hahn chris kennelly chris taylor ciara kamahelesanfratello colin phipps colm buckley craig paterson daniel eisenbud daniel v klein daniel spoonhower dan watson dave phillips david hixson dina betser doron meyer dmitry fedoruk eric grosse eric schrock filip zyzniewski francis tang gary arneson georgina wilcox gretta bartels gustavo franco harald wagener healfdene goguen hugo santos hyrum wright ian gulliver jakub turski james chivers james kane james youngman jan monsch jason parkerburlingham jason petsod jeffry mcneil jeff dean jeff peck jennifer mace jerry cen jess frame john brady john gunderman john kochmar john tobin jordyn buchanan joseph bironas julio merino julius plenz kate ward kathy polizzi katrina sostek kenn hamm kirk russell kripa krishnan larry greenfield lea oliveira luca cittadini lucas pereira magnus ringman mahesh palekar marco paganini mario bonilla mathew mills mathew monroe matt d brown matt proud max saltonstall michal jaszczyk mihai bivol misha brukman olivier oansaldi patrick bernier pierre palatin rob shanley robert van gent rory ward rui zhangshen salim virji sanjay ghemawat sarah coty sean dorward sean quinlan sean sechrist shari trumbomchenry shawn morrissey shuntak leung stan jedrus stefano lattarini steven schirripa tanya reilly terry bolt tim chaplin toby weingartner tom black udi meiri victor terron vlad grama wes hertlein zoltan egyed much appreciate thoughtful indepth feedback received external reviewers andrew fong björn rabenstein charles border david blankedelman frossie economou james meickle josh ryder mark burgess russ allbery would like extend special thanks cian synnott original book team member coconspirator left google project completed deeply influential margaret hamilton graciously allowed us reference story preface additionally would like extend special thanks shylaja nukala generously gave time technical writers supported necessary valued efforts wholeheartedly editors would also like personally thank following people betsy beyer grandmother personal hero supplying endless amounts phone pep talks popcorn riba supplying sweatpants necessary fuel several late nights course addition cast sre allstars indeed delightful collaborators chris jones michelle saving life crime high seas uncanny ability find manzanas unexpected places taught engineering years jennifer petoff husband scott incredibly supportive two year process writing book keeping editors supplied plenty sugar dessert island niall murphy léan oisín fiachra considerably patient right expect substantially rantier father husband usual years dermot transfer offer the fact large variance estimates tells something software engineering discipline see eg gla details for purposes reliability probability system perform required function without failure stated conditions stated period time following definition oco the software systems concerned largely websites similar services discuss reliability concerns face software intended nuclear power plants aircraft medical equipment safetycritical systems however compare approaches used industries lessons learned industries in distinct industry term devops although definitely regard infrastructure code reliability main focus additionally strongly oriented toward removing necessity operationssee evolution automation google details in addition great story also substantial claim popularizing term software engineering part introduction section provides highlevel guidance sre different conventional industry practices ben treynor sloss senior vp overseeing technical operations google originator term site reliability engineering provides view sre means works compares ways things industry introduction provide guide production environment google production environment google viewpoint sre way help acquaint wealth new terms systems meet rest book chapter introduction written benjamin treynor sloss edited betsy beyer hope strategy traditional sre saying truth universally acknowledged systems run systemparticularly complex computing system operates large scalebe run sysadmin approach service management historically companies employed systems administrators run complex computing systems systems administrator sysadmin approach involves assembling existing software components deploying work together produce service sysadmins tasked running service responding events updates occur system grows complexity traffic volume generating corresponding increase events updates sysadmin team grows absorb additional work sysadmin role requires markedly different skill set required product developers developers sysadmins divided discrete teams development operations ops sysadmin model service management several advantages companies deciding run staff service approach relatively easy implement familiar industry paradigm many examples learn emulate relevant talent pool already widely available array existing tools software components shelf otherwise integration companies available help run assembled systems novice sysadmin team reinvent wheel design system scratch sysadmin approach accompanying developmentops split number disadvantages pitfalls fall broadly two categories direct costs indirect costs direct costs neither subtle ambiguous running service team relies manual intervention change management event handling becomes expensive service andor traffic service grows size team necessarily scales load generated system indirect costs developmentops split subtle often expensive organization direct costs costs arise fact two teams quite different background skill set incentives use different vocabulary describe situations carry different assumptions risk possibilities technical solutions different assumptions target level product stability split groups easily become one incentives also communication goals eventually trust respect outcome pathology traditional operations teams counterparts product development thus often end conflict visibly quickly software released production core development teams want launch new features see adopted users core ops teams want make sure service break holding pager outages caused kind changea new configuration new feature launch new type user trafficthe two teams goals fundamentally tension groups understand unacceptable state interests baldest possible terms want launch anything time without hindrance versus want ever change anything system works vocabulary risk assumptions differ groups often resort familiar form trench warfare advance interests ops team attempts safeguard running system risk change introducing launch change gates example launch reviews may contain explicit check every problem ever caused outage past could arbitrarily long list elements providing equal value dev team quickly learns respond fewer launches flag flips incremental updates cherrypicks adopt tactics sharding product fewer features subject launch review google approach service management site reliability engineering conflict inevitable part offering software service google chosen run systems different approach site reliability engineering teams focus hiring software engineers run products create systems accomplish work would otherwise performed often manually sysadmins exactly site reliability engineering come defined google explanation simple sre happens ask software engineer design operations team joined google tasked running production team seven engineers entire life point software engineering designed managed group way would want work worked sre group since matured become google presentday sre team remains true origins envisioned lifelong software engineer primary building block google approach service management composition sre team whole sres broken two main categories google software engineers precisely people hired via standard procedure google software engineers candidates close google software engineering qualifications ie skill set required addition set technical skills useful sre rare software engineers far unix system internals networking layer layer expertise two common types alternate technical skills seek common sres belief aptitude developing software systems solve complex problems within sre track career progress groups closely date found practical difference performance engineers two tracks fact somewhat diverse background sre team frequently results clever highquality systems clearly product synthesis several skill sets result approach hiring sre end team people quickly become bored performing tasks hand b skill set necessary write software replace previously manual work even solution complicated sres also end sharing academic intellectual background rest development organization therefore sre fundamentally work historically done operations team using engineers software expertise banking fact engineers inherently predisposed ability design implement automation software replace human labor design crucial sre teams focused engineering without constant engineering operations load increases teams need people keep pace workload eventually traditional opsfocused group scales linearly service size products supported service succeed operational load grow traffic means hiring people tasks avoid fate team tasked managing service needs code drown therefore google places cap aggregate ops work srestickets oncall manual tasks etc cap ensures sre team enough time schedule make service stable operable cap upper bound time left devices sre team end little operational load almost entirely engage development tasks service basically runs repairs want systems automatic automated practice scale new features keep sres toes google rule thumb sre team must spend remaining time actually development enforce threshold first place measure sre time spent measurement hand ensure teams consistently spending less time development work change practices often means shifting operations burden back development team adding staff team without assigning team additional operational responsibilities consciously maintaining balance ops development work allows us ensure sres bandwidth engage creative autonomous engineering still retaining wisdom gleaned operations side running service found google sre approach running largescale systems many advantages sres directly modifying code pursuit making google systems run sre teams characterized rapid innovation large acceptance change teams relatively inexpensivesupporting service opsoriented team would require significantly larger number people instead number sres needed run maintain improve system scales sublinearly size system finally sre circumvent dysfunctionality devops split structure also improves product development teams easy transfers product development sre teams crosstrain entire group improve skills developers otherwise may difficulty learning build millioncore distributed system despite net gains sre model characterized distinct set challenges one continual challenge google faces hiring sres sre compete candidates product development hiring pipeline fact set hiring bar high terms coding system engineering skills means hiring pool necessarily small discipline relatively new unique much industry information exists build manage sre team although hopefully book make strides direction sre team place potentially unorthodox approaches service management require strong management support example decision stop releases remainder quarter error budget depleted might embraced product development team unless mandated management devops sre term devops emerged industry late writing early still state flux core principlesinvolvement function phase system design development heavy reliance automation versus human effort application engineering practices tools operations tasksare consistent many sre principles practices one could view devops generalization several core sre principles wider range organizations management structures personnel one could equivalently view sre specific implementation devops idiosyncratic extensions tenets sre tenets sre nuances workflows priorities daytoday operations vary sre team sre team share set basic responsibilities service support adhere core tenets general sre team responsible availability latency performance efficiency change management monitoring emergency response capacity planning service codified rules engagement principles sre teams interact environmentnot production environment also product development teams testing teams users rules work practices help us maintain focus engineering work opposed operations work following section discusses core tenets google sre ensuring durable focus engineering already discussed google caps operational work sres time remaining time spent using coding skills project work practice accomplished monitoring amount operational work done sres redirecting excess operational work product development teams reassigning bugs tickets development managers integrating developers oncall pager rotations redirection ends operational load drops back lower also provides effective feedback mechanism guiding developers build systems need manual intervention approach works well entire organizationsre development alikeunderstands safety valve mechanism exists supports goal overflow events product generate enough operational load require focused operations work average sres receive maximum two events per hour oncall shift target volume gives oncall engineer enough time handle event accurately quickly clean restore normal service conduct postmortem two events occur regularly per oncall shift problems investigated thoroughly engineers sufficiently overwhelmed prevent learning events scenario pager fatigue also improve scale conversely oncall sres consistently receive fewer one event per shift keeping point waste time postmortems written significant incidents regardless whether paged postmortems trigger page even valuable likely point clear monitoring gaps investigation establish happened detail find root causes event assign actions correct problem improve addressed next time google operates blamefree postmortem culture goal exposing faults applying engineering fix faults rather avoiding minimizing pursuing maximum change velocity without violating service slo product development sre teams enjoy productive working relationship eliminating structural conflict respective goals structural conflict pace innovation product stability described earlier conflict often expressed indirectly sre bring conflict fore resolve introduction error budget error budget stems observation wrong reliability target basically everything pacemakers antilock brakes notable exceptions general software service system right reliability target user tell difference system available available many systems path user service laptop home wifi isp power grid systems collectively far less available thus marginal difference gets lost noise unavailability user receives benefit enormous effort required add last availability wrong reliability target system right reliability target system actually technical question all product question take following considerations account level availability users happy given use product alternatives available users dissatisfied product availability happens users usage product different availability levels business product must establish system availability target target established error budget one minus availability target service available unavailable permitted unavailability service error budget spend budget anything want long overspend want spend error budget development team wants launch features attract new users ideally would spend error budget taking risks things launch order launch quickly basic premise describes whole model error budgets soon sre activities conceptualized framework freeing error budget tactics phased rollouts experiments optimize quicker launches use error budget resolves structural conflict incentives development sre sre goal longer zero outages rather sres product developers aim spend error budget getting maximum feature velocity change makes difference outage longer bad thingit expected part process innovation occurrence development sre teams manage rather fear monitoring monitoring one primary means service owners keep track system health availability monitoring strategy constructed thoughtfully classic common approach monitoring watch specific value condition trigger email alert value exceeded condition occurs however type email alerting effective solution system requires human read email decide whether type action needs taken response fundamentally flawed monitoring never require human interpret part alerting domain instead software interpreting humans notified need take action three kinds valid monitoring output alerts signify human needs take action immediately response something either happening happen order improve situation tickets signify human needs take action immediately system automatically handle situation human takes action days damage result logging one needs look information recorded diagnostic forensic purposes expectation one reads logs unless something else prompts emergency response reliability function mean time failure mttf mean time repair mttr sch relevant metric evaluating effectiveness emergency response quickly response team bring system back healththat mttr humans add latency even given system experiences actual failures system avoid emergencies require human intervention higher availability system requires handson intervention humans necessary found thinking recording best practices ahead time playbook produces roughly x improvement mttr compared strategy winging hero jackofalltrades oncall engineer work practiced oncall engineer armed playbook works much better playbook matter comprehensive may substitute smart engineers able think fly clear thorough troubleshooting steps tips valuable responding highstakes timesensitive page thus google sre relies oncall playbooks addition exercises wheel misfortune prepare engineers react oncall events change management sre found roughly outages due changes live system best practices domain use automation accomplish following implementing progressive rollouts quickly accurately detecting problems rolling back changes safely problems arise trio practices effectively minimizes aggregate number users operations exposed bad changes removing humans loop practices avoid normal problems fatigue familiaritycontempt inattention highly repetitive tasks result release velocity safety increase demand forecasting capacity planning demand forecasting capacity planning viewed ensuring sufficient capacity redundancy serve projected future demand required availability nothing particularly special concepts except surprising number services teams take steps necessary ensure required capacity place time needed capacity planning take organic growth stems natural product adoption usage customers inorganic growth results events like feature launches marketing campaigns businessdriven changes account several steps mandatory capacity planning accurate organic demand forecast extends beyond lead time required acquiring capacity accurate incorporation inorganic demand sources demand forecast regular load testing system correlate raw capacity servers disks service capacity capacity critical availability naturally follows sre team must charge capacity planning means also must charge provisioning provisioning provisioning combines change management capacity planning experience provisioning must conducted quickly necessary capacity expensive exercise must also done correctly capacity work needed adding new capacity often involves spinning new instance location making significant modification existing systems configuration files load balancers networking validating new capacity performs delivers correct results thus riskier operation load shifting often done multiple times per hour must treated corresponding degree extra caution efficiency performance efficient use resources important time service cares money sre ultimately controls provisioning must also involved work utilization utilization function given service works provisioned follows paying close attention provisioning strategy service therefore utilization provides big lever service total costs resource use function demand load capacity software efficiency sres predict demand provision capacity modify software three factors large part though entirety service efficiency software systems become slower load added slowdown service equates loss capacity point slowing system stops serving corresponds infinite slowness sres provision meet capacity target specific response speed thus keenly interested service performance sres product developers monitor modify service improve performance thus adding capacity improving efficiency end beginning site reliability engineering represents significant break existing industry best practices managing large complicated services motivated originally familiarity software engineer would want invest time accomplish set repetitive tasks it become much set principles set practices set incentives field endeavor within larger software engineering discipline rest book explores sre way detail vice president google engineering founder google sre see disaster role playing for discussion collaboration work practice see communications production meetings chapter production environment google viewpoint sre written jc van winkeledited betsy beyer google datacenters different conventional datacenters smallscale server farms differences present extra problems opportunities chapter discusses challenges opportunities characterize google datacenters introduces terminology used throughout book hardware google compute resources googledesigned datacenters proprietary power distribution cooling networking compute hardware see bar unlike standard colocation datacenters compute hardware googledesigned datacenter across board eliminate confusion server hardware server software use following terminology throughout book machine piece hardware perhaps vm server piece software implements service machines run server dedicate specific machines specific server programs specific machine runs mail server example instead resource allocation handled cluster operating system borg realize use word server unusual common use word conflates binary accepts network connection machine differentiating two important talking computing google get used usage server becomes apparent makes sense use specialized terminology within google also rest book figure illustrates topology google datacenter tens machines placed rack racks stand row one rows form cluster usually datacenter building houses multiple clusters multiple datacenter buildings located close together form campus figure example google datacenter campus topology machines within given datacenter need able talk created fast virtual switch tens thousands ports accomplished connecting hundreds googlebuilt switches clos network fabric clos named jupiter sin largest configuration jupiter supports pbps bisection bandwidth among servers datacenters connected globespanning backbone network b jai b softwaredefined networking architecture uses openflow openstandard communications protocol supplies massive bandwidth modest number sites uses elastic bandwidth allocation maximize average bandwidth kum system software organizes hardware hardware must controlled administered software handle massive scale hardware failures one notable problem manage software given large number hardware components cluster hardware failures occur quite frequently single cluster typical year thousands machines fail thousands hard disks break multiplied number clusters operate globally numbers become somewhat breathtaking therefore want abstract problems away users teams running services similarly want bothered hardware failures datacenter campus teams dedicated maintaining hardware datacenter infrastructure managing machines borg illustrated figure distributed cluster operating system ver similar apache mesos borg manages jobs cluster level figure highlevel borg cluster architecture borg responsible running users jobs either indefinitely running servers batch processes like mapreduce dea jobs consist one sometimes thousands identical tasks reasons reliability single process usually handle cluster traffic borg starts job finds machines tasks tells machines start server program borg continually monitors tasks task malfunctions killed restarted possibly different machine tasks fluidly allocated machines simply rely ip addresses port numbers refer tasks solve problem extra level indirection starting job borg allocates name index number task using borg naming service bns rather using ip address port number processes connect borg tasks via bns name translated ip address port number bns example bns path might string bns cluster user job name task number would resolve ip address port borg also responsible allocation resources jobs every job needs specify required resources eg cpu cores gib ram using list requirements jobs borg binpack tasks machines optimal way also accounts failure domains example borg run job tasks rack means top rack switch single point failure job task tries use resources requested borg kills task restarts slowly crashlooping task usually preferable task restarted storage tasks use local disk machines scratch pad several cluster storage options permanent storage even scratch space eventually move cluster storage model comparable lustre hadoop distributed file system hdfs open source cluster filesystems storage layer responsible offering users easy reliable access storage available cluster shown figure storage many layers lowest layer called disk although uses spinning disks flash storage fileserver running almost machines cluster however users want access data want remember machine storing data next layer comes play layer top called colossus creates clusterwide filesystem offers usual filesystem semantics well replication encryption colossus successor gfs google file system ghe several databaselike services built top colossus bigtable cha nosql database system handle databases petabytes size bigtable sparse distributed persistent multidimensional sorted map indexed row key column key timestamp value map uninterpreted array bytes bigtable supports eventually consistent crossdatacenter replication spanner cor offers sqllike interface users require real consistency across world several database systems blobstore available options comes set tradeoffs see data integrity read wrote figure portions google storage stack networking google network hardware controlled several ways discussed earlier use openflowbased softwaredefined network instead using smart routing hardware rely less expensive dumb switching components combination central duplicated controller precomputes best paths across network therefore able move computeexpensive routing decisions away routers use simple switching hardware network bandwidth needs allocated wisely borg limits compute resources task use bandwidth enforcer bwe manages available bandwidth maximize average available bandwidth optimizing bandwidth cost centralized traffic engineering shown solve number problems traditionally extremely difficult solve combination distributed routing traffic engineering kum services jobs running multiple clusters distributed across world order minimize latency globally distributed services want direct users closest datacenter available capacity global software load balancer gslb performs load balancing three levels geographic load balancing dns requests example wwwgooglecom described load balancing frontend load balancing user service level example youtube google maps load balancing remote procedure call rpc level described load balancing datacenter service owners specify symbolic name service list bns addresses servers capacity available locations typically measured queries per second gslb directs traffic bns addresses system software several components datacenter also important lock service chubby bur lock service provides filesystemlike api maintaining locks chubby handles locks across datacenter locations uses paxos protocol asynchronous consensus see managing critical state distributed consensus reliability chubby also plays important role master election service five replicas job running reliability purposes one replica may perform actual work chubby used select replica may proceed data must consistent well suited storage chubby reason bns uses chubby store mapping bns paths ip address port pairs monitoring alerting want make sure services running required therefore run many instances borgmon monitoring program see practical alerting timeseries data borgmon regularly scrapes metrics monitored servers metrics used instantaneously alerting also stored use historic overviews eg graphs use monitoring several ways set alerting acute problems compare behavior software update make server faster examine resource consumption behavior evolves time essential capacity planning software infrastructure software architecture designed make efficient use hardware infrastructure code heavily multithreaded one task easily use many cores facilitate dashboards monitoring debugging every server http server provides diagnostics statistics given task google services communicate using remote procedure call rpc infrastructure named stubby open source version grpc available often rpc call made even call subroutine local program needs performed makes easier refactor call different server modularity needed server codebase grows gslb load balance rpcs way load balances externally visible services server receives rpc requests frontend sends rpcs backend traditional terms frontend called client backend called server data transferred rpc using protocol buffers often abbreviated protobufs similar apache thrift protocol buffers many advantages xml serializing structured data simpler use times smaller times faster less ambiguous development environment development velocity important google built complete development environment make use infrastructure morb apart groups open source repositories eg android chrome google software engineers work single shared repository pot important practical implications workflows engineers encounter problem component outside project fix problem send proposed changes changelist cl owner review submit cl mainline changes source code engineer project require review software reviewed submitted software built build request sent build servers datacenter even large builds executed quickly many build servers compile parallel infrastructure also used continuous testing time cl submitted tests run software may depend cl either directly indirectly framework determines change likely broke parts system notifies owner submitted change projects use pushongreen system new version automatically pushed production passing tests shakespeare sample service provide model service would hypothetically deployed google production environment let look example service interacts multiple google technologies suppose want offer service lets determine given word used throughout shakespeare works divide system two parts batch component reads shakespeare texts creates index writes index bigtable job need run perhaps infrequently never know new text might discovered application frontend handles enduser requests job always users time zones want search shakespeare books batch component mapreduce comprising three phases mapping phase reads shakespeare texts splits individual words faster performed parallel multiple workers shuffle phase sorts tuples word reduce phase tuple word list locations created tuple written row bigtable using word key life request figure shows user request serviced first user points browser shakespearegooglecom obtain corresponding ip address user device resolves address dns server request ultimately ends google dns server talks gslb gslb keeps track traffic load among frontend servers across regions picks server ip address send user figure life request browser connects http server ip server named google frontend gfe reverse proxy terminates tcp connection gfe looks service required web search maps orin caseshakespeare using gslb server finds available shakespeare frontend server sends server rpc containing http request shakespeare server analyzes http request constructs protobuf containing word look shakespeare frontend server needs contact shakespeare backend server frontend server contacts gslb obtain bns address suitable unloaded backend server shakespeare backend server contacts bigtable server obtain requested data answer written reply protobuf returned shakespeare backend server backend hands protobuf containing results shakespeare frontend server assembles html returns answer user entire chain events executed blink eyejust hundred milliseconds many moving parts involved many potential points failure particular failing gslb would wreak havoc however google policies rigorous testing careful rollout addition proactive error recovery methods graceful degradation allow us deliver reliable service users come expect people regularly use wwwgooglecom check internet connection set correctly job data organization load testing determined backend server handle queries per second qps trials performed limited set users lead us expect peak load qps need least tasks however following considerations mean need least tasks job n updates one task time unavailable leaving tasks machine failure might occur task update leaving tasks enough serve peak load closer examination user traffic shows peak usage distributed globally qps north america south america europe africa asia australia instead locating backends one site distribute across usa south america europe asia allowing n redundancy per region means end tasks usa europe asia however decide use tasks instead south america lower overhead n n case willing tolerate small risk higher latency exchange lower hardware costs gslb redirects traffic one continent another south american datacenter capacity save resources spend hardware larger regions spread tasks across two three clusters extra resiliency backends need contact bigtable holding data need also design storage element strategically backend asia contacting bigtable usa adds significant amount latency replicate bigtable region bigtable replication helps us two ways provides resilience bigtable server fail lowers dataaccess latency bigtable offers eventual consistency major problem need update contents often introduced lot terminology need remember useful framing many systems refer later well roughly mostly except stuff different datacenters end multiple generations compute hardware sometimes augment datacenters built part datacenter hardware homogeneous some readers may familiar borg descendant kubernetesan open source container cluster orchestration framework started google see http kubernetesio bur details similarities borg apache mesos see ver see http grpcio protocol buffers languageneutral platformneutral extensible mechanism serializing structured data details see https developersgooglecomprotocolbuffers we assume probability two simultaneous task failures environment low enough negligible single points failure topofrack switches power distribution may make assumption invalid environments part ii principles section examines principles underlying sre teams typically work the patterns behaviors areas concern influence general domain sre operations first chapter section important piece read want attain widestangle picture exactly sre reason embracing risk looks sre lens riskits assessment management use error budgets provide usefully neutral approaches service management service level objectives another foundational conceptual unit sre industry commonly lumps disparate concepts general banner service level agreements tendency makes harder think concepts clearly service level objectives attempts disentangle indicators objectives agreements examines sre uses terms provides recommendations find useful metrics applications eliminating toil one sre important tasks subject eliminating toil define toil mundane repetitive operational work providing enduring value scales linearly service growth whether google elsewhere monitoring absolutely essential component right thing production monitor service know happening blind happening reliable read monitoring distributed systems recommendations monitor implementationagnostic best practices evolution automation google examine sre approach automation walk case studies sre implemented automation successfully unsuccessfully companies treat release engineering afterthought however learn release engineering release engineering critical overall system stabilityas outages result pushing change kind also best way ensure releases consistent key principle effective software engineering reliabilityoriented engineering simplicity quality lost extraordinarily difficult recapture nevertheless old adage goes complex system works necessarily evolved simple system works simplicity goes topic detail reading google sre increasing product velocity safely core principle organization making push green reality kle published october show taking humans release process paradoxically reduce sres toil increasing system reliability chapter embracing risk written marc alvidrez edited kavita guliani might expect google try build reliable servicesones never fail turns past certain point however increasing reliability worse service users rather better extreme reliability comes cost maximizing stability limits fast new features developed quickly products delivered users dramatically increases cost turn reduces numbers features team afford offer users typically notice difference high reliability extreme reliability service user experience dominated less reliable components like cellular network device working put simply user reliable smartphone tell difference service reliability mind rather simply maximizing uptime site reliability engineering seeks balance risk unavailability goals rapid innovation efficient service operations users overall happinesswith features service performanceis optimized managing risk unreliable systems quickly erode users confidence want reduce chance system failure however experience shows build systems cost increase linearly reliability incrementsan incremental improvement reliability may cost x previous increment costliness two dimensions cost redundant machinecompute resources cost associated redundant equipment example allows us take systems offline routine unforeseen maintenance provides space us store parity code blocks provide minimum data durability guarantee opportunity cost cost borne organization allocates engineering resources build systems features diminish risk instead features directly visible usable end users engineers longer work new features products end users sre manage service reliability largely managing risk conceptualize risk continuum give equal importance figuring engineer greater reliability google systems identifying appropriate level tolerance services run allows us perform costbenefit analysis determine example nonlinear risk continuum place search ads gmail photos goal explicitly align risk taken given service risk business willing bear strive make service reliable enough reliable needs set availability target want exceed much would waste opportunities add features system clean technical debt reduce operational costs sense view availability target minimum maximum key advantage framing unlocks explicit thoughtful risktaking measuring service risk standard practice google often best served identifying objective metric represent property system want optimize setting target assess current performance track improvements degradations time service risk immediately clear reduce potential factors single metric service failures many potential effects including user dissatisfaction harm loss trust direct indirect revenue loss brand reputational impact undesirable press coverage clearly factors hard measure make problem tractable consistent across many types systems run focus unplanned downtime services straightforward way representing risk tolerance terms acceptable level unplanned downtime unplanned downtime captured desired level service availability usually expressed terms number nines would like provide availability additional nine corresponds order magnitude improvement toward availability serving systems metric traditionally calculated based proportion system uptime see timebased availability timebased availability using formula period year calculate acceptable number minutes downtime reach given number nines availability example system availability target minutes year stay within availability target see availability table table google however timebased metric availability usually meaningful looking across globally distributed services approach fault isolation makes likely serving least subset traffic given service somewhere world given time ie least partially times therefore instead using metrics around uptime define availability terms request success rate aggregate availability shows yieldbased metric calculated rolling window ie proportion successful requests oneday window aggregate availability example system serves m requests day daily availability target serve errors still hit target given day typical application requests equal failing new user signup request different failing request polling new email background many cases however availability calculated request success rate requests reasonable approximation unplanned downtime viewed enduser perspective quantifying unplanned downtime request success rate also makes availability metric amenable use systems typically serve end users directly nonserving systems eg batch pipeline storage transactional systems welldefined notion successful unsuccessful units work indeed systems discussed chapter primarily consumer infrastructure serving systems many principles also apply nonserving systems minimal modification example batch process extracts transforms inserts contents one customer databases data warehouse enable analysis may set run periodically using request success rate defined terms records successfully unsuccessfully processed calculate useful availability metric despite fact batch system run constantly often set quarterly availability targets service track performance targets weekly even daily basis strategy lets us manage service highlevel availability objective looking tracking fixing meaningful deviations inevitably arise see service level objectives details risk tolerance services mean identify risk tolerance service formal environment case safetycritical systems risk tolerance services typically built directly basic product service definition google services risk tolerance tends less clearly defined identify risk tolerance service sres must work product owners turn set business goals explicit objectives engineer case business goals concerned direct impact performance reliability service offered practice translation easier said done consumer services often clear product owners unusual infrastructure services eg storage systems generalpurpose http caching layer similar structure product ownership discuss consumer infrastructure cases turn identifying risk tolerance consumer services consumer services often product team acts business owner application example search google maps google docs product managers product managers charged understanding users business shaping product success marketplace product team exists team usually best resource discuss reliability requirements service absence dedicated product team engineers building system often play role either knowingly unknowingly many factors consider assessing risk tolerance services following level availability required different types failures different effects service use service cost help locate service risk continuum service metrics important take account target level availability target level availability given google service usually depends function provides service positioned marketplace following list includes issues consider level service users expect service tie directly revenue either revenue customers revenue paid service free competitors marketplace level service competitors provide service targeted consumers enterprises consider requirements google apps work majority users enterprise users large small enterprises depend google apps work services eg gmail calendar drive docs provide tools enable employees perform daily work stated another way outage google apps work service outage google also enterprises critically depend us typical google apps work service might set external quarterly availability target back target stronger internal availability target contract stipulates penalties fail deliver external target youtube provides contrasting set considerations google acquired youtube decide appropriate availability target website youtube focused consumers different phase business lifecycle google time youtube already great product still changing growing rapidly set lower availability target youtube enterprise products rapid feature development correspondingly important types failures expected shape failures given service another important consideration resilient business service downtime worse service constant low rate failures occasional fullsite outage types failure may result absolute number errors may vastly different impacts business illustrative example difference full partial outages naturally arises systems serve private information consider contact management application difference intermittent failures cause profile pictures fail render versus failure case results user private contacts shown another user first case clearly poor user experience sres would work remediate problem quickly second case however risk exposing private data could easily undermine basic user trust significant way result taking service entirely would appropriate debugging potential cleanup phase second case end services offered google sometimes acceptable regular outages maintenance windows number years ago ads frontend used one service used advertisers website publishers set configure run monitor advertising campaigns work takes place normal business hours determined occasional regular scheduled outages form maintenance windows would acceptable counted scheduled outages planned downtime unplanned downtime cost cost cost often key factor determining appropriate availability target service ads particularly good position make tradeoff request successes failures directly translated revenue gained lost determining availability target service ask questions build operate systems one nine availability would incremental increase revenue additional revenue offset cost reaching level reliability make tradeoff equation concrete consider following costbenefit example service request equal value proposed improvement availability target proposed increase availability service revenue m value improved availability m case cost improving availability one nine less worth investment cost greater costs exceed projected increase revenue may harder set targets simple translation function reliability revenue one useful strategy may consider background error rate isps internet failures measured enduser perspective possible drive error rate service background error rate errors fall within noise given user internet connection significant differences isps protocols eg tcp versus udp ipv versus ipv measured typical background error rate isps falling service metrics examining risk tolerance services relation metrics besides availability often fruitful understanding metrics important metrics important provides us degrees freedom attempting take thoughtful risks service latency ads systems provides illustrative example google first launched web search one service key distinguishing features speed introduced adwords displays advertisements next search results key requirement system ads slow search experience requirement driven engineering goals generation adwords systems treated invariant adsense google ads system serves contextual ads response requests javascript code publishers insert websites different latency goal latency goal adsense avoid slowing rendering thirdparty page inserting contextual ads specific latency target dependent speed given publisher page renders means adsense ads generally served hundreds milliseconds slower adwords ads looser serving latency requirement allowed us make many smart tradeoffs provisioning ie determining quantity locations serving resources use save us substantial cost naive provisioning words given relative insensitivity adsense service moderate changes latency performance able consolidate serving fewer geographical locations reducing operational overhead identifying risk tolerance infrastructure services requirements building running infrastructure components differ requirements consumer products number ways fundamental difference definition infrastructure components multiple clients often varying needs target level availability consider bigtable cha massivescale distributed storage system structured data consumer services serve data directly bigtable path user request services need low latency high reliability teams use bigtable repository data use perform offline analysis eg mapreduce regular basis teams tend concerned throughput reliability risk tolerance two use cases quite distinct one approach meeting needs use cases engineer infrastructure services ultrareliable given fact infrastructure services also tend aggregate huge amounts resources approach usually far expensive practice understand different needs different types users look desired state request queue type bigtable user types failures lowlatency user wants bigtable request queues almost always empty system process outstanding request immediately upon arrival indeed inefficient queuing often cause high tail latency user concerned offline analysis interested system throughput user wants request queues never empty optimize throughput bigtable system never need idle waiting next request see success failure antithetical sets users success lowlatency user failure user concerned offline analysis cost one way satisfy competing constraints costeffective manner partition infrastructure offer multiple independent levels service bigtable example build two types clusters lowlatency clusters throughput clusters lowlatency clusters designed operated used services need low latency high reliability ensure short queue lengths satisfy stringent client isolation requirements bigtable system provisioned substantial amount slack capacity reduced contention increased redundancy throughput clusters hand provisioned run hot less redundancy optimizing throughput latency practice able satisfy relaxed needs much lower cost perhaps little cost lowlatency cluster given bigtable massive scale cost savings becomes significant quickly key strategy regards infrastructure deliver services explicitly delineated levels service thus enabling clients make right risk cost tradeoffs building systems explicitly delineated levels service infrastructure providers effectively externalize difference cost takes provide service given level clients exposing cost way motivates clients choose level service lowest cost still meets needs example google decide put data critical enforcing user privacy highavailability globally consistent datastore eg globally replicated sqllike system like spanner cor putting optional data data critical enhances user experience cheaper less reliable less fresh eventually consistent datastore eg nosql store besteffort replication like bigtable note run multiple classes services using identical hardware software provide vastly different service guarantees adjusting variety service characteristics quantities resources degree redundancy geographical provisioning constraints critically infrastructure software configuration example frontend infrastructure demonstrate risktolerance assessment principles apply storage infrastructure let look another large class service google frontend infrastructure frontend infrastructure consists reverse proxy load balancing systems running close edge network systems among things serve one endpoint connections end users eg terminate tcp user browser given critical role engineer systems deliver extremely high level reliability consumer services often limit visibility unreliability backends infrastructure systems lucky request never makes application service frontend server lost explored ways identify risk tolerance consumer infrastructure services discuss using tolerance level manage unreliability via error budgets motivation error budgets written mark roth edited carmela quinito chapters book discuss tensions arise product development teams sre teams given generally evaluated different metrics product development performance largely evaluated product velocity creates incentive push new code quickly possible meanwhile sre performance unsurprisingly evaluated based upon reliability service implies incentive push back high rate change information asymmetry two teams amplifies inherent tension product developers visibility time effort involved writing releasing code sres visibility service reliability state production general tensions often reflect different opinions level effort put engineering practices following list presents typical tensions software fault tolerance hardened make software unexpected events little brittle unusable product much product one wants use runs stably testing enough testing embarrassing outages privacy data leaks number pressworthy events much testing might lose market push frequency every push risky much work reducing risk versus work canary duration size best practice test new release small subset typical workload practice often called canarying long wait big canary usually preexisting teams worked kind informal balance riskeffort boundary lies unfortunately one rarely prove balance optimal rather function negotiating skills engineers involved decisions driven politics fear hope indeed google sre unofficial motto hope strategy instead goal define objective metric agreed upon sides used guide negotiations reproducible way databased decision better forming error budget order base decisions objective data two teams jointly define quarterly error budget based service service level objective slo see service level objectives error budget provides clear objective metric determines unreliable service allowed within single quarter metric removes politics negotiations sres product developers deciding much risk allow practice follows product management defines slo sets expectation much uptime service per quarter actual uptime measured neutral third party monitoring system difference two numbers budget much unreliability remaining quarter long uptime measured sloin words long error budget remainingnew releases pushed example imagine service slo successfully serve queries per quarter means service error budget failure rate given quarter problem causes us fail expected queries quarter problem spends service quarterly error budget benefits benefits main benefit error budget provides common incentive allows product development sre focus finding right balance innovation reliability many products use control loop manage release velocity long system slos met releases continue slo violations occur frequently enough expend error budget releases temporarily halted additional resources invested system testing development make system resilient improve performance subtle effective approaches available simple onoff technique instance slowing releases rolling back sloviolation error budget close used example product development wants skimp testing increase push velocity sre resistant error budget guides decision budget large product developers take risks budget nearly drained product developers push testing slower push velocity want risk using budget stall launch effect product development team becomes selfpolicing know budget manage risk course outcome relies sre team authority actually stop launches slo broken happens network outage datacenter failure reduces measured slo events also eat error budget result number new pushes may reduced remainder quarter entire team supports reduction everyone shares responsibility uptime budget also helps highlight costs overly high reliability targets terms inflexibility slow innovation team trouble launching new features may elect loosen slo thus increasing error budget order increase innovation key insights managing service reliability largely managing risk managing risk costly probably never right reliability target impossible achieve typically reliability service users want notice match profile service risk business willing take error budget aligns incentives emphasizes joint ownership sre product development error budgets make easier decide rate releases effectively defuse discussions outages stakeholders allows multiple teams reach conclusion production risk without rancor an early version section appeared article login august vol known bangbang controlsee https enwikipediaorgwikibang bangcontrol chapter service level objectives written chris jones john wilkes niall murphy cody smith edited betsy beyer impossible manage service correctly let alone well without understanding behaviors really matter service measure evaluate behaviors end would like define deliver given level service users whether use internal api public product use intuition experience understanding users want define service level indicators slis objectives slos agreements slas measurements describe basic properties metrics matter values want metrics react provide expected service ultimately choosing appropriate metrics helps drive right action something goes wrong also gives sre team confidence service healthy chapter describes framework use wrestle problems metric modeling metric selection metric analysis much explanation would quite abstract without example use shakespeare service outlined shakespeare sample service illustrate main points service level terminology many readers likely familiar concept sla terms sli slo also worth careful definition common use term sla overloaded taken number meanings depending context prefer separate meanings clarity indicators sli service level indicatora carefully defined quantitative measure aspect level service provided services consider request latencyhow long takes return response requestas key sli common slis include error rate often expressed fraction requests received system throughput typically measured requests per second measurements often aggregated ie raw data collected measurement window turned rate average percentile ideally sli directly measures service level interest sometimes proxy available desired measure may hard obtain interpret example clientside latency often userrelevant metric might possible measure latency server another kind sli important sres availability fraction time service usable often defined terms fraction wellformed requests succeed sometimes called yield durabilitythe likelihood data retained long period timeis equally important data storage systems although availability impossible near availability often readily achievable industry commonly expresses highavailability values terms number nines availability percentage example availabilities referred nines nines availability respectively current published target google compute engine availability three half nines availability objectives slo service level objective target value range values service level measured sli natural structure slos thus sli target lower bound sli upper bound example might decide return shakespeare search results quickly adopting slo average search request latency less milliseconds choosing appropriate slo complex begin always get choose value incoming http requests outside world service queries per second qps metric essentially determined desires users really set slo hand say want average latency per request milliseconds setting goal could turn motivate write frontend lowlatency behaviors various kinds buy certain kinds lowlatency equipment milliseconds obviously arbitrary value general lower latency numbers good excellent reasons believe fast better slow userexperienced latency certain values actually drives people away see speed matters bru details subtle might first appear two slis qps latencymight connected behind scenes higher qps often leads larger latencies common services performance cliff beyond load threshold choosing publishing slos users sets expectations service perform strategy reduce unfounded complaints service owners example service slow without explicit slo users often develop beliefs desired performance may unrelated beliefs held people designing operating service dynamic lead overreliance service users incorrectly believe service available actually happened chubby see global chubby planned outage underreliance prospective users believe system flakier less reliable actually global chubby planned outage written marc alvidrez chubby bur google lock service loosely coupled distributed systems global case distribute chubby instances replica different geographical region time found failures global instance chubby consistently generated service outages many visible end users turns true global chubby outages infrequent service owners began add dependencies chubby assuming would never go high reliability provided false sense security services could function appropriately chubby unavailable however rarely occurred solution chubby scenario interesting sre makes sure global chubby meets significantly exceed service level objective given quarter true failure dropped availability target controlled outage synthesized intentionally taking system way able flush unreasonable dependencies chubby shortly added forces service owners reckon reality distributed systems sooner rather later agreements finally slas service level agreements explicit implicit contract users includes consequences meeting missing slos contain consequences easily recognized financial rebate penaltybut take forms easy way tell difference slo sla ask happens slos met explicit consequence almost certainly looking slo sre typically get involved constructing slas slas closely tied business product decisions sre however get involved helping avoid triggering consequences missed slos also help define slis obviously needs objective way measure slos agreement disagreements arise google search example important service sla public want everyone use search fluidly efficiently possible signed contract whole world even still consequences search availableunavailability results hit reputation well drop advertising revenue many google services google work explicit slas users whether particular service sla valuable define slis slos use manage service much theorynow experience indicators practice given made case choosing appropriate metrics measure service important go identifying metrics meaningful service system users care users care use every metric track monitoring system sli understanding users want system inform judicious selection indicators choosing many indicators makes hard pay right level attention indicators matter choosing may leave significant behaviors system unexamined typically find handful representative indicators enough evaluate reason system health services tend fall broad categories terms slis find relevant userfacing serving systems shakespeare search frontends generally care availability latency throughput words could respond request long take respond many requests could handled storage systems often emphasize latency availability durability words long take read write data access data demand data still need see data integrity read wrote extended discussion issues big data systems data processing pipelines tend care throughput endtoend latency words much data processed long take data progress ingestion completion pipelines may also targets latency individual processing stages systems care correctness right answer returned right data retrieved right analysis done correctness important track indicator system health even though often property data system rather infrastructure per se usually sre responsibility meet collecting indicators many indicator metrics naturally gathered server side using monitoring system borgmon see practical alerting timeseries data prometheus periodic log analysisfor instance http responses fraction requests however systems instrumented clientside collection measuring behavior client miss range problems affect users affect serverside metrics example concentrating response latency shakespeare search backend might miss poor user latency due problems page javascript case measuring long takes page become usable browser better proxy user actually experiences aggregation simplicity usability often aggregate raw measurements needs done carefully metrics seemingly straightforward like number requests per second served even apparently straightforward measurement implicitly aggregates data measurement window measurement obtained second averaging requests minute latter may hide much higher instantaneous request rates bursts last seconds consider system serves requestss evennumbered seconds others average load one serves constant requestss instantaneous load twice large average one similarly averaging request latencies may seem attractive obscures important detail entirely possible requests fast long tail requests much much slower metrics better thought distributions rather averages example latency sli requests serviced quickly others invariably take longersometimes much longer simple average obscure tail latencies well changes figure provides example although typical request served ms requests times slower monitoring alerting based average latency would show change behavior course day fact significant changes tail latency topmost line figure th th th th percentile latencies system note yaxis logarithmic scale using percentiles indicators allows consider shape distribution differing attributes highorder percentile th th shows plausible worstcase value using th percentile also known median emphasizes typical case higher variance response times typical user experience affected longtail behavior effect exacerbated high load queuing effects user studies shown people typically prefer slightly slower system one high variance response time sre teams focus high percentile values grounds th percentile behavior good typical experience certainly going note statistical fallacies generally prefer work percentiles rather mean arithmetic average set values makes possible consider long tail data points often significantly different interesting characteristics average artificial nature computing systems data points often skewedfor instance request response less ms timeout ms means successful responses values greater timeout result assume mean median sameor even close try assume data normally distributed without verifying first case standard intuitions approximations hold example distribution expected process takes action sees outliers eg restarting server high request latencies may often often enough standardize indicators recommend standardize common definitions slis reason first principles time feature conforms standard definition templates omitted specification individual sli eg aggregation intervals averaged minute aggregation regions tasks cluster frequently measurements made every seconds requests included http gets blackbox monitoring jobs data acquired monitoring measured server dataaccess latency time last byte save effort build set reusable sli templates common metric also make simpler everyone understand specific sli means objectives practice start thinking finding users care measure often users care difficult impossible measure end approximating users needs way however simply start easy measure end less useful slos result sometimes found working desired objectives backward specific indicators works better choosing indicators coming targets defining objectives maximum clarity slos specify measured conditions valid instance might say following second line first relies sli defaults previous section remove redundancy averaged minute get rpc calls complete less ms measured across backend servers get rpc calls complete less ms shape performance curves important specify multiple slo targets get rpc calls complete less ms get rpc calls complete less ms get rpc calls complete less ms users heterogeneous workloads bulk processing pipeline cares throughput interactive client cares latency may appropriate define separate objectives class workload throughput clients set rpc calls complete s latency clients set rpc calls payloads kb complete ms unrealistic undesirable insist slos met time reduce rate innovation deployment require expensive overly conservative solutions instead better allow error budgeta rate slos missedand track daily weekly basis upper management probably want monthly quarterly assessment error budget slo meeting slos slo violation rate compared error budget see motivation error budgets gap used input process decides roll new releases choosing targets choosing targets slos purely technical activity product business implications reflected slis slos maybe slas selected similarly may necessary trade certain product attributes others within constraints posed staffing time market hardware availability funding sre part conversation advise risks viability different options learned lessons help make productive discussion pick target based current performance understanding merits limits system essential adopting values without reflection may lock supporting system requires heroic efforts meet targets improved without significant redesign keep simple complicated aggregations slis obscure changes system performance also harder reason avoid absolutes tempting ask system scale load infinitely without latency increase always available requirement unrealistic even system approaches ideals probably take long time design build expensive operateand probably turn unnecessarily better users would happy even delighted slos possible choose enough slos provide good coverage system attributes defend slos pick ever win conversation priorities quoting particular slo probably worth slo however product attributes amenable slos hard specify user delight slo perfection wait always refine slo definitions targets time learn system behavior better start loose target tighten choose overly strict target relaxed discover unattainable slos canand shouldbe major driver prioritizing work sres product developers reflect users care good slo helpful legitimate forcing function development team poorly thoughtout slo result wasted work team uses heroic efforts meet overly aggressive slo bad product slo lax slos massive lever use wisely control measures slis slos crucial elements control loops used manage systems monitor measure system slis compare slis slos decide whether action needed action needed figure needs happen order meet target take action example step shows request latency increasing miss slo hours unless something done step might include testing hypothesis servers cpubound deciding add spread load without slo know whether take action slos set expectations publishing slos sets expectations system behavior users potential users often want know expect service order understand whether appropriate use case instance team wanting build photosharing website might want avoid using service promises strong durability low cost exchange slightly lower availability though service might perfect fit archival records management system order set realistic expectations users might consider using one following tactics keep safety margin using tighter internal slo slo advertised users gives room respond chronic problems become visible externally slo buffer also makes possible accommodate reimplementations trade performance attributes cost ease maintenance without disappoint users overachieve users build reality offer rather say supply particularly infrastructure services service actual performance much better stated slo users come rely current performance avoid overdependence deliberately taking system offline occasionally google chubby service introduced planned outages response overly available throttling requests designing system faster light loads understanding well system meeting expectations helps decide whether invest making system faster available resilient alternatively service fine perhaps staff time spent priorities paying technical debt adding new features introducing products agreements practice crafting sla requires business legal teams pick appropriate consequences penalties breach sre role help understand likelihood difficulty meeting slos contained sla much advice slo construction also applicable slas wise conservative advertise users broader constituency harder change delete slas prove unwise difficult work most people really mean slo say sla one giveaway somebody talks sla violation almost always talking missed slo real sla violation might trigger court case breach contract if ever win conversation slos probably worth sre team product failure injection ben serves different purpose also help set expectations chapter eliminating toil written vivek rau edited betsy beyer human operator needs touch system normal operations bug definition normal changes systems grow carla geisser google sre sre want spend time longterm engineering project work instead operational work term operational work may misinterpreted use specific word toil toil defined toil work like also simply equivalent administrative chores grungy work preferences types work satisfying enjoyable vary person person people even enjoy manual repetitive work also administrative chores get done categorized toil overhead overhead often work directly tied running production service includes tasks like team meetings setting grading goals snippets hr paperwork grungy work sometimes longterm value case toil either cleaning entire alerting configuration service removing clutter may grungy toil toil toil kind work tied running production service tends manual repetitive automatable tactical devoid enduring value scales linearly service grows every task deemed toil attributes closely work matches one following descriptions likely toil manual includes work manually running script automates task running script may quicker manually executing step script handson time human spends running script elapsed time still toil time repetitive performing task first time ever even second time work toil toil work solving novel problem inventing new solution work toil automatable machine could accomplish task well human need task could designed away task toil human judgment essential task good chance toil tactical toil interruptdriven reactive rather strategydriven proactive handling pager alerts toil may never able eliminate type work completely continually work toward minimizing enduring value service remains state finished task task probably toil task produced permanent improvement service probably toil even amount grunt work digging legacy code configurations straightening outwas involved n service growth work involved task scales linearly service size traffic volume user count task probably toil ideally managed designed service grow least one order magnitude zero additional work onetime efforts add resources less toil better sre organization advertised goal keeping operational work ie toil sre time least sre time spent engineering project work either reduce future toil add service features feature development typically focuses improving reliability performance utilization often reduces toil secondorder effect share goal toil tends expand left unchecked quickly fill everyone time work reducing toil scaling services engineering site reliability engineering engineering work enables sre organization scale sublinearly service size manage services efficiently either pure dev team pure ops team furthermore hire new sres promise sre typical ops organization quoting rule mentioned need keep promise allowing sre organization subteam within devolve ops team calculating toil seek cap time sre spends toil time spent floor amount toil sre handle oncall typical sre one week primary oncall one week secondary oncall cycle discussion primary versus secondary oncall shifts see oncall follows person rotation least every weeks dedicated oncall shifts interrupt handling means lower bound potential toil sre time person rotation lower bound consistent data sres report top source toil interrupts nonurgent servicerelated messages emails next leading source oncall urgent response followed releases pushes even though release push processes usually handled fair amount automation still plenty room improvement area quarterly surveys google sres show average time spent toiling much better overall target however average capture outliers sres claim toil pure development projects oncall work others claim toil individual sres report excessive toil often indicates need managers spread toil load evenly across team encourage sres find satisfying engineering projects qualifies engineering engineering work novel intrinsically requires human judgment produces permanent improvement service guided strategy frequently creative innovative taking designdriven approach solving problemthe generalized better engineering work helps team sre organization handle larger service services level staffing typical sre activities fall following approximate categories software engineering involves writing modifying code addition associated design documentation work examples include writing automation scripts creating tools frameworks adding service features scalability reliability modifying infrastructure code make robust systems engineering involves configuring production systems modifying configurations documenting systems way produces lasting improvements onetime effort examples include monitoring setup updates load balancing configuration server configuration tuning os parameters load balancer setup systems engineering also includes consulting architecture design productionization developer teams toil work directly tied running service repetitive manual etc overhead administrative work tied directly running service examples include hiring hr paperwork teamcompany meetings bug queue hygiene snippets peer reviews selfassessments training courses every sre needs spend least time engineering work averaged quarters year toil tends spiky steady time spent engineering may realistic sre teams may dip target quarters fraction time spent projects averages significantly long haul affected team needs step back figure wrong toil always bad toil make everyone unhappy time especially small amounts predictable repetitive tasks quite calming produce sense accomplishment quick wins lowrisk lowstress activities people gravitate toward tasks involving toil may even enjoy type work toil always invariably bad everyone needs absolutely clear amount toil unavoidable sre role indeed almost engineering role fine small doses happy small doses toil problem toil becomes toxic experienced large quantities burdened much toil concerned complain loudly among many reasons much toil bad consider following career stagnation career progress slow grind halt spend little time projects google rewards grungy work inevitable big positive impact make career grunge low morale people different limits much toil tolerate everyone limit much toil leads burnout boredom discontent additionally spending much time toil expense time spent engineering hurts sre organization following ways creates confusion work hard ensure everyone works sre organization understands engineering organization individuals teams within sre engage much toil undermine clarity communication confuse people role slows progress excessive toil makes team less productive product feature velocity slow sre team busy manual work firefighting roll new features promptly sets precedent willing take toil dev counterparts incentives load even toil sometimes shifting operational tasks rightfully performed devs sre teams may also start expecting sres take work bad obvious reasons promotes attrition even personally unhappy toil current future teammates might like much less build much toil team procedures motivate team best engineers start looking elsewhere rewarding job causes breach faith new hires transfers joined sre promise project work feel cheated bad morale conclusion commit eliminate bit toil week good engineering steadily clean services shift collective efforts engineering scale architecting next generation services building crosssre toolchains let invent toil less we use objectives key results system pioneered andy grove intel see kla googlers record short freeform summaries snippets worked week we careful saying task toil needs human judgment need think carefully whether nature task intrinsically requires human judgment addressed better design example one could build built service alerts sres several times day alert requires complex response involving plenty human judgment service poorly designed unnecessary complexity system needs simplified rebuilt either eliminate underlying failure conditions deal conditions automatically redesign reimplementation finished improved service rolled work applying human judgment respond alert definitely toil chapter monitoring distributed systems written rob ewaschuk edited betsy beyer google sre teams basic principles best practices building successful monitoring alerting systems chapter offers guidelines issues interrupt human via page deal issues serious enough trigger page definitions uniformly shared vocabulary discussing topics related monitoring even within google usage following terms varies common interpretations listed monitoring collecting processing aggregating displaying realtime quantitative data system query counts types error counts types processing times server lifetimes whitebox monitoring monitoring based metrics exposed internals system including logs interfaces like java virtual machine profiling interface http handler emits internal statistics blackbox monitoring testing externally visible behavior user would see dashboard application usually webbased provides summary view service core metrics dashboard may filters selectors prebuilt expose metrics important users dashboard might also display team information ticket queue length list highpriority bugs current oncall engineer given area responsibility recent pushes alert notification intended read human pushed system bug ticket queue email alias pager respectively alerts classified tickets email alerts pages root cause defect software human system repaired instills confidence event happen way given incident might multiple root causes example perhaps caused combination insufficient process automation software crashed bogus input insufficient testing script used generate configuration factors might stand alone root cause repaired node machine used interchangeably indicate single instance running kernel either physical server virtual machine container might multiple services worth monitoring single machine services may either related example caching server web server unrelated services sharing hardware example code repository master configuration system like puppet chef push change service running software configuration monitor many reasons monitor system including analyzing longterm trends big database fast growing quickly dailyactive user count growing comparing time experiment groups queries faster acme bucket bytes versus ajax db much better memcache hit rate extra node site slower last week alerting something broken somebody needs fix right something might break soon somebody look soon building dashboards dashboards answer basic questions service normally include form four golden signals discussed four golden signals conducting ad hoc retrospective analysis ie debugging latency shot else happened around time system monitoring also helpful supplying raw input business analytics facilitating analysis security breaches book focuses engineering domains sre particular expertise discuss applications monitoring monitoring alerting enables system tell us broken perhaps tell us break system able automatically fix want human investigate alert determine real problem hand mitigate problem determine root cause problem unless performing security auditing narrowly scoped components system never trigger alert simply something seems bit weird paging human quite expensive use employee time employee work page interrupts workflow employee home page interrupts personal time perhaps even sleep pages occur frequently employees secondguess skim even ignore incoming alerts sometimes even ignoring real page masked noise outages prolonged noise interferes rapid diagnosis fix effective alerting systems good signal low noise setting reasonable expectations monitoring monitoring complex application significant engineering endeavor even substantial existing infrastructure instrumentation collection display alerting place google sre team members typically one sometimes two members whose primary assignment build maintain monitoring systems service number decreased time generalize centralize common monitoring infrastructure every sre team typically least one monitoring person said fun access traffic graph dashboards like sre teams carefully avoid situation requires someone stare screen watch problems general google trended toward simpler faster monitoring systems better tools post hoc analysis avoid magic systems try learn thresholds automatically detect causality rules detect unexpected changes enduser request rates one counterexample rules still kept simple possible give quick detection simple specific severe anomaly uses monitoring data capacity planning traffic prediction tolerate fragility thus complexity observational experiments conducted long time horizon months years low sampling rate hours days also often tolerate fragility occasional missed samples hide longrunning trend google sre experienced limited success complex dependency hierarchies seldom use rules know database slow alert slow database otherwise alert website generally slow dependencyreliant rules usually pertain stable parts system system draining user traffic away datacenter example datacenter drained alert latency one common datacenter alerting rule teams google maintain complex dependency hierarchies infrastructure steady rate continuous refactoring ideas described chapter still aspirational always room move rapidly symptom root cause especially everchanging systems chapter sets goals monitoring systems ways achieve goals important monitoring systemsespecially critical path onset production problem page human basic triage deep debuggingbe kept simple comprehensible everyone team similarly keep noise low signal high elements monitoring system direct pager need simple robust rules generate alerts humans simple understand represent clear failure symptoms versus causes monitoring system address two questions broken broken indicates symptom indicates possibly intermediate cause table lists hypothetical symptoms corresponding causes table example symptoms causes symptom cause serving http s s database servers refusing connections responses slow cpus overloaded bogosort ethernet cable crimped rack visible partial packet loss content distribution network users antarctica receiving hates scientists felines thus animated cat gifs blacklisted client ips private content worldreadable new software push caused acls forgotten allowed requests versus one important distinctions writing good monitoring maximum signal minimum noise blackbox versus whitebox combine heavy use whitebox monitoring modest critical uses blackbox monitoring simplest way think blackbox monitoring versus whitebox monitoring blackbox monitoring symptomoriented represents activenot predictedproblems system working correctly right whitebox monitoring depends ability inspect innards system logs http endpoints instrumentation whitebox monitoring therefore allows detection imminent problems failures masked retries forth note multilayered system one person symptom another person cause example suppose database performance slow slow database reads symptom database sre detects however frontend sre observing slow website slow database reads cause therefore whitebox monitoring sometimes symptomoriented sometimes causeoriented depending informative whitebox collecting telemetry debugging whitebox monitoring essential web servers seem slow databaseheavy requests need know fast web server perceives database fast database believes otherwise distinguish actually slow database server network problem web server database paging blackbox monitoring key benefit forcing discipline nag human problem already ongoing contributing real symptoms hand notyetoccurring imminent problems blackbox monitoring fairly useless four golden signals four golden signals monitoring latency traffic errors saturation measure four metrics userfacing system focus four latency time takes service request important distinguish latency successful requests latency failed requests example http error triggered due loss connection database critical backend might served quickly however http error indicates failed request factoring s overall latency might result misleading calculations hand slow error even worse fast error therefore important track error latency opposed filtering errors traffic measure much demand placed system measured highlevel systemspecific metric web service measurement usually http requests per second perhaps broken nature requests eg static versus dynamic content audio streaming system measurement might focus network io rate concurrent sessions keyvalue storage system measurement might transactions retrievals per second errors rate requests fail either explicitly eg http s implicitly example http success response coupled wrong content policy example committed onesecond response times request one second error protocol response codes insufficient express failure conditions secondary internal protocols may necessary track partial failure modes monitoring cases drastically different catching http s load balancer decent job catching completely failed requests endtoend system tests detect serving wrong content saturation full service measure system fraction emphasizing resources constrained eg memoryconstrained system show memory ioconstrained system show io note many systems degrade performance achieve utilization utilization target essential complex systems saturation supplemented higherlevel load measurement service properly handle double traffic handle traffic handle even less traffic currently receives simple services parameters alter complexity request eg give nonce need globally unique monotonic integer rarely change configuration static value load test might adequate discussed previous paragraph however services need use indirect signals like cpu utilization network bandwidth known upper bound latency increases often leading indicator saturation measuring th percentile response time small window eg one minute give early signal saturation finally saturation also concerned predictions impending saturation looks like database fill hard drive hours measure four golden signals page human one signal problematic case saturation nearly problematic service least decently covered monitoring worrying tail instrumentation performance building monitoring system scratch tempting design system based upon mean quantity mean latency mean cpu usage nodes mean fullness databases danger presented latter two cases obvious cpus databases easily utilized imbalanced way holds latency run web service average latency ms requests per second requests might easily take seconds users depend several web services render page th percentile one backend easily become median response frontend simplest way differentiate slow average slow tail requests collect request counts bucketed latencies suitable rendering histogram rather actual latencies many requests serve took ms ms ms ms ms ms ms ms distributing histogram boundaries approximately exponentially case factors roughly often easy way visualize distribution requests choosing appropriate resolution measurements different aspects system measured different levels granularity example observing cpu load time span minute reveal even quite longlived spikes drive high tail latencies hand web service targeting hours aggregate downtime per year annual uptime probing success status twice minute probably unnecessarily frequent similarly checking hard drive fullness service targeting availability every minutes probably unnecessary take care structure granularity measurements collecting persecond measurements cpu load might yield interesting data frequent measurements may expensive collect store analyze monitoring goal calls high resolution require extremely low latency reduce costs performing internal sampling server configuring external system collect aggregate distribution time across servers might record current cpu utilization second using buckets granularity increment appropriate cpu utilization bucket second aggregate values every minute strategy allows observe brief cpu hotspots without incurring high cost due collection retention simple possible simpler piling requirements top add complex monitoring systemyour system might end following levels complexity alerts different latency thresholds different percentiles kinds different metrics extra code detect expose possible causes associated dashboards possible causes sources potential complexity neverending like software systems monitoring become complex fragile complicated change maintenance burden therefore design monitoring system eye toward simplicity choosing monitor keep following guidelines mind rules catch real incidents often simple predictable reliable possible data collection aggregation alerting configuration rarely exercised eg less quarter sre teams removal signals collected exposed prebaked dashboard used alert candidates removal google experience basic collection aggregation metrics paired alerting dashboards worked well relatively standalone system fact google monitoring system broken several binaries typically people learn aspects binaries tempting combine monitoring aspects inspecting complex systems detailed system profiling singleprocess debugging tracking details exceptions crashes load testing log collection analysis traffic inspection subjects share commonalities basic monitoring blending together many results overly complex fragile systems many aspects software engineering maintaining distinct systems clear simple loosely coupled points integration better strategy example using web apis pulling summary data format remain constant extended period time tying principles together principles discussed chapter tied together philosophy monitoring alerting widely endorsed followed within google sre teams monitoring philosophy bit aspirational good starting point writing reviewing new alert help organization ask right questions regardless size organization complexity service system creating rules monitoring alerting asking following questions help avoid false positives pager burnout rule detect otherwise undetected condition urgent actionable actively imminently uservisible ever able ignore alert knowing benign able ignore alert avoid scenario alert definitely indicate users negatively affected detectable cases users negatively impacted drained traffic test deployments filtered take action response alert action urgent could wait morning could action safely automated action longterm fix shortterm workaround people getting paged issue therefore rendering least one pages unnecessary questions reflect fundamental philosophy pages pagers every time pager goes able react sense urgency react sense urgency times day become fatigued every page actionable every page response require intelligence page merely merits robotic response page pages novel problem event seen perspective dissipates certain distinctions page satisfies preceding four bullets irrelevant whether page triggered whitebox blackbox monitoring perspective also amplifies certain distinctions better spend much effort catching symptoms causes comes causes worry definite imminent causes monitoring long term modern production systems monitoring systems track everevolving system changing software architecture load characteristics performance targets alert currently exceptionally rare hard automate might become frequent perhaps even meriting hackedtogether script resolve point someone find eliminate root causes problem resolution possible alert response deserves fully automated important decisions monitoring made longterm goals mind every page happens today distracts human improving system tomorrow often case taking shortterm hit availability performance order improve longterm outlook system let take look two case studies illustrate tradeoff bigtable sre tale overalerting google internal infrastructure typically offered measured service level objective slo see service level objectives many years ago bigtable service slo based synthetic wellbehaved client mean performance problems bigtable lower layers storage stack mean performance driven large tail worst requests often significantly slower rest email alerts triggered slo approached paging alerts triggered slo exceeded types alerts firing voluminously consuming unacceptable amounts engineering time team spent significant amounts time triaging alerts find really actionable often missed problems actually affected users many pages nonurgent due wellunderstood problems infrastructure either rote responses received response remedy situation team used threepronged approach making great efforts improve performance bigtable also temporarily dialed back slo target using th percentile request latency also disabled email alerts many spending time diagnosing infeasible strategy gave us enough breathing room actually fix longerterm problems bigtable lower layers storage stack rather constantly fixing tactical problems oncall engineers could actually accomplish work kept pages hours ultimately temporarily backing alerts allowed us make faster progress toward better service gmail predictable scriptable responses humans early days gmail service built retrofitted distributed process management system called workqueue originally created batch processing pieces search index workqueue adapted longlived processes subsequently applied gmail certain bugs relatively opaque codebase scheduler proved hard beat time gmail monitoring structured alerts fired individual tasks descheduled workqueue setup less ideal even time gmail many many thousands tasks task representing fraction percent users cared deeply providing good user experience gmail users alerting setup unmaintainable address problem gmail sre built tool helped poke scheduler right way minimize impact users team several discussions whether simply automate entire loop detecting problem nudging rescheduler better longterm solution achieved worried kind workaround would delay real fix kind tension common within team often reflects underlying mistrust team selfdiscipline team members want implement hack allow time proper fix others worry hack forgotten proper fix deprioritized indefinitely concern credible easy build layers unmaintainable technical debt patching problems instead making real fixes managers technical leaders play key role implementing true longterm fixes supporting prioritizing potentially timeconsuming longterm fixes even initial pain paging subsides pages rote algorithmic responses red flag unwillingness part team automate pages implies team lacks confidence clean technical debt major problem worth escalating long run common theme connects previous examples bigtable gmail tension shortterm longterm availability often sheer force effort help rickety system achieve high availability path usually shortlived fraught burnout dependence small number heroic team members taking controlled shortterm decrease availability often painful strategic trade longrun stability system important think every page event isolation consider whether overall level paging leads toward healthy appropriately available system healthy viable team longterm outlook review statistics page frequency usually expressed incidents per shift incident might composed related pages quarterly reports management ensuring decision makers kept date pager load overall health teams conclusion healthy monitoring alerting pipeline simple easy reason focuses primarily symptoms paging reserving causeoriented heuristics serve aids debugging problems monitoring symptoms easier stack monitor though monitoring saturation performance subsystems databases often must performed directly subsystem email alerts limited value tend easily become overrun noise instead favor dashboard monitors ongoing subcritical problems sort information typically ends email alerts dashboard might also paired log order analyze historical correlations long haul achieving successful oncall rotation product includes choosing alert symptoms imminent real problems adapting targets goals actually achievable making sure monitoring supports rapid diagnosis sometimes known alert spam rarely read acted if requests x average means rest requests twice fast average measuring distribution idea requests near mean hopeful thinking see applying cardiac alarm management techniques oncall hol example alert fatigue another context zeroredundancy n situations count imminent nearly full parts service details concept redundancy see https enwikipediaorgwikin bredundancy chapter evolution automation google written niall murphy john looney michael kacirek edited betsy beyer besides black art automation mechanization federico garcía lorca spanish poet playwright sre automation force multiplier panacea course multiplying force naturally change accuracy force applied automation thoughtlessly create many problems solves therefore believe softwarebased automation superior manual operation circumstances better either option higherlevel system design requiring neither theman autonomous system put another way value automation comes judicious application discuss value automation attitude evolved time value automation exactly value automation consistency although scale obvious motivation automation many reasons use take example university computing systems many systems engineering folks started careers systems administrators background generally charged running collection machines software accustomed manually performing various actions discharge duty one common example creating user accounts others include purely operational duties like making sure backups happen managing server failover small data manipulations like changing upstream dns servers resolvconf dns server zone data similar activities ultimately however prevalence manual tasks unsatisfactory organizations indeed people maintaining systems way start action performed human humans hundreds times performed way time even best world us ever consistent machine inevitable lack consistency leads mistakes oversights issues data quality yes reliability problems domainthe execution wellscoped known procedures value consistency many ways primary value automation platform automation provide consistency designed done properly automatic systems also provide platform extended applied systems perhaps even spun profit alternative automation neither cost effective extensible instead tax levied operation system platform also centralizes mistakes words bug fixed code fixed forever unlike sufficiently large set humans performing procedure discussed previously platform extended perform additional tasks easily humans instructed perform sometimes even realize done depending nature task run either continuously much frequently humans could appropriately accomplish task times inconvenient humans furthermore platform export metrics performance otherwise allow discover details process know previously details easily measurable within context platform faster repairs additional benefit systems automation used resolve common faults system frequent situation srecreated automation automation runs regularly successfully enough result reduced mean time repair mttr common faults spend time tasks instead thereby achieving increased developer velocity spend time either preventing problem commonly cleaning well understood industry later product lifecycle problem discovered expensive fix see testing reliability generally problems occur actual production expensive fix terms time money means automated system looking problems soon arise good chance lowering total cost system given system sufficiently large faster action infrastructural situations sre automation tends deployed humans usually react fast machines common cases example failover traffic switching well defined particular application makes sense effectively require human intermittently press button called allow system continue run yes true sometimes automatic procedures end making bad situation worse procedures scoped welldefined domains google large amount automation many cases services support could long survive without automation crossed threshold manageable manual operation long ago time saving finally time saving oftquoted rationale automation although people cite rationale automation others many ways benefit often less immediately calculable engineers often waver whether particular piece automation code worth writing terms effort saved requiring task performed manually versus effort required write it easy overlook fact encapsulated task automation anyone execute task therefore time savings apply across anyone would plausibly use automation decoupling operator operation powerful joseph bironas sre led google datacenter turnup efforts time forcefully argued engineering processes solutions automatable continue staff humans maintain system staff humans work feeding machines blood sweat tears human beings think matrix less special effects pissed system administrators value google sre benefits tradeoffs apply us much anyone else google strong bias toward automation part preference automation springs particular business challenges products services look planetspanning scale typically time engage kind machine service handholding common organizations truly large services factors consistency quickness reliability dominate conversations tradeoffs performing automation another argument favor automation particularly case google complicated yet surprisingly uniform production environment described production environment google viewpoint sre organizations might important piece equipment without readily accessible api software source code available another impediment complete control production operations google generally avoids scenarios built apis systems api available vendor even though purchasing software particular task would much cheaper short term chose write solutions produced apis potential much greater longterm benefits spent lot time overcoming obstacles automatic system management resolutely developed automatic system management given google manages source code pot availability code less system sre touches also means mission product production much easier control entirety stack course although google ideologically bent upon using machines manage machines possible reality requires modification approach appropriate automate every component every system everyone ability inclination develop automation particular time essential systems started quick prototypes designed last interface automation previous paragraphs state maximalist view position one broadly successful putting action within google context general chosen create platforms could position could create platforms time view platformbased approach necessary manageability scalability use cases automation industry automation term generally used writing code solve wide variety problems although motivations writing code solutions often quite different broadly view automation metasoftware software act software implied earlier number use cases automation nonexhaustive list examples user account creation cluster turnup turndown services software hardware installation preparation decommissioning rollouts new software versions runtime configuration changes special case runtime config changes changes dependencies list could continue essentially ad infinitum google sre use cases automation google use cases listed however within google sre primary affinity typically running infrastructure opposed managing quality data passes infrastructure line totally clearfor example care deeply half dataset vanishes push therefore alert coarsegrain differences like rare us write equivalent changing properties arbitrary subset accounts system therefore context automation often automation manage lifecycle systems data example deployments service new cluster extent sre automation efforts far many people organizations except use different tools manage different focus discuss widely available tools like puppet chef cfengine even perl provide ways automate particular tasks differ mostly terms level abstraction components provided help act automating full language like perl provides posixlevel affordances theory provide essentially unlimited scope automation across apis accessible system whereas chef puppet provide outofthebox abstractions services higherlevel entities manipulated tradeoff classic higherlevel abstractions easier manage reason encounter leaky abstraction fail systemically repeatedly potentially inconsistently example often assume pushing new binary cluster atomic cluster either end old version new version however realworld behavior complicated cluster network fail halfway machines fail communication cluster management layer fail leaving system inconsistent state depending situation new binaries could staged pushed pushed restarted restarted verifiable abstractions model kinds outcomes successfully generally end halting calling intervention truly bad automation systems even sre number philosophies products domain automation look like generic rollout tools without particularly detailed modeling higherlevel entities look like languages describing service deployment abstract level work done latter tends reusable common platform former complexity production environment sometimes means former approach immediately tractable option hierarchy automation classes although automation steps valuable indeed automation platform valuable ideal world need externalized automation fact instead system external glue logic would even better system needs glue logic internalization efficient although efficiency useful designed need glue logic first place accomplishing involves taking use cases glue logic generally first order manipulations system adding accounts performing system turnupand finding way handle use cases directly within application detailed example turnup automation google problematic ends maintained separately core system therefore suffers bit rot ie changing underlying systems change despite best intentions attempting tightly couple two turnup automation core system often fails due unaligned priorities product developers unreasonably resist test deployment requirement every change secondly automation crucial executed infrequent intervals therefore difficult test often particularly fragile extended feedback cycle cluster failover one classic example infrequently executed automation failovers might occur every months infrequently enough inconsistencies instances introduced evolution automation follows path automation database master failed manually locations externally maintained systemspecific automation sre failover script home directory externally maintained generic automation sre adds database support generic failover script everyone uses internally maintained systemspecific automation database ships failover script systems need automation database notices problems automatically fails without human intervention sre hates manual operations obviously try create systems require however sometimes manual operations unavoidable additionally subvariety automation applies changes across domain specific systemrelated configuration across domain production whole highly centralized proprietary production environment like google large number changes non servicespecific scopeeg changing upstream chubby servers flag change bigtable client library make access reliable onwhich nonetheless need safely managed rolled back necessary beyond certain volume changes infeasible productionwide changes accomplished manually time point waste manual oversight process large proportion changes either trivial accomplished successfully basic relaunchandcheck strategies let use internal case studies illustrate preceding points detail first case study due diligent farsighted work managed achieve selfprofessed nirvana sre automate job automate job automate things long ads products google stored data mysql database ads data obviously high reliability requirements sre team charged looking infrastructure ads database mostly ran considered mature managed state example automated away worst routine work standard replica replacements believed ads database well managed harvested lowhanging fruit terms optimization scale however daily operations became comfortable team members began look next level system development migrating mysql onto google cluster scheduling system borg hoped migration would provide two main benefits completely eliminate machinereplica maintenance borg would automatically handle setuprestart new broken tasks enable binpacking multiple mysql instances physical machine borg would enable efficient use machine resources via containers late successfully deployed proof concept mysql instance borg unfortunately accompanied significant new difficulty core operating characteristic borg tasks move around automatically tasks commonly move within borg frequently twice per week frequency tolerable database replicas unacceptable masters time process master failover took minutes per instance simply ran shared machines subject reboots kernel upgrades addition normal rate machine failure expect number otherwise unrelated failovers every week factor combination number shards system hosted meant manual failovers would consume substantial amount human hours would give us bestcase availability uptime fell short actual business requirements product order meet error budgets failover would take less seconds downtime way optimize humandependent procedure make downtime shorter seconds therefore choice automate failover actually needed automate failover ads sre completed automated failover daemon dubbed decider decider could complete mysql failovers planned unplanned failovers less seconds time creation decider mysql borg mob finally became reality graduated optimizing infrastructure lack failover embracing idea failure inevitable therefore optimizing recover quickly automation automation let us achieve highly available mysql world forced two restarts per week come set costs applications changed include significantly failurehandling logic given norm mysql development world assume mysql instance stable component stack switch meant customizing software like jdbc tolerant failureprone environment however benefits migrating mob decider well worth costs mob time team spent mundane operational tasks dropped failovers automated outage single database task longer paged human main upshot new automation lot free time spend improving parts infrastructure improvements cascading effect time saved time able spend optimizing automating tedious work eventually able automate schema changes causing cost total operational maintenance ads database drop nearly might say successfully automated job hardware side domain also saw improvement migrating mob freed considerable resources could schedule multiple mysql instances machines improved utilization hardware total able free hardware team flush hardware engineering resources example demonstrates wisdom going extra mile deliver platform rather replacing existing manual procedures next example comes cluster infrastructure group illustrates difficult tradeoffs might encounter way automating things soothing pain applying automation cluster turnups ten years ago cluster infrastructure sre team seemed get new hire every months turned approximately frequency turned new cluster turning service new cluster gives new hires exposure service internals task seemed like natural useful training tool steps taken get cluster ready use something like following fit datacenter building power cooling install configure core switches connections backbone install initial racks servers configure basic services dns installers configure lock service storage computing deploy remaining racks machines assign userfacing services resources teams set services steps extremely complex basic services like dns relatively simple storage compute subsystems time still heavy development new flags components optimizations added weekly services hundred different component subsystems complex web dependencies failing configure one subsystem configuring system component differently deployments customerimpacting outage waiting happen one case multipetabyte bigtable cluster configured use first logging disk disk systems latency reasons year later automation assumed machine first disk used machine storage configured therefore safe wipe machine set scratch bigtable data wiped instantly thankfully multiple realtime replicas dataset surprises unwelcome automation needs careful relying implicit safety signals early automation focused accelerating cluster delivery approach tended rely upon creative use ssh tedious package distribution service initialization problems strategy initial win freeform scripts became cholesterol technical debt detecting inconsistencies prodtest numbers clusters grew clusters required handtuned flags settings result teams wasted time chasing difficulttospot misconfigurations flag made gfs responsive log processing leaked default templates cells many files could run memory load infuriating timeconsuming misconfigurations crept nearly every large configuration change creativethough brittleshell scripts used configure clusters neither scaling number people wanted make changes sheer number cluster permutations needed built shell scripts also failed resolve significant concerns declaring service good take customerfacing traffic service dependencies available correctly configured configurations packages consistent deployments could team confirm every configuration exception desired prodtest production test ingenious solution unwelcome surprises extended python unit test framework allow unit testing realworld services unit tests dependencies allowing chain tests failure one test would quickly abort take test shown figure example figure prodtest dns service showing one failed test aborts subsequent chain tests given team prodtest given cluster name could validate team services cluster later additions allowed us generate graph unit tests states functionality allowed engineer see quickly service correctly configured clusters graph highlighted failed step failing python unit test output verbose error message time team encountered delay due another team unexpected misconfiguration bug could filed extend prodtest ensured similar problem would discovered earlier future sres proud able assure customers servicesboth newly turned services existing services new configurationwould reliably serve production traffic first time project managers could predict cluster could go live complete understanding clusters took six weeks go networkready serving live traffic blue sre received mission senior management three months five new clusters reach networkready day please turn one week resolving inconsistencies idempotently one week turnup terrifying mission tens thousands lines shell script owned dozens teams could quickly tell unprepared given cluster fixing meant dozens teams would file hundreds bugs hope bugs would promptly fixed realized evolving python unit tests finding misconfigurations python code fixing misconfigurations could enable us fix issues faster unit test already knew cluster examining specific test failing paired test fix fix written idempotent could assume dependencies met resolving problem easyand safeto resolve requiring idempotent fixes meant teams could run fix script every minutes without fearing damage cluster configuration dns team test blocked machine database team configuration new cluster soon cluster appeared database dns team tests fixes would start working take test shown figure example testdnsmonitoringconfigexists fails shown call fixdnsmonitoringcreateconfig scrapes configuration database checks skeleton configuration file revision control system testdnsmonitoringconfigexists passes retry testdnsmonitoringconfigpushed test attempted test fails fixdnsmonitoringpushconfig step runs fix fails multiple times automation assumes fix failed stops notifying user armed scripts small group engineers could ensure could go network works machines listed database serving websearch ads traffic matter week two time seemed apex automation technology looking back approach deeply flawed latency test fix second test introduced flaky tests sometimes worked sometimes failed fixes naturally idempotent flaky test followed fix might render system inconsistent state figure prodtest dns service showing one failed test resulted running one fix inclination specialize automation processes vary three respects competence ie accuracy latency quickly steps executed initiated relevance proportion realworld process covered automation began process highly competent maintained run service owners highlatency service owners performed process spare time assigned new engineers relevant service owners knew real world changed could fix automation reduce turnup latency many service owning teams instructed single turnup team automation run turnup team used tickets start stage turnup could track remaining tasks tasks assigned human interactions regarding automation modules occurred people room cluster turnups could happen much shorter time finally competent accurate timely automation process state last long real world chaotic software configuration data etc changed resulting thousand separate changes day affected systems people affected automation bugs longer domain experts automation became less relevant meaning new steps missed less competent new flags might caused automation fail however took drop quality impact velocity automation code like unit test code dies maintaining team obsessive keeping code sync codebase covers world changes around code dns team adds new configuration options storage team changes package names networking team needs support new devices relieving teams ran services responsibility maintain run automation code created ugly organizational incentives team whose primary task speed current turnup incentive reduce technical debt serviceowning team running service production later team running automation incentive build systems easy automate product manager whose schedule affected lowquality automation always prioritize new features simplicity automation functional tools usually written use similar argument applies product development teams benefit keeping least operational awareness systems production turnups highlatency inaccurate incompetentthe worst worlds however unrelated security mandate allowed us trap much distributed automation relied time ssh clumsy security perspective people must root many machines run commands growing awareness advanced persistent security threats drove us reduce privileges sres enjoyed absolute minimum needed jobs replace use sshd authenticated acldriven rpcbased local admin daemon also known admin servers permissions perform local changes result one could install modify server without audit trail changes local admin daemon package repo gated code reviews making difficult someone exceed authority giving someone access install packages would let view colocated logs admin server logged rpc requestor parameters results rpcs enhance debugging security audits serviceoriented clusterturnup next iteration admin servers became part service teams workflows related machinespecific admin servers installing packages rebooting clusterlevel admin servers actions like draining turning service sres moved writing shell scripts home directories building peerreviewed rpc servers finegrained acls later realization turnup processes owned teams owned services fully sank saw way approach cluster turnup serviceoriented architecture soa problem service owners would responsible creating admin server handle cluster turnupturndown rpcs sent system knew clusters ready turn team would provide contract api turnup automation needed still free change underlying implementation cluster reached networkready automation sent rpc admin server played part turning cluster lowlatency competent accurate process importantly process stayed strong rate change number teams number services seem double year mentioned earlier evolution turnup automation followed path operatortriggered manual action automation operatorwritten systemspecific automation externally maintained generic automation internally maintained systemspecific automation autonomous systems need human intervention evolution broadly speaking success borg case study illustrates another way come think problem automation borg birth warehousescale computer another way understand development attitude toward automation automation best deployed consider history development cluster management systems like mysql borg demonstrated success converting manual operations automatic ones cluster turnup process demonstrated downside thinking carefully enough automation implemented developing cluster management also ended demonstrating another lesson automation done like previous two examples something quite sophisticated created eventual result continuous evolution simpler beginnings google clusters initially deployed much like everyone else small networks time racks machines specific purposes heterogeneous configurations engineers would log wellknown master machine perform administrative tasks golden binaries configuration lived masters one colo provider naming logic implicitly assumed location production grew began use multiple clusters different domains cluster names entered picture became necessary file describing machine grouped machines loose naming strategy descriptor file combination equivalent parallel ssh allowed us reboot example search machines one go around time common get tickets like search done machine x crawl machine now automation development began initially automation consisted simple python scripts operations following service management keeping services running eg restarts segfaults tracking services supposed run machines log message parsing sshing machine looking regexps automation eventually mutated proper database tracked machine state also incorporated sophisticated monitoring tools union set automation available could automatically manage much lifecycle machines noticing machines broken removing services sending repair restoring configuration came back repair take step back automation useful yet profoundly limited due fact abstractions system relentlessly tied physical machines needed new approach hence borg ver born system moved away relatively static hostportjob assignments previous world toward treating collection machines managed sea resources central successand conceptionwas notion turning cluster management entity api calls could issued central coordinator liberated extra dimensions efficiency flexibility reliability unlike previous model machine ownership borg could allow machines schedule example batch userfacing tasks machine functionality ultimately resulted continuous automatic operating system upgrades small amount constant efforteffort scale total size production deployments slight deviations machine state automatically fixed brokenness lifecycle management essentially noops sre point thousands machines born die go repairs daily sre effort echo words ben treynor sloss taking approach software problem initial automation bought us enough time turn cluster management something autonomous opposed automated achieved goal bringing ideas related data distribution apis hubandspoke architectures classic distributed system software development bear upon domain infrastructure management interesting analogy possible make direct mapping single machine case development cluster management abstractions view rescheduling another machine looks lot like process moving one cpu another course compute resources happen end network link extent actually matter thinking terms rescheduling looks like intrinsic feature system rather something one would automate humans react fast enough anyway similarly case cluster turnup metaphor cluster turnup simply additional schedulable capacity bit like adding disk ram single computer however singlenode computer general expected continue operating large number components fail global computer isit must selfrepairing operate grows past certain size due essentially statistically guaranteed large number failures taking place every second implies move systems hierarchy manually triggered automatically triggered autonomous capacity selfintrospection necessary survive reliability fundamental feature course effective troubleshooting details internal operation introspection relies upon also exposed humans managing overall system analogous discussions impact automation noncomputer domainfor example airplane flight industrial applications often point downside highly effective automation human operators progressively relieved useful direct contact system automation covers daily activities time inevitably situation arises automation fails humans unable successfully operate system fluidity reactions lost due lack practice mental models system longer reflect reality doing situation arises system nonautonomousie automation replaces manual actions manual actions presumed always performable available sadly time ultimately becomes false manual actions always performable functionalityto permit longer exists experienced situations automation actively harmful number occasionssee automation enabling failure scale but google experience systems automation autonomous behavior longer optional extras scale course case still strong arguments autonomous behavior systems irrespective size reliability fundamental feature autonomous resilient behavior one useful way get recommendations might read examples chapter decide need googlescale anything automation whatsoever untrue two reasons automation provides time saving worth implementing cases simple timeexpended versus timesaved calculation might suggest approach highest leverage actually occurs design phase shipping iterating rapidly might allow implement functionality faster yet rarely makes resilient system autonomous operation difficult convincingly retrofit sufficiently large systems standard good practices software engineering help considerably decoupled subsystems introducing apis minimizing side effects automation enabling failure scale google runs dozen large datacenters also depend machines many thirdparty colocation facilities colos machines colos used terminate incoming connections cache content delivery network order lower enduser latency point time number racks installed decommissioned processes largely automated one step decommission involves overwriting full content disk machines rack point independent system verifies successful erase call process diskerase upon time automation charge decommissioning particular rack failed diskerase step completed successfully later decommission process restarted beginning debug failure iteration trying send set machines rack diskerase automation determined set machines still needed diskerased correctly empty unfortunately empty set used special value interpreted mean everything means automation sent almost machines colos diskerase within minutes highly efficient diskerase wiped disks machines cdn machines longer able terminate connections users anything else useful still able serve users datacenters minutes effect visible externally slight increase latency far could tell users noticed problem thanks good capacity planning least got right meanwhile spent better part two days reinstalling machines affected colo racks spent following weeks auditing adding sanity checksincluding rate limitinginto automation making decommission workflow idempotent for readers already feel precisely understand value automation skip ahead value google sre however note description contains nuances might useful keep mind reading rest chapter the expertise acquired building automation also valuable engineers deeply understand existing processes automated later automate novel processes quickly see following xkcd cartoon http xkcdcom see example http blogengineyardcompetsvscattle of course every system needs managed actually provides callable apis managementforcing tooling use eg cli invocations automated website clicks we compressed simplified history aid understanding as small unchanging number see eg https enwikipediaorgwikiairfranceflight see eg bai sar this yet another good reason regular practice drills see disaster role playing chapter release engineering written dinah mcnutt edited betsy beyer tim harvey release engineering relatively new fastgrowing discipline software engineering concisely described building delivering software mcna release engineers solid expert understanding source code management compilers build configuration languages automated build tools package managers installers skill set includes deep knowledge multiple domains development configuration management test integration system administration customer support running reliable services requires reliable release processes site reliability engineers sres need know binaries configurations use built reproducible automated way releases repeatable unique snowflakes changes aspect release process intentional rather accidental sres care process source code deployment release engineering specific job function google release engineers work software engineers swes product development sres define steps required release softwarefrom software stored source code repository build rules compilation testing packaging deployment conducted role release engineer google datadriven company release engineering follows suit tools report host metrics much time takes code change deployed production words release velocity statistics features used build configuration files ada tools envisioned developed release engineers release engineers define best practices using tools order make sure projects released using consistent repeatable methodologies best practices cover elements release process examples include compiler flags formats build identification tags required steps build making sure tools behave correctly default adequately documented makes easy teams stay focused features users rather spending time reinventing wheel poorly comes releasing software google large number sres charged safely deploying products keeping google services running order make sure release processes meet business requirements release engineers sres work together develop strategies canarying changes pushing new releases without interrupting services rolling back features demonstrate problems philosophy release engineering guided engineering service philosophy expressed four major principles detailed following sections selfservice model order work scale teams must selfsufficient release engineering developed best practices tools allow product development teams control run release processes although thousands engineers products achieve high release velocity individual teams decide often release new versions products release processes automated point require minimal involvement engineers many projects automatically built released using combination automated build system deployment tools releases truly automatic require engineer involvement problems arise high velocity userfacing software many components google search rebuilt frequently aim roll customerfacing features quickly possible embraced philosophy frequent releases result fewer changes versions approach makes testing troubleshooting easier teams perform hourly builds select version actually deploy production resulting pool builds selection based upon test results features contained given build teams adopted push green release model deploy every build passes tests kle hermetic builds build tools must allow us ensure consistency repeatability two people attempt build product revision number source code repository different machines expect identical results builds hermetic meaning insensitive libraries software installed build machine instead builds depend known versions build tools compilers dependencies libraries build process selfcontained must rely services external build environment rebuilding older releases need fix bug software running production challenge accomplish task rebuilding revision original build including specific changes submitted point time call tactic cherry picking build tools versioned based revision source code repository project built therefore project built last month use month version compiler cherry pick required version may contain incompatible undesired features enforcement policies procedures several layers security access control determine perform specific operations releasing project gated operations include approving source code changesthis operation managed configuration files scattered throughout codebase specifying actions performed release process creating new release approving initial integration proposal request perform build specific revision number source code repository subsequent cherry picks deploying new release making changes project build configuration almost changes codebase require code review streamlined action integrated normal developer workflow automated release system produces report changes contained release archived build artifacts allowing sres understand changes included new release project report expedite troubleshooting problems release continuous build deployment google developed automated release system called rapid rapid system leverages number google technologies provide framework delivers scalable hermetic reliable releases following sections describe software lifecycle google managed using rapid associated tools building blaze google build tool choice supports building binaries range languages including standard languages c java python go javascript engineers use blaze define build targets eg output build jar file specify dependencies target kem performing build blaze automatically builds dependency targets build targets binaries unit tests defined rapid project configuration files projectspecific flags unique build identifier passed rapid blaze binaries support flag displays build date revision number build identifier allow us easily associate binary record built branching code checked main branch source code tree mainline however major projects release directly mainline instead branch mainline specific revision never merge changes branch back mainline bug fixes submitted mainline cherry picked branch inclusion release practice avoids inadvertently picking unrelated changes submitted mainline since original build occurred using branch cherry pick method know exact contents release testing continuous test system runs unit tests code mainline time change submitted allowing us detect build test failures quickly release engineering recommends continuous build test targets correspond test targets gate project release also recommend creating releases revision number version last continuous test build successfully completed tests measures decrease chance subsequent changes made mainline cause failures build performed release time release process rerun unit tests using release branch create audit trail showing tests passed step important release involves cherry picks release branch may contain version code exist anywhere mainline want guarantee tests pass context actually released complement continuous test system use independent testing environment runs systemlevel tests packaged build artifacts tests launched manually rapid packaging software distributed production machines via midas package manager mpm mcnc mpm assembles packages based blaze rules list build artifacts include along owners permissions packages named eg searchshakespearefrontend versioned unique hash signed ensure authenticity mpm supports applying labels particular version package rapid applies label containing build id guarantees package uniquely referenced using name package label labels applied mpm package indicate package location release process eg dev canary production apply existing label new package label automatically moved old package new package example package labeled canary someone subsequently installing canary version package automatically receive newest version package label canary rapid figure shows main components rapid system rapid configured files called blueprints blueprints written internal configuration language used define build test targets rules deployment administrative information like project owners rolebased access control lists determine perform specific actions rapid project figure simplified view rapid architecture showing main components system rapid project workflows define actions perform release process workflow actions performed serially parallel workflow launch workflows rapid dispatches work requests tasks running borg job production servers rapid uses production infrastructure handle thousands release requests simultaneously typical release process proceeds follows rapid uses requested integration revision number often obtained automatically continuous test system create release branch rapid uses blaze compile binaries execute unit tests often performing two steps parallel compilation testing occur environments dedicated specific tasks opposed taking place borg job rapid workflow executing separation allows us parallelize work easily build artifacts available system testing canary deployments typical canary deployment involves starting jobs production environment completion system tests results step process logged report changes since last release created rapid allows us manage release branches cherry picks individual cherry pick requests approved rejected inclusion release deployment rapid often used drive simple deployments directly updates borg jobs use newly built mpm packages based deployment definitions blueprint files specialized task executors complicated deployments use sisyphus generalpurpose rollout automation framework developed sre rollout logical unit work composed one individual tasks sisyphus provides set python classes extended support deployment process dashboard allows finer control rollout performed provides way monitor rollout progress typical integration rapid creates rollout longrunning sisyphus job rapid knows build label associated mpm package created specify build label creating rollout sisyphus sisyphus uses build label specify version mpm packages deployed sisyphus rollout process simple complicated necessary example update associated jobs immediately roll new binary successive clusters period several hours goal fit deployment process risk profile given service development preproduction environments may build hourly push releases automatically tests pass large userfacing services may push starting one cluster expand exponentially clusters updated sensitive pieces infrastructure may extend rollout several days interleaving across instances different geographic regions configuration management configuration management one area particularly close collaboration release engineers sres although configuration management may initially seem deceptively simple problem configuration changes potential source instability result approach releasing managing system service configurations evolved substantially time today use several models distributing configuration files described following paragraphs schemes involve storing configuration primary source code repository enforcing strict code review requirement use mainline configuration first method used configure services borg systems predated borg using scheme developers sres modify configuration files head main branch changes reviewed applied running system result binary releases configuration changes decoupled conceptually procedurally simple technique often leads skew checkedin version configuration files running version configuration file jobs must updated order pick changes include configuration files binaries mpm package projects configuration files projects files subset files change release cycle configuration files included mpm package binaries strategy limits flexibility binding binary configuration files tightly simplifies deployment requires installing one package package configuration files mpm configuration packages apply hermetic principle configuration management binary configurations tend tightly bound particular versions binaries leverage build packaging systems snapshot release configuration files alongside binaries similar treatment binaries use build id reconstruct configuration specific point time example change implements new feature released flag setting configures feature generating two mpm packages one binary one configuration retain ability change package independently feature released flag setting firstfolio realize instead badquarto cherry pick change onto release branch rebuild configuration package deploy approach advantage requiring new binary build leverage mpm labeling feature indicate versions mpm packages installed together label muchado applied mpm packages described previous paragraph allows us fetch packages using label new version project built muchado label applied new packages tags unique within namespace mpm package latest package tag used read configuration files external store projects configuration files need change frequently dynamically ie binary running files stored chubby bigtable sourcebased filesystem yor summary project owners consider different options distributing managing configuration files decide works best casebycase basis conclusions chapter specifically discussed google approach release engineering ways release engineers work collaborate sres practices also applied widely googlers equipped right tools proper automation welldefined policies developers sres worry releasing software releases painless simply pressing button companies deal set release engineering problems regardless size tools use handle versioning packages use continuous build deploy model perform periodic builds often release configuration management policies use release metrics interest google release engineers developed tools necessity open sourced vendorsupplied tools work scale require custom tools allow us include functionality support even enforce release process policies however policies must first defined order add appropriate features tools companies take effort define release processes whether processes automated andor enforced start release engineering beginning release engineering often afterthought way thinking must change platforms services continue grow size complexity teams budget release engineering resources beginning product development cycle cheaper put good practices process place early rather retrofit system later essential developers sres release engineers work together release engineer needs understand intention code built deployed developers build throw results fence handled release engineers individual project teams decide release engineering becomes involved project release engineering still relatively young discipline managers always plan budget release engineering early stages project therefore considering incorporate release engineering practices sure consider role applied entire lifecycle product serviceparticularly early stages information information release engineering see following presentations video available online embracing continuous release reduced change complexity usenix release engineering summit west dic maintaining consistency massively parallel environment usenixconfiguration management summit mcn commandments release engineering nd international workshop release engineering mcnb distributing software massively parallel environment lisa mcnc google uses monolithic unified source code repository see pot blaze open sourced bazel see bazel faq bazel website http bazeliofaqhtml chapter simplicity written max luebbe edited tim harvey price reliability pursuit utmost simplicity car hoare turing award lecture software systems inherently dynamic unstable software system perfectly stable exists vacuum stop changing codebase stop introducing bugs underlying hardware libraries never change neither components introduce bugs freeze current user base never scale system fact good summary sre approach managing systems end day job keep agility stability balance system system stability versus agility sometimes makes sense sacrifice stability sake agility often approached unfamiliar problem domain conducting call exploratory codingsetting explicit shelf life whatever code write understanding need try fail order really understand task need accomplish code comes expiration date much liberal test coverage release management never shipped production seen users majority production software systems want balanced mix stability agility sres work create procedures practices tools render software reliable time sres ensure work little impact developer agility possible fact sre experience found reliable processes tend actually increase developer agility rapid reliable production rollouts make changes production easier see result bug surfaces takes less time find fix bug building reliability development allows developers focus attention really care aboutthe functionality performance software systems virtue boring unlike everything else life boring actually positive attribute comes software want programs spontaneous interesting want stick script predictably accomplish business goals words google engineer robert muth unlike detective story lack excitement suspense puzzles actually desirable property source code surprises production nemeses sre fred brooks suggests silver bullet essay bro important consider difference essential complexity accidental complexity essential complexity complexity inherent given situation removed problem definition whereas accidental complexity fluid resolved engineering effort example writing web server entails dealing essential complexity serving web pages quickly however write web server java may introduce accidental complexity trying minimize performance impact garbage collection eye towards minimizing accidental complexity sre teams push back accidental complexity introduced systems responsible constantly strive eliminate complexity systems onboard assume operational responsibility give code engineers human beings often form emotional attachment creations confrontations largescale purges source tree uncommon might protest need code later comment code easily add later gate code flag instead deleting terrible suggestions source control systems make easy reverse changes whereas hundreds lines commented code create distractions confusion especially source files continue evolve code never executed gated flag always disabled metaphorical time bomb waiting explode painfully experienced knight capital example see order matter knight capital americas llc sec risk sounding extreme consider web service expected available extent every new line code written liability sre promotes practices make likely code essential purpose scrutinizing code make sure actually drives business goals routinely removing dead code building bloat detection levels testing negative lines code metric term software bloat coined describe tendency software become slower bigger time result constant stream additional features bloated software seems intuitively undesirable negative aspects become even clear considered sre perspective every line code changed added project creates potential introducing new defects bugs smaller project easier understand easier test frequently fewer defects bearing perspective mind perhaps entertain reservations urge add new features project satisfying coding ever done deleting thousands lines code time longer useful minimal apis french poet antoine de saint exupery wrote perfection finally attained longer add longer anything take away sai principle also applicable design construction software apis particularly clear expression rule followed writing clear minimal apis essential aspect managing simplicity software system fewer methods arguments provide consumers api easier api understand effort devote making methods good possibly recurring theme appears conscious decision take certain problems allows us focus core problem make solutions explicitly set create substantially better software less small simple api usually also hallmark wellunderstood problem modularity expanding outward apis single binaries many rules thumb apply objectoriented programming also apply design distributed systems ability make changes parts system isolation essential creating supportable system specifically loose coupling binaries binaries configuration simplicity pattern simultaneously promotes developer agility system stability bug discovered one program component larger system bug fixed pushed production independent rest system modularity apis offer may seem straightforward apparent notion modularity also extends changes apis introduced single change api force developers rebuild entire system run risk introducing new bugs versioning apis allows developers continue use version system depends upon upgrade newer version safe considered way release cadence vary throughout system instead requiring full production push entire system every time feature added improved system grows complex separation responsibility apis binaries becomes increasingly important direct analogy objectoriented class design understood poor practice write grab bag class contains unrelated functions also poor practice create put production util misc binary welldesigned distributed system consists collaborators clear wellscoped purpose concept modularity also applies data formats one central strengths design goals google protocol buffers create wire format backward forward compatible release simplicity simple releases generally better complicated releases much easier measure understand impact single change rather batch changes released simultaneously release unrelated changes system time performance gets worse understanding changes impacted performance take considerable effort additional instrumentation release performed smaller batches move faster confidence code change understood isolation larger system approach releases compared gradient descent machine learning find optimum solution taking small steps time considering change results improvement degradation simple conclusion chapter repeated one theme software simplicity prerequisite reliability lazy consider might simplify step given task instead clarifying actually want accomplish might easily every time say feature restricting innovation keeping environment uncluttered distractions focus remains squarely innovation real engineering proceed this often true complex systems general see per coo coined former manager johan anderson around time became sre protocol buffers also referred protobufs languageneutral platformneutral extensible mechanism serializing structured data details see https developersgooglecomprotocolbuffersdocsoverview abitofhistory part iii practices put simply sres run servicesa set related systems operated users may internal externaland ultimately responsible health services successfully operating service entails wide range activities developing monitoring systems planning capacity responding incidents ensuring root causes outages addressed section addresses theory practice sre daytoday activity building operating large distributed computing systems characterize health servicein much way abraham maslow categorized human needs mas from basic requirements needed system function service higher levels functionpermitting selfactualization taking active control direction service rather reactively fighting fires understanding fundamental evaluate services google explicitly developed number google sres including former colleague mikey dickerson temporarily joined radically different culture united states government help launch healthcaregov late early needed way explain increase systems reliability use hierarchy illustrated figure look elements go making service reliable basic advanced figure iii service reliability hierarchy monitoring without monitoring way tell whether service even working absent thoughtfully designed monitoring infrastructure flying blind maybe everyone tries use website gets error maybe not want aware problems users notice discuss tools philosophy practical alerting timeseries data incident response sres go oncall merely sake rather oncall support tool use achieve larger mission remain touch distributed computing systems actually work fail could find way relieve carrying pager would oncall explain balance oncall duties responsibilities aware problem make go away necessarily mean fixing allmaybe stop bleeding reducing system precision turning features temporarily allowing gracefully degrade maybe direct traffic another instance service working properly details solution choose implement necessarily specific service organization responding effectively incidents however something applicable teams figuring wrong first step offer structured approach effective troubleshooting incident often tempting give adrenalin start responding ad hoc advise temptation emergency response counsel managing incidents managing incidents effectively reduce impact limit outageinduced anxiety postmortem rootcause analysis aim alerted manually solve new exciting problems presented service woefully boring fix issue fact mindset one key differentiators sre philosophy traditional operationsfocused environments theme explored two chapters building blameless postmortem culture first step understanding went wrong went right described postmortem culture learning failure related discussion tracking outages briefly describe internal tool outage tracker allows sre teams keep track recent production incidents causes actions taken response testing understand tends go wrong next step attempting prevent ounce prevention worth pound cure test suites offer assurance software making certain classes errors released production talk best use testing reliability capacity planning software engineering sre offer case study software engineering sre auxon tool automating capacity planning naturally following capacity planning load balancing ensures properly using capacity built discuss requests services get sent datacenters load balancing frontend continue discussion load balancing datacenter handling overload essential ensuring service reliability finally addressing cascading failures offer advice addressing cascading failures system design service caught cascading failure development one key aspects google approach site reliability engineering significant largescale system design software engineering work within organization managing critical state distributed consensus reliability explain distributed consensus guise paxos core many google distributed systems including globally distributed cron system distributed periodic scheduling cron outline system scales whole datacenters beyond easy task data processing pipelines discusses various forms data processing pipelines take oneshot mapreduce jobs running periodically systems operate near realtime different architectures lead surprising counterintuitive challenges making sure data stored still want read heart data integrity data integrity read wrote explain keep data safe product finally made way reliability pyramid find point workable product reliable product launches scale write google reliable product launches scale try give users best possible experience starting day zero reading google sre discussed previously testing subtle improper execution large effects overall stability acm article kri explain google performs companywide resilience testing ensure capable weathering unexpected zombie apocalypse disaster strike often thought dark art full mystifying spreadsheets divining future capacity planning nonetheless vital hixa shows actually need crystal ball right finally interesting new approach corporate network security detailed war initiative replace privileged intranets device user credentials driven sres infrastructure level definitely approach keep mind creating next network chapter practical alerting written jamie wilkinson edited kavita guliani may queries flow pager stay silent traditional sre blessing monitoring bottom layer hierarchy production needs fundamental running stable service monitoring enables service owners make rational decisions impact changes service apply scientific method incident response course ensure reason existence measure service alignment business goals see monitoring distributed systems regardless whether service enjoys sre support run symbiotic relationship monitoring tasked ultimate responsibility google production sres develop particularly intimate knowledge monitoring infrastructure supports service monitoring large system challenging couple reasons sheer number components analyze need maintain reasonably low maintenance burden engineers responsible system google monitoring systems measure simple metrics average response time unladen european web server also need understand distribution response times across web servers region knowledge enables us identify factors contributing latency tail scale systems operate alerted singlemachine failures unacceptable data noisy actionable instead try build systems robust failures systems depend rather requiring management many individual components large system designed aggregate signals prune outliers need monitoring systems allow us alert highlevel service objectives retain granularity inspect individual components needed google monitoring systems evolved course years traditional model custom scripts check responses alert wholly separated visual display trends new paradigm new model made collection timeseries firstclass role monitoring system replaced check scripts rich language manipulating timeseries charts alerts rise borgmon shortly job scheduling infrastructure borg ver created new monitoring systemborgmonwas built complement timeseries monitoring outside google chapter describes architecture programming interface internal monitoring tool foundational growth reliability google almost yearsbut help dear reader recent years monitoring undergone cambrian explosion riemann heka bosun prometheus emerged open source tools similar borgmon timeseriesbased alerting particular prometheus shares many similarities borgmon especially compare two rule languages principles variable collection rule evaluation remain across tools provide environment experiment hopefully launch production ideas inspired chapter instead executing custom scripts detect system failures borgmon relies common data exposition format enables mass data collection low overheads avoids costs subprocess execution network connection setup call whitebox monitoring see monitoring distributed systems comparison whitebox blackbox monitoring data used rendering charts creating alerts accomplished using simple arithmetic collection longer short lived process history collected data used alert computation well features help meet goal simplicity described monitoring distributed systems allow system overhead kept low people running services remain agile respond continuous change system grows facilitate mass collection metrics format standardized older method exporting internal state known varz formalized allow collection metrics single target one http fetch example view page metrics manually could use following command curl http webservervarz httprequests errorstotal borgmon collect borgmon build hierarchies follow topology service aggregating summarizing information discarding strategically level typically team runs single borgmon per cluster pair global level large services shard cluster level many scraper borgmon turn feed clusterlevel borgmon instrumentation applications varz http handler simply lists exported variables plain text spaceseparated keys values one per line later extension added mapped variable allows exporter define several labels variable name export table values histogram example mapvalued variable looks like following showing http responses http s httpresponses map code adding metric program requires single declaration code metric needed hindsight apparent schemaless textual interface makes barrier adding new instrumentation low positive software engineering sre teams however tradeoff ongoing maintenance decoupling variable definition use borgmon rules requires careful change management practice tradeoff satisfactory tools validate generate rules written well exporting variables google web roots run deep major languages used google implementation exported variable interface automagically registers http server built every google binary default instances variable exported allow server author perform obvious operations like adding amount current value setting key specific value forth go expvar library json output form variant api collection exported data find targets borgmon instance configured list targets using one many name resolution methods target list often dynamic using service discovery reduces cost maintaining allows monitoring scale predefined intervals borgmon fetches varz uri target decodes results stores values memory borgmon also spreads collection instance target list whole interval collection target lockstep peers borgmon also records synthetic variables target order identify name resolved host port target responded collection target responded health check time collection finished synthetic variables make easy write rules detect monitored tasks unavailable interesting varz quite dissimilar snmp simple network management protocol designed minimal transport requirements continue working network applications fail mic scraping targets http seems odds design principle however experience shows rarely issue system already designed robust network machine failures borgmon allows engineers write smarter alerting rules using collection failure signal storage timeseries arena service typically made many binaries running many tasks many machines many clusters borgmon needs keep data organized allowing flexible querying slicing data borgmon stores data inmemory database regularly checkpointed disk data points form timestamp value stored chronological lists called timeseries timeseries named unique set labels form namevalue presented figure timeseries conceptually onedimensional matrix numbers progressing time add permutations labels timeseries matrix becomes multidimensional figure timeseries errors labeled original host collected practice structure fixedsized block memory known timeseries arena garbage collector expires oldest entries arena full time interval recent oldest entries arena horizon indicates much queryable data kept ram typically datacenter global borgmon sized hold hours data rendering consoles much less time lowestlevel collector shards memory requirement single data point bytes fit million unique timeseries hours minute intervals gb ram periodically inmemory state archived external system known timeseries database tsdb borgmon query tsdb older data slower tsdb cheaper larger borgmon ram labels vectors shown example timeseries figure timeseries stored sequences numbers timestamps referred vectors like vectors linear algebra vectors slices crosssections multidimensional matrix data points arena conceptually timestamps ignored values inserted vector regular intervals timefor example seconds minute apart figure example timeseries name timeseries labelset implemented set labels expressed keyvalue pairs one labels variable name key appears varz page label names declared important timeseries timeseries database identifiable must minimum following labels var name variable job name given type server monitored service loosely defined collection jobs provide service users either internal external zone google convention refers location typically datacenter borgmon performed collection variable together variables appear something like following called variable expression query timeseries require specification labels search labelset returns matching timeseries vector could return vector results removing instance label preceding query one instance cluster example might result five rows vector recent value timeseries like labels added timeseries target name eg job instance target eg via mapvalued variables borgmon configuration eg annotations location relabeling borgmon rules evaluated also query timeseries time specifying duration variable expression returns last minutes history timeseries matched expression collecting data points per minute would expect return data points minute window like so rule evaluation borgmon really programmable calculator syntactic sugar enables generate alerts data collection storage components already described necessary evils make programmable calculator ultimately fit purpose monitoring system note centralizing rule evaluation monitoring system rather delegating forked subprocesses means computations run parallel many similar targets practice keeps configuration relatively small size example removing duplication code yet powerful expressiveness borgmon program code also known borgmon rules consists simple algebraic expressions compute timeseries timeseries rules quite powerful query history single timeseries ie time axis query different subsets labels many timeseries ie space axis apply many mathematical operations rules run parallel threadpool possible dependent ordering using previously defined rules input size vectors returned query expressions also determines overall runtime rule thus typically case one add cpu resources borgmon task response running slow assist detailed analysis internal metrics runtime rules exported performance debugging monitoring monitoring aggregation cornerstone rule evaluation distributed environment aggregation entails taking sum set timeseries tasks job order treat job whole sums overall rates computed example total queriespersecond rate job datacenter sum rates change query counters tip counter monotonically nondecreasing variablewhich say counters increase value gauges hand may take value like counters measure increasing values total number kilometers driven gauges show current state amount fuel remaining current speed collecting borgmonstyle data better use counters lose meaning events occur sampling intervals activity changes occur sampling intervals gauge collection likely miss activity example web server might want alert web server cluster starts serve errors percent requests think normalor technically sum rates nonhttp return codes tasks cluster divided sum rates requests tasks cluster greater value accomplished aggregating rates response codes across tasks outputting vector rates point time one code computing total error rate sum vector outputting single value cluster point time total error rate excludes code sum error computing clusterwide ratio errors requests dividing total error rate rate requests arrived outputting single value cluster point time outputs point time gets appended named variable expression creates new timeseries result able inspect history error rates error ratios time rate requests rules would written borgmon rule language following rules compute rate requests task count requests rate sum rates get aggregate rate queries cluster without instance instructs borgmon remove instance label right hand side sum without instance rate function takes enclosed expression returns total delta divided total time earliest latest values example timeseries data query results task httprequests ratem rule would look like results dc httprequests ratem rule would second rule uses first one input note instance label missing output discarded aggregation rule remained rule borgmon would able sum five rows together examples use time window dealing discrete points timeseries opposed continuous functions makes rate calculation easier performing calculus means compute rate need select sufficient number data points also deal possibility recent collections failed recall historical variable expression notation uses range m avoid missing data points caused collection errors example also uses google convention helps readability computed variable name contains colonseparated triplet indicating aggregation level variable name operation created name example lefthand variables task http requests minute rate datacenter http requests minute rate know create rate queries build also compute rate errors calculate ratio responses requests understand much useful work service compare ratio rate errors service level objective see service level objectives alert objective missed danger missed rules compute rate pertask per code label rate code compute cluster level response rate per code label sum without instance compute new cluster level rate summing non codes sum without code compute ratio rate errors rate requests calculation demonstrates convention suffixing new timeseries variable name operation created result read datacenter http errors minute ratio rates output rules might look like note preceding output shows intermediate query dc httperrors ratem rule filters non error codes though value expressions observe code label retained one removed mentioned previously borgmon rules create new timeseries results computations kept timeseries arena inspected source timeseries ability allows ad hoc querying evaluation exploration tables charts useful feature debugging oncall ad hoc queries prove useful made permanent visualizations service console alerting alerting rule evaluated borgmon result either true case alert triggered false experience shows alerts flap toggle state quickly therefore rules allow minimum duration alerting rule must true alert sent typically duration set least two rule evaluation cycles ensure missed collections cause false alert following example creates alert error ratio minutes exceeds total number errors exceeds per second rules job error m errorratiotoohigh details webserver error ratio triggervalue labels example holds ratio rate well threshold alerting rule however number errors greater moment alert active number errors exceeds alert go pending two minutes ensure transient state fire alert rule contains small template filling message containing contextual information job alert name alert numerical value triggering rule contextual information filled borgmon alert fires sent alert rpc borgmon connected centrally run service known alertmanager receives alert rpcs rule first triggers alert considered firing alertmanager responsible routing alert notification correct destination alertmanager configured following inhibit certain alerts others active deduplicate alerts multiple borgmon labelsets fanin fanout alerts based labelsets multiple alerts similar labelsets fire described monitoring distributed systems teams send pageworthy alerts oncall rotation important subcritical alerts ticket queues alerts retained informational data status dashboards comprehensive guide alert design found service level objectives sharding monitoring topology borgmon import timeseries data borgmon well one could attempt collect tasks service globally quickly becomes scaling bottleneck introduces single point failure design instead streaming protocol used transmit timeseries data borgmon saving cpu time network bytes compared textbased varz format typical deployment uses two global borgmon toplevel aggregation one borgmon datacenter monitor jobs running location google divides production network zones production changes two global replicas provides diversity face maintenance outages otherwise single point failure shown figure complicated deployments shard datacenter borgmon purely scrapingonly layer often due ram cpu constraints single borgmon large services dc aggregation layer performs mostly rule evaluation aggregation sometimes global layer split rule evaluation dashboarding uppertier borgmon filter data want stream lowertier borgmon global borgmon fill arena pertask timeseries lower tiers thus aggregation hierarchy builds local caches relevant timeseries drilled required figure data flow model hierarchy borgmon three clusters blackbox monitoring borgmon whitebox monitoring systemit inspects internal state target service rules written knowledge internals mind transparent nature model provides great power identify quickly components failing queues full bottlenecks occur responding incident testing new feature deployment however whitebox monitoring provide full picture system monitored relying solely upon whitebox monitoring means aware users see see queries arrive target queries never make due dns error invisible queries lost due server crash never make sound alert failures expected teams google solve coverage issue prober runs protocol check target reports success failure prober send alerts directly alertmanager varz collected borgmon prober validate response payload protocol eg html contents http response validate contents expected even extract export values timeseries teams often use prober export histograms response times operation type payload size slice dice uservisible performance prober hybrid checkandtest model richer variable extraction create timeseries prober pointed either frontend domain behind load balancer using targets detect localized failures suppress alerts example might monitor load balanced wwwgooglecom web servers datacenter behind load balancer setup allows us either know traffic still served datacenter fails quickly isolate edge traffic flow graph failure occurred maintaining configuration borgmon configuration separates definition rules targets monitored means sets rules applied many targets instead writing nearly identical configuration separation concerns might seem incidental greatly reduces cost maintaining monitoring avoiding lots repetition describing target systems borgmon also supports language templates macrolike system enables engineers construct libraries rules reused functionality reduces repetition thus reducing likelihood bugs configuration course highlevel programming environment creates opportunity complexity borgmon provides way build extensive unit regression tests synthesizing timeseries data order ensure rules behave author thinks production monitoring team runs continuous integration service executes suite tests packages configuration ships configuration borgmon production validate configuration accepting vast library common templates created two classes monitoring configuration emerged first class simply codifies emergent schema variables exported given library code user library reuse template varz templates exist http server library memory allocation storage client library generic rpc services among others varz interface declares schema rule library associated code library ends declaring schema second class library emerged built templates manage aggregation data singleserver task global service footprint libraries contain generic aggregation rules exported variables engineers use model topology service example service may provide single global api homed many datacenters within datacenter service composed several shards shard composed several jobs various numbers tasks engineer model breakdown borgmon rules debugging subcomponents isolated rest system groupings typically follow shared fate components eg individual tasks share fate due configuration files jobs shard share fate homed datacenter physical sites share fate due networking labeling conventions make division possible borgmon adds labels indicating target instance name shard datacenter occupies used group aggregate timeseries together thus multiple uses labels timeseries though interchangeable labels define breakdowns data eg http response code httpresponses variable labels define source data eg instance job name labels indicate locality aggregation data within service whole eg zone label describing physical location shard label describing logical grouping tasks templated nature libraries allows flexibility use template used aggregate tier ten years on borgmon transposed model checkandalert per target mass variable collection centralized rule evaluation across timeseries alerting diagnostics decoupling allows size system monitored scale independently size alerting rules rules cost less maintain abstracted common timeseries format new applications come ready metric exports components libraries link welltraveled aggregation console templates reduces burden implementation ensuring cost maintenance scales sublinearly size service key making monitoring sustaining operations work maintainable theme recurs sre work sres work scale aspects work global scale ten years long time though course today shape monitoring landscape within google evolved experiments changes striving continual improvement company grows even though borgmon remains internal google idea treating timeseries data data source generating alerts accessible everyone open source tools like prometheus riemann heka bosun probably others time read prometheus open source monitoring timeseries database system available http prometheusio google born usa pronounce varzee the plural borgmon borgmon like sheep many nonsre teams use generator stamp initial boilerplate ongoing updates find generator much easier use though less powerful directly editing rules many applications use service protocol export internal state well openldap exports cnmonitor subtree mysql report state show variables query apache modstatus handler https golangorgpkgexpvar the borg name system bns described production environment google viewpoint sre recall monitoring distributed systems distinction alerting symptoms causes this hour horizon magic number aims enough information debugging incident ram fast queries without costing much ram the service zone labels elided space present returned expression computing sum rates instead rate sums defends result counter resets missing data perhaps due task restart failed collection data despite untyped majority varz simple counters borgmon rate function handles corner cases counter resets the service zone labels elided space the service zone labels elided space chapter oncall written andrea spadaccini edited kavita guliani oncall critical duty many operations engineering teams must undertake order keep services reliable available however several pitfalls organization oncall rotations responsibilities lead serious consequences services teams avoided chapter describes primary tenets approach oncall google site reliability engineers sres developed years explains approach led reliable services sustainable workload time introduction several professions require employees perform sort oncall duty entails available calls working nonworking hours context oncall activities historically performed dedicated ops teams tasked primary responsibility keeping service responsible good health many important services google eg search ads gmail dedicated teams sres responsible performance reliability services thus sres oncall services support sre teams quite different purely operational teams place heavy emphasis use engineering approach problems problems typically fall operational domain exist scale would intractable without software engineering solutions enforce type problem solving google hires people diverse background systems software engineering sre teams cap amount time sres spend purely operational work minimum sre time allocated engineering projects scale impact team automation addition improving service life oncall engineer section describes typical activities oncall engineer provides background rest chapter guardians production systems oncall engineers take care assigned operations managing outages affect team performing andor vetting production changes oncall engineer available perform operations production systems within minutes according paging response times agreed team business system owners typical values minutes userfacing otherwise highly timecritical services minutes less timesensitive systems company provides pagereceiving device typically phone google flexible alert delivery systems dispatch pages via multiple mechanisms email sms robot call app across multiple devices response times related desired service availability demonstrated following simplistic example userfacing system must obtain nines availability given quarter allowed quarterly downtime around minutes availability table constraint implies reaction time oncall engineers order minutes strictly speaking minutes systems relaxed slos reaction time order tens minutes soon page received acknowledged oncall engineer expected triage problem work toward resolution possibly involving team members escalating needed nonpaging production events lower priority alerts software releases also handled andor vetted oncall engineer business hours activities less urgent paging events take priority almost every task including project work insight interrupts nonpaging events contribute operational load see dealing interrupts many teams primary secondary oncall rotation distribution duties primary secondary varies team team one team might employ secondary fallthrough pages primary oncall misses another team might specify primary oncall handles pages secondary handles nonurgent production activities teams secondary rotation strictly required duty distribution common two related teams serve secondary oncall fallthrough handling duties setup eliminates need exclusive secondary oncall rotation many ways organize oncall rotations detailed analysis refer oncall chapter lim balanced oncall sre teams specific constraints quantity quality oncall shifts quantity oncall calculated percent time spent engineers oncall duties quality oncall calculated number incidents occur oncall shift sre managers responsibility keeping oncall workload balanced sustainable across two axes balance quantity strongly believe e sre defining characteristic organization strive invest least sre time engineering remainder spent oncall leaving another types operational nonproject work using oncall rule derive minimum number sres required sustain oncall rotation assuming always two people oncall primary secondary different duties minimum number engineers needed oncall duty singlesite team eight assuming weeklong shifts engineer oncall primary secondary one week every month dualsite teams reasonable minimum size team six honor rule ensure substantial critical mass engineers team service entails enough work justify growing singlesite team prefer create multisite team multisite team advantageous two reasons night shifts detrimental effects people health dur multisite follow sun rotation allows teams avoid night shifts altogether limiting number engineers oncall rotation ensures engineers lose touch production systems see treacherous enemy operational underload however multisite teams incur communication coordination overhead therefore decision go multisite singlesite based upon tradeoffs option entails importance system workload system generates balance quality oncall shift engineer sufficient time deal incidents followup activities writing postmortems loo let define incident sequence events alerts related root cause would discussed part postmortem found average dealing tasks involved oncall incidentrootcause analysis remediation followup activities like writing postmortem fixing bugstakes hours follows maximum number incidents per day per hour oncall shift order stay within upper bound distribution paging events flat time likely median value given component issue causes pages every day median incidentsday likely something else break point thus causing incidents permitted limit temporarily exceeded eg quarter corrective measures put place make sure operational load returns sustainable state see operational overload embedding sre recover operational overload compensation adequate compensation needs considered outofhours support different organizations handle oncall compensation different ways google offers timeoffinlieu straight cash compensation capped proportion overall salary compensation cap represents practice limit amount oncall work taken individual compensation structure ensures incentivization involved oncall duties required team also promotes balanced oncall work distribution limits potential drawbacks excessive oncall work burnout inadequate time project work feeling safe mentioned earlier sre teams support google critical systems sre oncall typically means assuming responsibility userfacing revenuecritical systems infrastructure required keep systems running sre methodology thinking tackling problems vital appropriate operation services modern research identifies two distinct ways thinking individual may consciously subconsciously choose faced challenges kah intuitive automatic rapid action rational focused deliberate cognitive functions one dealing outages related complex systems second options likely produce better results lead wellplanned incident handling make sure engineers appropriate frame mind leverage latter mindset important reduce stress related oncall importance impact services consequences potential outages create significant pressure oncall engineers damaging wellbeing individual team members possibly prompting sres make incorrect choices endanger availability service stress hormones like cortisol corticotropinreleasing hormone crh known cause behavioral consequencesincluding fearthat impair cognitive functions cause suboptimal decision making chr influence stress hormones deliberate cognitive approach typically subsumed unreflective unconsidered immediate action leading potential abuse heuristics heuristics tempting behaviors one oncall example alert pages fourth time week previous three pages initiated external infrastructure system extremely tempting exercise confirmation bias automatically associating fourth occurrence problem previous cause intuition quick reactions seem like desirable traits middle incident management downsides intuition wrong often less supportable obvious data thus following intuition lead engineer waste time pursuing line reasoning incorrect start quick reactions deeprooted habit habitual responses unconsidered means disastrous ideal methodology incident management strikes perfect balance taking steps desired pace enough data available make reasonable decision simultaneously critically examining assumptions important oncall sres understand rely several resources make experience oncall less daunting may seem important oncall resources clear escalation paths welldefined incidentmanagement procedures blameless postmortem culture loo all developer teams sresupported systems usually participate oncall rotation always possible escalate partner teams necessary appropriate escalation outages generally principled way react serious outages significant unknown dimensions one handling incidents issue complex enough involve multiple teams investigation yet possible estimate upper bound incident time span useful adopt formal incidentmanagement protocol google sre uses protocol described managing incidents offers easytofollow welldefined set steps aid oncall engineer rationally pursue satisfactory incident resolution required help protocol internally supported webbased tool automates incident management actions handing roles recording communicating status updates tool allows incident managers focus dealing incident rather spending time cognitive effort mundane actions formatting emails updating several communication channels finally incident occurs important evaluate went wrong recognize went well take action prevent errors recurring future sre teams must write postmortems significant incidents detail full timeline events occurred focusing events rather people postmortems provide significant value rather placing blame individuals derive value systematic analysis production incidents mistakes happen software make sure make mistakes possible recognizing automation opportunities one best ways prevent human errors loo avoiding inappropriate operational load mentioned balanced oncall sres spend time operational work happens operational activities exceed limit operational overload sre team leadership responsible including concrete objectives quarterly work planning order make sure workload returns sustainable levels temporarily loaning experienced sre overloaded team discussed embedding sre recover operational overload provide enough breathing room team make headway addressing issues ideally symptoms operational overload measurable goals quantified eg number daily tickets paging events per shift misconfigured monitoring common cause operational overload paging alerts aligned symptoms threaten service slos paging alerts also actionable lowpriority alerts bother oncall engineer every hour frequently disrupt productivity fatigue alerts induce also cause serious alerts treated less attention necessarysee dealing interrupts discussion also important control number alerts oncall engineers receive single incident sometimes single abnormal condition generate several alerts important regulate alert fanout ensuring related alerts grouped together monitoring alerting system reason duplicate uninformative alerts generated incident silencing alerts provide necessary quiet oncall engineer focus incident noisy alerts systematically generate one alert per incident tweaked approach alertincident ratio allows oncall engineer focus incident instead triaging duplicate alerts sometimes changes cause operational overload control sre teams example application developers might introduce changes cause system noisy less reliable case appropriate work together application developers set common goals improve system extreme cases sre teams may option give back pager sre ask developer team exclusively oncall system meets standards sre team question giving back pager happen frequently almost always possible work developer team reduce operational load make given system reliable cases though complex architectural changes spanning multiple quarters might required make system sustainable operational point view cases sre team subject excessive operational load instead appropriate negotiate reorganization oncall responsibilities development team possibly routing paging alerts developer oncall solution typically temporary measure time sre developer teams work together get service shape onboarded sre team possibility renegotiating oncall responsibilities sre product development teams attests balance powers teams working relationship also exemplifies healthy tension two teams values representreliability versus feature velocityis typically resolved greatly benefiting service extension company whole treacherous enemy operational underload oncall quiet system blissful happens system quiet sres oncall often enough operational underload undesirable sre team touch production long periods time lead confidence issues terms overconfidence underconfidence knowledge gaps discovered incident occurs counteract eventuality sre teams sized allow every engineer oncall least twice quarter thus ensuring team member sufficiently exposed production wheel misfortune exercises discussed accelerating sres oncall beyond also useful team activities help hone improve troubleshooting skills knowledge service google also companywide annual disaster recovery event called dirt disaster recovery training combines theoretical practical drills perform multiday testing infrastructure systems individual services see kri conclusions approach oncall described chapter serves guideline sre teams google key fostering sustainable manageable work environment google approach oncall enabled us use engineering work primary means scale production responsibilities maintain high reliability availability despite increasing complexity number systems services sres responsible approach might immediately applicable contexts engineers need oncall services believe represents solid model organizations adopt scaling meet growing volume oncall work an earlier version chapter appeared article login october vol for discussion natural tension sre product development teams see introduction chapter effective troubleshooting written chris jones warned expert understanding system supposed work expertise gained investigating system nt work brian redman ways things go right special cases ways things go wrong john allspaw troubleshooting critical skill anyone operates distributed computing systemsespecially sresbut often viewed innate skill people others one reason assumption troubleshoot often ingrained process explaining troubleshoot difficult much like explaining ride bike however believe troubleshooting learnable teachable novices often tripped troubleshooting exercise ideally depends upon two factors understanding troubleshoot generically ie without particular system knowledge solid knowledge system investigate problem using generic process derivation first principles usually find approach less efficient less effective understanding things supposed work knowledge system typically limits effectiveness sre new system little substitute learning system designed built let look general model troubleshooting process readers expertise troubleshooting may quibble definitions process method effective reason stick theory theory formally think troubleshooting process application hypotheticodeductive method given set observations system theoretical basis understanding system behavior iteratively hypothesize potential causes failure try test hypotheses idealized model figure start problem report telling us something wrong system look system telemetry logs understand current state information combined knowledge system built operate failure modes enables us identify possible causes figure process troubleshooting test hypotheses one two ways compare observed state system theories find confirming disconfirming evidence cases actively treat system change system controlled wayand observe results second approach refines understanding system state possible cause reported problems using either strategies repeatedly test root cause identified point take corrective action prevent recurrence write postmortem course fixing proximate cause always wait rootcausing postmortem writing common pitfalls ineffective troubleshooting sessions plagued problems triage examine diagnose steps often lack deep system understanding following common pitfalls avoid looking symptoms relevant misunderstanding meaning system metrics wild goose chases often result misunderstanding change system inputs environment safely effectively test hypotheses coming wildly improbable theories wrong latching causes past problems reasoning since happened must happening hunting spurious correlations actually coincidences correlated shared causes fixing first second common pitfalls matter learning system question becoming experienced common patterns used distributed systems third trap set logical fallacies avoided remembering failures equally probableas doctors taught hear hoofbeats think horses zebras also remember things equal prefer simpler explanations finally remember correlation causation correlated events say packet loss within cluster failed hard drives cluster share common causesin case power outage though network failure clearly cause hard drive failures vice versa even worse systems grow size complexity metrics monitored inevitable events happen correlate well events purely coincidence understanding failures reasoning process first step avoiding becoming effective solving problems methodical approach knowing know know need know makes simpler straightforward figure gone wrong fix practice practice course troubleshooting never clean idealized model suggests steps make process less painful productive experiencing system problems responding problem report every problem starts problem report might automated alert one colleagues saying system slow effective report tell expected behavior actual behavior possible reproduce behavior ideally reports consistent form stored searchable location bug tracking system teams often customized forms small web apps ask information relevant diagnosing particular systems support automatically generate route bug may also good point provide tools problem reporters try selfdiagnosing selfrepairing common issues common practice google open bug every issue even received via email instant messaging creates log investigation remediation activities referenced future many teams discourage reporting problems directly person several reasons practice introduces additional step transcribing report bug produces lowerquality reports visible members team tends concentrate problemsolving load handful team members reporters happen know rather person currently duty see also dealing interrupts shakespeare problem shakespeare problem oncall shakespeare search service receive alert shakespeareblackboxprobesearchfailure blackbox monitoring able find search results forms things unknown past five minutes alerting system filed bugwith links blackbox prober recent results playbook entry alertand assigned time spring action triage receive problem report next step figure problems vary severity issue might affect one user specific circumstances might workaround might entail complete global outage service response appropriate problem impact appropriate declare allhandsondeck emergency latter see managing incidents former overkill assessing issue severity requires exercise good engineering judgment often degree calm pressure first response major outage may start troubleshooting try find root cause quickly possible ignore instinct instead course action make system work well circumstances may entail emergency options diverting traffic broken cluster others still working dropping traffic wholesale prevent cascading failure disabling subsystems lighten load stopping bleeding first priority helping users system dies rootcausing course emphasis rapid triage preclude taking steps preserve evidence going wrong logs help subsequent rootcause analysis novice pilots taught first responsibility emergency fly airplane gaw troubleshooting secondary getting plane everyone safely onto ground approach also applicable computer systems example bug leading possibly unrecoverable data corruption freezing system prevent failure may better letting behavior continue realization often quite unsettling counterintuitive new sres particularly whose prior experience product development organizations examine need able examine component system order understand whether behaving correctly ideally monitoring system recording metrics system discussed practical alerting timeseries data metrics good place start figuring wrong graphing timeseries operations timeseries effective way understand behavior specific pieces system find correlations might suggest problems began logging another invaluable tool exporting information operation system state makes possible understand exactly process given point time may need analyze system logs across one many processes tracing requests whole stack using tools dapper sig provides powerful way understand distributed system working though varying use cases imply significantly different tracing designs sam logging text logs helpful reactive debugging real time storing logs structured binary format make possible build tools conduct retrospective analysis much information really useful multiple verbosity levels available along way increase levels fly functionality enables examine operations incredible detail without restart process still allowing dial back verbosity levels service operating normally depending volume traffic service receives might better use statistical sampling example might show one every operations next step include selection language say show operations match x wide range xeg set rpcs payload size bytes operations took longer ms return called dosomethinginteresting rpchandlerpy might even want design logging infrastructure turn needed quickly selectively exposing current state third trick toolbox example google servers endpoints show sample rpcs recently sent received possible understand one server communicating others without referencing architecture diagram endpoints also show histograms error rates latency type rpc possible quickly tell unhealthy systems endpoints show current configuration allow examination data instance google borgmon servers practical alerting timeseries data show monitoring rules using even allow tracing particular computation stepbystep source metrics value derived finally may even need instrument client experiment order discover component returning response requests debugging shakespeare using link blackbox monitoring results bug discover prober sends http get request apisearch endpoint searchtext forms things unknown expects receive response http response code json payload exactly matching work midsummer night s dream act scene line speaker theseus system set send probe minute past minutes half probes succeeded though discernible pattern unfortunately prober show returned failed make note fix future using curl manually send requests search endpoint get failed response http response code bad gateway payload http header xrequesttrace lists addresses backend servers responsible responding request information examine backends test whether responding appropriately diagnose thorough understanding system design decidedly helpful coming plausible hypotheses gone wrong also generic practices help even without domain knowledge simplify reduce ideally components system welldefined interfaces perform known transformations input output example given input search text component might return output containing possible matches possible look connections componentsor equivalently data flowing themto determine whether given component working properly injecting known test data order check resulting output expected form blackbox testing step especially effective injecting data intended probe possible causes errors solid reproducible test case makes debugging much faster may possible use case nonproduction environment invasive riskier techniques available would possible production dividing conquering useful generalpurpose solution technique multilayer system work happens throughout stack components often best start systematically one end stack work toward end examining component turn strategy also wellsuited use data processing pipelines exceptionally large systems proceeding linearly may slow alternative bisection splits system half examines communication paths components one side determining whether one half seems working properly repeat process left possibly faulty component ask ask malfunctioning system often still trying somethingjust thing want finding asking resources used output going help understand things gone wrong unpacking causes symptom symptom spanner cluster high latency rpcs servers timing spanner server tasks using cpu time make progress requests clients send server cpu time used profiling server shows sorting entries logs checkpointed disk logsorting code used evaluating regular expression paths log files solutions rewrite regular expression avoid backtracking look codebase similar patterns consider using re backtrack guarantees linear runtime growth input size touched last systems inertia found working computer system tends remain motion acted upon external force configuration change shift type load served recent changes system productive place start identifying going wrong welldesigned systems extensive production logging track new version deployments configuration changes layers stack server binaries handling user traffic packages installed individual nodes cluster correlating changes system performance behavior events system environment also helpful constructing monitoring dashboards example might annotate graph showing system error rates start end times deployment new version seen figure figure error rates graphed deployment start end times manually sending request apisearch endpoint see debugging shakespeare seeing failure listing backend servers handled response lets discount likelihood problem api frontend server load balancers response probably included information request least made search backends failed focus efforts backends analyzing logs sending test queries see responses return examining exported metrics specific diagnoses generic tools described previously helpful across broad range problem domains likely find helpful build tools systems help diagnosing particular services google sres spend much time building tools many tools necessarily specific given system sure look commonalities services teams avoid duplicating effort test treat come short list possible causes time try find factor root actual problem using experimental method try rule rule hypotheses instance suppose think problem caused either network failure application logic server database server database refusing connections trying connect database credentials application logic server uses refute second hypothesis pinging database server may able refute first depending network topology firewall rules factors following code trying imitate code flow stepbystep may point exactly going wrong number considerations keep mind designing tests may simple sending ping complicated removing traffic cluster injecting specially formed requests find race condition ideal test mutually exclusive alternatives rule one group hypotheses rule another set practice may difficult achieve consider obvious first perform tests decreasing order likelihood considering possible risks system test probably makes sense test network connectivity problems two machines looking whether recent configuration change removed user access second machine experiment may provide misleading results due confounding factors example firewall rule might permit access specific ip address might make pinging database workstation fail even pinging application logic server machine would succeeded active tests may side effects change future test results instance allowing process use cpus may make operations faster might increase likelihood encountering data races similarly turning verbose logging might make latency problem even worse confuse results problem getting worse logging tests may definitive suggestive difficult make race conditions deadlocks happen timely reproducible manner may settle less certain evidence causes take clear notes ideas tests ran results saw particularly dealing complicated drawnout cases documentation may crucial helping remember exactly happened prevent repeat steps performed active testing changing systemfor instance giving resources processmaking changes systematic documented fashion help return system pretest setup rather running unknown hodgepodge configuration negative results magic written randall bosetti edited joan wendt negative result experimental outcome expected effect absentthat experiment work planned includes new designs heuristics human processes fail improve upon systems replace negative results ignored discounted realizing wrong much value clear negative result resolve hardest design questions often team two seemingly reasonable designs progress one direction address vague speculative questions whether direction might better experiments negative results conclusive tell us something certain production design space performance limits existing system help others determine whether experiments designs worthwhile example given development team might decide using particular web server handle connections needed connections failing due lock contention subsequent development team decides evaluate web servers instead starting scratch use already welldocumented negative result starting point decide quickly whether need fewer connections b lock contention problems resolved even negative results apply directly someone else experiment supplementary data gathered help others choose new experiments avoid pitfalls previous designs microbenchmarks documented antipatterns project postmortems fit category consider scope negative result designing experiment broad especially robust negative result help peers even tools methods outlive experiment inform future work example benchmarking tools load generators result easily disconfirming experiment supporting one many webmasters benefited difficult detailoriented work produced apache bench web server loadtest even though first results likely disappointing building tools repeatable experiments indirect benefits well although one application build may benefit database ssds creating indices dense keys next one might writing script allows easily try configuration changes ensures forget miss optimizations next project publishing negative results improves industry datadriven culture accounting negative results statistical insignificance reduces bias metrics provides example others maturely accept uncertainty publishing everything encourage others everyone industry collectively learns much quickly sre already learned lesson highquality postmortems large positive effect production stability publish results interested experiment results good chance people well publish results people design run similar experiment tempting common avoid reporting negative results easy perceive experiment failed experiments doomed tend caught review many experiments simply unreported people mistakenly believe negative results progress part telling everyone designs algorithms team workflows ruled encourage peers recognizing negative results part thoughtful risk taking every welldesigned experiment merit skeptical design document performance review essay mention failure document potentially either heavily filtered author rigorous methods publish results find surprising othersincluding future selfaren surprised cure ideally narrowed set possible causes one next like prove actual cause definitively proving given factor caused problemby reproducing willcan difficult production systems often find probable causal factors following reasons systems complex quite likely multiple factors individually cause taken jointly causes real systems also often pathdependent must specific state failure occurs reproducing problem live production system may option either complexity getting system state failure triggered downtime may unacceptable nonproduction environment mitigate challenges though cost another copy system run found factors caused problem time write notes went wrong system tracked problem fixed problem prevent happening words need write postmortem although ideally system alive point case study app engine part google cloud platform platformasaservice product allows developers build services atop google infrastructure one internal customers filed problem report indicating recently seen dramatic increase latency cpu usage number running processes needed serve traffic app contentmanagement system used build documentation developers customer find recent changes code correlated increase resources increase traffic app see figure wondering change app engine service responsible investigation discovered latency indeed increased nearly order magnitude shown figure simultaneously amount cpu time figure number serving processes figure nearly quadrupled clearly something wrong time start troubleshooting figure application requests received per second showing brief spike return normal figure application latency showing th th th percentiles lines heatmap showing many requests fell given latency bucket point time shade figure aggregate cpu usage application figure number instances application typically sudden increase latency resource usage indicates either increase traffic sent system change system configuration however could easily rule possible causes spike traffic app around could explain brief surge resource usage expect traffic return baseline fairly soon request volume normalized spike certainly continued multiple days beginning app developers filed report started looking problem second change performance happened saturday neither changes app production environment flight service recent code pushes configuration pushes completed days furthermore problem originated service expect see similar effects apps using infrastructure however apps experiencing similar effects referred problem report counterparts app engine developers investigate whether customer encountering idiosyncrasies serving infrastructure developers able find oddities either however developer notice correlation latency increase increase specific data storage api call mergejoin often indicates suboptimal indexing reading datastore adding composite index properties app uses select objects datastore would speed requests principle speed application wholebut need figure properties needed indexing quick look application code reveal obvious suspects time pull heavy machinery toolkit using dapper sig traced steps individual http requests tookfrom receipt frontend reverse proxy point app code returned responseand looked rpcs issued server involved handling request would allow us see properties included requests datastore create appropriate indices investigating discovered requests static content images served datastore also much slower expected looking graphs filelevel granularity saw responses much faster days implied observed correlation mergejoin latency increase spurious suboptimalindexing theory fatally flawed examining unexpectedly slow requests static content rpcs sent application memcache service requests faston order milliseconds requests turn fast problem seem originate however time app started working request made first rpcs ms period app doingwell something app engine runs code provided users sre team profile inspect app code tell app interval similarly dapper help track going since trace rpc calls none made period faced point quite mystery decided solve ityet customer public launch scheduled following week sure soon able identify problem fix instead recommended customer increase resources allocated app cpurich instance type available reduced app latency acceptable levels though low prefer concluded latency mitigation good enough team could conduct launch successfully investigate leisure point suspected app victim yet another common cause sudden increases latency resource usage change type work seen increase writes datastore app latency increased increase largenor sustainedwe written coincidental however behavior resemble common pattern instance app initialized reading objects datastore storing instance memory instance avoids reading rarely changing configuration datastore request instead checks inmemory objects time takes handle requests often scale amount configuration data prove behavior root problem common antipattern app developers added instrumentation understand app spending time identified method called every request checked whether user whitelisted access given path method used caching layer sought minimize accesses datastore memcache service holding whitelist objects instances memory one app developers noted investigation know fire yet blinded smoke coming whitelist cache time later root cause found due longstanding bug app access control system whenever one specific path accessed whitelist object would created stored datastore runup launch automated security scanner testing app vulnerabilities side effect scan produced thousands whitelist objects course half hour superfluous whitelist objects checked every request app led pathologically slow responseswithout causing rpc calls app services fixing bug removing objects returned app performance expected levels making troubleshooting easier many ways simplify speed troubleshooting perhaps fundamental building observabilitywith whitebox metrics structured logs component ground designing systems wellunderstood observable interfaces components ensuring information available consistent way throughout system instance using unique request identifier throughout span rpcs generated various componentsreduces need figure log entry upstream component matches log entry downstream component speeding time diagnosis recovery problems correctly representing state reality code change environment change often lead need troubleshoot simplifying controlling logging changes reduce need troubleshooting make easier happens conclusion looked steps take make troubleshooting process clear understandable novices become effective solving problems adopting systematic approach troubleshootingas opposed relying luck experiencecan help bound services time recovery leading better experience users indeed using first principles troubleshooting skills often effective way learn system works see accelerating sres oncall beyond see https enwikipediaorgwikihypotheticodeductivemodel for instance exported variables described practical alerting time series data attributed theodore woodward university maryland school medicine s see https enwikipediaorgwikizebra medicine works domains systems entire classes failures may eliminable instance using welldesigned cluster filesystem means latency problem unlikely due single dead disk occam razor see https enwikipediaorgwikioccam srazor remember may still case multiple problems particular may likely system number common lowgrade problems taken together explain symptoms rather single rare problem causes cf https enwikipediaorgwikihickam sdictum of course see https xkcdcom at least plausible theory explain number phds awarded computer science us extremely well correlated r per capita consumption cheese http tylervigencomviewcorrelation id it may useful refer prospective bug reporters tat help provide highquality problem reports but beware false correlations lead wrong paths in many respects similar five whys technique ohn introduced taiichi ohno understand root causes manufacturing errors in contrast re pcre require exponential time evaluate regular expressions re available https githubcomgooglere all observes frequently used heuristic resolving outages using shared document realtime chat notes provides timestamp something helpful postmortems also shares information others speed current state world need interrupt troubleshooting see also negative results magic point see mea think systems also coo dek limitations finding single root cause instead examining system environment causative factors see https cloudgooglecomappengine we compressed simplified case study aid understanding while launching unidentified bug ideal often impractical eliminate known bugs instead sometimes make secondbest measures mitigate risk best using good engineering judgment the datastore lookup use index speed comparison frequent inmemory implementation simple loop comparison across cached objects objects matter takes linear timebut cause significant increase latency resource usage number cached objects grows chapter emergency response written corey adam baye edited diane bates things break life regardless stakes involved size organization one trait vital longterm health organization consequently sets organization apart others people involved respond emergency us naturally respond well emergency proper response takes preparation periodic pertinent handson training establishing maintaining thorough training testing processes requires support board management addition careful attention staff elements essential fostering environment teams spend money time energy possibly even uptime ensure systems processes people respond efficiently emergency note chapter postmortem culture discusses specifics write postmortems order make sure incidents require emergency response also become learning opportunity see postmortem culture learning failure chapter provides concrete examples incidents systems break first panic alone sky falling professional trained handle sort situation typically one physical dangeronly poor electrons peril worst half internet take deep breathand carry feel overwhelmed pull people sometimes may even necessary page entire company company incident response process see managing incidents make sure familiar follow process testinduced emergency google adopted proactive approach disaster emergency testing see kri sres break systems watch fail make changes improve reliability prevent failures recurring time controlled failures go planned target system dependent systems behave roughly manner expect identify weaknesses hidden dependencies document followup actions rectify flaws uncover however sometimes assumptions actual results worlds apart one example test unearthed number unexpected dependencies details wanted flush hidden dependencies test database within one larger distributed mysql databases plan block access one database hundred one foresaw results would unfold response within minutes commencing test numerous dependent services reported external internal users unable access key systems systems intermittently partially accessible assuming test responsible sre immediately aborted exercise attempted roll back permissions change unsuccessful instead panicking immediately brainstormed restore proper access using already tested approach restored permissions replicas failovers parallel effort reached key developers correct flaw database application layer library within hour original decision access fully restored services able connect broad impact test motivated rapid thorough fix libraries plan periodic retesting prevent major flaw recurring findings went well went well dependent services affected incident immediately escalated issues within company assumed correctly controlled experiment gotten hand immediately aborted test able fully restore permissions within hour first report time systems started behaving properly teams took different approach reconfigured systems avoid test database parallel efforts helped restore service quickly possible followup action items resolved quickly thoroughly avoid similar outage instituted periodic testing ensure similar flaws recur learned although test thoroughly reviewed thought well scoped reality revealed insufficient understanding particular interaction among dependent systems failed follow incident response process put place weeks thoroughly disseminated process would ensured services customers aware outage avoid similar scenarios future sre continually refines tests incident response tools processes addition making sure updates incident management procedures clearly communicated relevant parties tested rollback procedures test environment procedures flawed lengthened outage require thorough testing rollback procedures largescale tests changeinduced emergency imagine google lot configurationcomplex configuration and constantly make changes configuration prevent breaking systems outright perform numerous tests configuration changes make sure result unexpected undesired behavior however scale complexity google infrastructure make impossible anticipate every dependency interaction sometimes configuration changes go entirely according plan following one example details configuration change infrastructure helps protect services abuse pushed globally friday infrastructure interacts essentially externally facing systems change triggered crashloop bug systems caused entire fleet begin crashloop almost simultaneously google internal infrastructure also depends upon services many internal applications suddenly became unavailable well response within seconds monitoring alerts started firing indicating certain sites oncall engineers simultaneously experienced believed failure corporate network relocated dedicated secure rooms panic rooms backup access production environment joined additional engineers struggling corporate access within five minutes first configuration push engineer responsible push become aware corporate outage still unaware broader outage pushed another configuration change roll back first change point services began recover within minutes first push oncall engineers declared incident proceeded follow internal procedures incident response began notifying rest company situation push engineer informed oncall engineers outage likely due change pushed later rolled back nevertheless services experienced unrelated bugs misconfigurations triggered original event fully recover hour findings went well several factors play prevented incident resulting longerterm outage many google internal systems begin monitoring almost immediately detected alerted us problem however noted case monitoring less ideal alerts fired repeatedly constantly overwhelming oncalls spamming regular emergency communication channels problem detected incident management generally went well updates communicated often clearly outofband communications systems kept everyone connected even complicated software stacks unusable experience reminded us sre retains highly reliable low overhead backup systems use regularly addition outofband communications systems google commandline tools alternative access methods enable us perform updates roll back changes even interfaces inaccessible tools access methods worked well outage caveat engineers needed familiar tools test routinely google infrastructure provided yet another layer protection affected system ratelimited quickly provided full updates new clients behavior may throttled crashloop prevented complete outage allowing jobs remain long enough service requests crashes finally overlook element luck quick resolution incident push engineer happened following realtime communication channelsan additional level diligence normal part release process push engineer noticed large number complaints corporate access directly following push rolled back change almost immediately swift rollback occurred outage could lasted considerably longer becoming immensely difficult troubleshoot learned earlier push new feature involved thorough canary trigger bug exercised rare specific configuration keyword combination new feature specific change triggered bug considered risky therefore followed less stringent canary process change pushed globally used untested keywordfeature combination triggered failure ironically improvements canarying automation slated become higher priority following quarter incident immediately raised priority reinforced need thorough canarying regardless perceived risk one would expect alerting vocal incident every location essentially offline minutes disrupted real work performed oncall engineers made communication among involved incident difficult google relies upon tools much software stack use troubleshooting communicating lies behind jobs crashlooping outage lasted longer debugging would severely hindered processinduced emergency poured considerable amount time energy automation manages machine fleet amazing many jobs one start stop retool across fleet little effort sometimes efficiency automation bit frightening things go quite according plan one example moving fast good thing details part routine automation testing two consecutive turndown requests soontobedecommissioned server installation submitted case second turndown request subtle bug automation sent machines installations globally diskerase queue hard drives destined wiped see automation enabling failure scale details response soon second turndown request issued oncall engineers received page first small server installation taken offline decommissioned investigation determined machines transferred diskerase queue following normal procedure oncall engineers drained traffic location machines location wiped unable respond requests avoid failing requests outright oncall engineers drained traffic away location traffic redirected locations could properly respond requests long pagers everywhere firing server installations around world response oncall engineers disabled team automation order prevent damage stopped froze additional automation production maintenance shortly thereafter within hour traffic diverted locations although users may experienced elevated latencies requests fulfilled outage officially hard part began recovery network links reporting heavy congestion network engineers implemented mitigations choke points surfaced server installation one location chosen first many rise ashes within three hours initial outage thanks tenacity several engineers installation rebuilt brought back online happily accepting user requests us teams handed european counterparts sre hatched plan prioritize reinstallations using streamlined manual process team divided three parts part responsible one step manual reinstall process within three days vast majority capacity back online stragglers would recovered next month two findings went well reverse proxies large server installations managed differently reverse proxies small installations large installations impacted oncall engineers able quickly move traffic smaller installations large installations design large installations handle full load without difficulty however network links became congested therefore required network engineers develop workarounds order reduce impact end users oncall engineers targeted congested networks highest priority turndown process small installations worked efficiently well start finish took less hour successfully turn securely wipe large number installations although turndown automation quickly tore monitoring small installations oncall engineers able promptly revert monitoring changes helped assess extent damage engineers quickly followed incident response protocols matured considerably year since first outage described chapter communication collaboration throughout company across teams superba real testament incident management program training hands within respective teams chipped bringing vast experience bear learned root cause turndown automation server lacked appropriate sanity checks commands sent server ran response initial failed turndown received empty response machine rack instead filtering response passed empty filter machine database telling machine database diskerase machines involved yes sometimes zero mean machine database complied turndown workflow started churning machines quickly possible reinstallations machines slow unreliable behavior due large part use trivial file transfer protocol tftp lowest network quality service qos distant locations bios machine system dealt poorly failures depending network cards involved bios either halted went constant reboot cycle failing transfer boot files cycle taxing installers oncall engineers able fix reinstall problems reclassifying installation traffic slightly higher priority using automation restart machines stuck machine reinstallation infrastructure unable handle simultaneous setup thousands machines inability partly due regression prevented infrastructure running two setup tasks per worker machine regression also used improper qos settings transfer files poorly tuned timeouts forced kernel reinstallation even machines still proper kernel diskerase yet occur remedy situation oncall engineers escalated parties responsible infrastructure able quickly retune support unusual load problems solutions time experience shown systems break break ways one could never previously imagine one greatest lessons google learned solution exists even may obvious especially person whose pager screaming think solution cast net farther involve teammates seek help whatever quickly highest priority resolve issue hand quickly oftentimes person state one whose actions somehow triggered event utilize person importantly emergency mitigated forget set aside time clean write incident to learn past repeat keep history outages better way learn document broken past history learning everyone mistakes thorough honest ask hard questions look specific actions might prevent outage recurring tactically also strategically ensure everyone within company learn learned publishing organizing postmortems hold others accountable following specific actions detailed postmortems prevent future outage nearly identical caused nearly triggers outage already documented solid track record learning past outages see prevent future ones ask big even improbable questions if greater test reality ask big openended questions building power fails network equipment racks standing two feet water primary datacenter suddenly goes dark someone compromises web server call write check plan know react know systems react could minimize impact happen could person sitting next encourage proactive testing comes failures theory reality two different realms system actually failed truly know system dependent systems users react rely assumptions tested would prefer failure happen am saturday morning company still away teambuilding offsite black forestor best brightest close hand monitoring test painstakingly reviewed previous weeks conclusion reviewed three different cases parts systems broke although three emergencies triggered differentlyone proactive test another configuration change yet another turndown automation the responses shared many characteristics responders panic pulled others thought necessary responders studied learned earlier outages subsequently built systems better respond types outages time new failure modes presented responders documented failure modes followup helped teams learn better troubleshoot fortify systems similar outages responders proactively tested systems testing ensured changes fixed underlying problems identified weaknesses became outages systems evolve cycle continues outage test resulting incremental improvements processes systems case studies chapter specific google approach emergency response applied time organization size bios basic inputoutput system bios software built computer send simple instructions hardware allowing input output operating system loaded chapter managing incidents written andrew stribblehill edited kavita guliani effective incident management key limiting disruption caused incident restoring normal business operations quickly possible gamed response potential incidents advance principled incident management go window reallife situations chapter walks portrait incident spirals control due ad hoc incident management practices outlines wellmanaged approach incident reviews incident might played handled wellfunctioning incident management unmanaged incidents put shoes mary oncall engineer firm pm thursday afternoon pager exploded blackbox monitoring tells service stopped serving traffic entire datacenter sigh put coffee set job fixing minutes task another alert tells second datacenter stopped serving third five datacenters fails exacerbate situation traffic remaining datacenters handle start overload know service overloaded unable serve requests stare logs seems like eternity thousands lines logging suggest error one recently updated modules decide revert servers previous release see rollback helped call josephine wrote code nowhemorrhaging service reminding am time zone blearily agrees log take look colleagues sabrina robin start poking around terminals looking tell one suits phoned boss angrily demanding know informed total meltdown businesscritical service independently vice presidents nagging eta repeatedly asking could possibly happened would sympathize would require cognitive effort holding reserve job vps call prior engineering experience make irrelevant hardtorefute comments like increase page size time passes two remaining datacenters fail completely unbeknown sleepaddled josephine called malcolm brainwave something cpu affinity felt certain could optimize remaining server processes could deploy one simple change production environment within seconds servers restarted picking change died anatomy unmanaged incident note everybody preceding scenario job saw could things go wrong common hazards caused incident spiral control sharp focus technical problem tend hire people like mary technical prowess surprising busy making operational changes system trying valiantly solve problem position think bigger picture mitigate problem technical task hand overwhelming poor communication reason mary far busy communicate clearly nobody knew actions coworkers taking business leaders angry customers frustrated engineers could lent hand debugging fixing issue used effectively freelancing malcolm making changes system best intentions however coordinate coworkersnot even mary technically charge troubleshooting changes made bad situation far worse elements incident management process incident management skills practices exist channel energies enthusiastic individuals google incident management system based incident command system known clarity scalability welldesigned incident management process following features recursive separation responsibilities important make sure everybody involved incident knows role stray onto someone else turf somewhat counterintuitively clear separation responsibilities allows individuals autonomy might otherwise since need secondguess colleagues load given member becomes excessive person needs ask planning lead staff delegate work others task might entail creating subincidents alternatively role leader might delegate system components colleagues report highlevel information back leaders several distinct roles delegated particular individuals incident command incident commander holds highlevel state incident structure incident response task force assigning responsibilities according need priority de facto commander holds positions delegated appropriate remove roadblocks prevent ops working effectively operational work ops lead works incident commander respond incident applying operational tools task hand operations team group modifying system incident communication person public face incident response task force duties definitely include issuing periodic updates incident response team stakeholders usually via email may extend tasks keeping incident document accurate date planning planning role supports ops dealing longerterm issues filing bugs ordering dinner arranging handoffs tracking system diverged norm reverted incident resolved recognized command post interested parties need understand interact incident commander many situations locating incident task force members central designated war room appropriate others teams may prefer work desks keeping alert incident updates via email irc google found irc huge boon incident response irc reliable used log communications event record invaluable keeping detailed state changes mind also written bots log incidentrelated traffic helpful postmortem analysis bots log events alerts channel irc also convenient medium geographically distributed teams coordinate live incident state document incident commander important responsibility keep living incident document live wiki ideally editable several people concurrently teams use google docs though google docs sre use google sites depending software trying fix part incident management system unlikely end well see example incident state document sample incident document living doc messy must functional using template makes generating documentation easier keeping important information top makes usable retain documentation postmortem analysis necessary meta analysis clear live handoff essential post incident commander clearly handed end working day handing command someone another location simply safely update new incident commander phone video call new incident commander fully apprised outgoing commander explicit handoff specifically stating incident commander okay leave call receiving firm acknowledgment handoff handoff communicated others working incident clear leading incident management efforts times managed incident let examine incident might played handled using principles incident management pm mary third coffee day pager harsh tone surprises gulps drink problem datacenter stopped serving traffic starts investigate shortly another alert fires second datacenter five order rapidly growing issue knows benefit structure incident management framework mary snags sabrina take command nodding agreement sabrina quickly gets rundown occurred thus far mary captures details email sends prearranged mailing list sabrina recognizes yet scope impact incident asks mary assessment mary responds users yet impacted let hope lose third datacenter sabrina records mary response live incident document third alert fires sabrina sees alert among debugging chatter irc quickly follows email thread update thread keeps vps abreast highlevel status without bogging minutiae sabrina asks external communications representative start drafting user messaging follows mary see contact developer oncall currently josephine receiving mary approval sabrina loops josephine time josephine logs robin already volunteered help sabrina reminds robin josephine prioritize tasks delegated mary must keep mary informed additional actions take robin josephine quickly familiarize current situation reading incident document mary tried old binary release found wanting mutters robin updates irc say attempted fix work sabrina pastes update live incident management document pm sabrina starts finding replacement staff take incident colleagues go home updates incident document brief phone conference takes place everyone aware current situation pm hand responsibilities colleagues sister office mary returns work following morning find transatlantic colleagues assumed responsibility bug mitigated problem closed incident started work postmortem problem solved brews fresh coffee settles plan structural improvements problems category afflict team declare incident better declare incident early find simple fix close incident spin incident management framework hours burgeoning problem set clear conditions declaring incident team follows broad guidelinesif following true event incident need involve second team fixing problem outage visible customers issue unsolved even hour concentrated analysis incident management proficiency atrophies quickly constant use engineers keep incident management skills date handle incidents fortunately incident management framework apply operational changes need span time zones andor teams use framework frequently regular part change management procedures easily follow framework actual incident occurs organization performs disasterrecovery testing fun see kri incident management part testing process often roleplay response oncall issue already solved perhaps colleagues another location familiarize incident management summary found formulating incident management strategy advance structuring plan scale smoothly regularly putting plan use able reduce mean time recovery provide staff less stressful way work emergent problems organization concerned reliability would benefit pursuing similar strategy best practices incident management prioritizestop bleeding restore service preserve evidence rootcausing preparedevelop document incident management procedures advance consultation incident participants trustgive full autonomy within assigned role incident participants introspectpay attention emotional state responding incident start feel panicky overwhelmed solicit support consider alternativesperiodically consider options reevaluate whether still makes sense continue whether taking another tack incident response practiceuse process routinely becomes second nature change aroundwere incident commander last time take different role time encourage every team member acquire familiarity role an earlier version chapter appeared article login april vol see details http wwwfemagovnationalincidentmanagementsystem chapter postmortem culture learning failure appendix example postmortem written john lunney sue lueder edited gary connor cost failure education devin carraway sres work largescale complex distributed systems constantly enhance services new features add new systems incidents outages inevitable given scale velocity change incident occurs fix underlying issue services return normal operating conditions unless formalized process learning incidents place may recur ad infinitum left unchecked incidents multiply complexity even cascade overwhelming system operators ultimately impacting users therefore postmortems essential tool sre postmortem concept well known technology industry all postmortem written record incident impact actions taken mitigate resolve root cause followup actions prevent incident recurring chapter describes criteria deciding conduct postmortems best practices around postmortems advice cultivate postmortem culture based experience gained years google postmortem philosophy primary goals writing postmortem ensure incident documented contributing root cause well understood especially effective preventive actions put place reduce likelihood andor impact recurrence detailed survey rootcause analysis techniques beyond scope chapter instead see roo however articles best practices tools abound system quality domain teams use variety techniques rootcause analysis choose technique best suited services postmortems expected significant undesirable event writing postmortem punishmentit learning opportunity entire company postmortem process present inherent cost terms time effort deliberate choosing write one teams internal flexibility common postmortem triggers include uservisible downtime degradation beyond certain threshold data loss kind oncall engineer intervention release rollback rerouting traffic etc resolution time threshold monitoring failure usually implies manual incident discovery important define postmortem criteria incident occurs everyone knows postmortem necessary addition objective triggers stakeholder may request postmortem event blameless postmortems tenet sre culture postmortem truly blameless must focus identifying contributing causes incident without indicting individual team bad inappropriate behavior blamelessly written postmortem assumes everyone involved incident good intentions right thing information culture finger pointing shaming individuals teams wrong thing prevails people bring issues light fear punishment blameless culture originated healthcare avionics industries mistakes fatal industries nurture environment every mistake seen opportunity strengthen system postmortems shift allocating blame investigating systematic reasons individual team incomplete incorrect information effective prevention plans put place fix people fix systems processes better support people making right choices designing maintaining complex systems outage occur postmortem written formality forgotten instead postmortem seen engineers opportunity fix weakness make google resilient whole blameless postmortem simply vent frustration pointing fingers call services improved two examples pointing fingers need rewrite entire complicated backend system breaking weekly last three quarters sure tired fixing things onesytwosy seriously get paged one time rewrite myself blameless action item rewrite entire backend system might actually prevent annoying pages continuing happen maintenance manual version quite long really difficult fully trained sure future oncallers thank us best practice avoid blame keep constructive blameless postmortems challenging write postmortem format clearly identifies actions led incident removing blame postmortem gives people confidence escalate issues without fear also important stigmatize frequent production postmortems person team atmosphere blame risks creating culture incidents issues swept rug leading greater risk organization boy collaborate share knowledge value collaboration postmortem process exception postmortem workflow includes collaboration knowledgesharing every stage postmortem documents google docs inhouse template see example postmortem regardless specific tool use look following key features realtime collaboration enables rapid collection data ideas essential early creation postmortem open commentingannotation system makes crowdsourcing solutions easy improves coverage email notifications directed collaborators within document used loop others provide input writing postmortem also involves formal review publication practice teams share first postmortem draft internally solicit group senior engineers assess draft completeness review criteria might include key incident data collected posterity impact assessments complete root cause sufficiently deep action plan appropriate resulting bug fixes appropriate priority share outcome relevant stakeholders initial review complete postmortem shared broadly typically larger engineering team internal mailing list goal share postmortems widest possible audience would benefit knowledge lessons imparted google stringent rules around access piece information might identify enduser even internal documents like postmortems never include information best practice postmortem left unreviewed unreviewed postmortem might well never existed ensure completed draft reviewed encourage regular review sessions postmortems meetings important close ongoing discussions comments capture ideas finalize state involved satisfied document action items postmortem added team organization repository past incidents transparent sharing makes easier others find learn postmortem introducing postmortem culture introducing postmortem culture organization easier said done effort requires continuous cultivation reinforcement reinforce collaborative postmortem culture senior management active participation review collaboration process management encourage culture blameless postmortems ideally product engineer selfmotivation spirit nurturing postmortem culture sres proactively create activities disseminate learn system infrastructure example activities include postmortem month monthly newsletter interesting wellwritten postmortem shared entire organization google postmortem group group shares discusses internal external postmortems best practices commentary postmortems postmortem reading clubs teams host regular postmortem reading clubs interesting impactful postmortem brought table along tasty refreshments open dialogue participants nonparticipants new googlers happened lessons incident imparted aftermath incident often postmortem reviewed months years old wheel misfortune new sres often treated wheel misfortune exercise see disaster role playing previous postmortem reenacted cast engineers playing roles laid postmortem original incident commander attends help make experience real possible one biggest challenges introducing postmortems organization may question value given cost preparation following strategies help facing challenge ease postmortems workflow trial period several complete successful postmortems may help prove value addition helping identify criteria initiate postmortem make sure writing effective postmortems rewarded celebrated practice publicly social methods mentioned earlier individual team performance management encourage senior leadership acknowledgment participation even larry page talks high value postmortems best practice visibly reward people right thing google founders larry page sergey brin host tgif weekly allhands held live headquarters mountain view california broadcast google offices around world tgif focused art postmortem featured sre discussion highimpact incidents one sre discussed release recently pushed despite thorough testing unexpected interaction inadvertently took critical service four minutes incident lasted four minutes sre presence mind roll back change immediately averting much longer largerscale outage engineer receive two peer bonuses immediately afterward recognition quick levelheaded handling incident also received huge round applause tgif audience included company founders audience googlers numbering thousands addition visible forum google array internal social networks drive peer praise toward wellwritten postmortems exceptional incident handling one example many recognition contributions comes peers ceos everyone between best practice ask feedback postmortem effectiveness google strive address problems arise share innovations internally regularly survey teams postmortem process supporting goals process might improved ask questions culture supporting work writing postmortem entail much toil see eliminating toil best practices team recommend teams kinds tools would like see developed survey results give sres trenches opportunity ask improvements increase effectiveness postmortem culture beyond operational aspects incident management followup postmortem practice woven culture google cultural norm significant incident followed comprehensive postmortem conclusion ongoing improvements say confidence thanks continuous investment cultivating postmortem culture google weathers fewer outages fosters better user experience postmortems google working group one example commitment culture blameless postmortems group coordinates postmortem efforts across company pulling together postmortem templates automating postmortem creation data tools used incident helping automate data extraction postmortems perform trend analysis able collaborate best practices products disparate youtube google fiber gmail google cloud adwords google maps products quite diverse conduct postmortems universal goal learning darkest hours large number postmortems produced month across google tools aggregate postmortems becoming useful tools help us identify common themes areas improvement across product boundaries facilitate comprehension automated analysis recently enhanced postmortem template see example postmortem additional metadata fields future work domain includes machine learning help predict weaknesses facilitate realtime incident investigation reduce duplicate incidents see http wwwgooglecompoliciesprivacy if like start repository etsy released morgue tool managing postmortems google peer bonus program way fellow googlers recognize colleagues exceptional efforts involves token cash reward for discussion particular incident see emergency response chapter tracking outages written gabe krabbe edited lisa carey improving reliability time possible start known baseline track progress outalator outage tracker one tools use outalator system passively receives alerts sent monitoring systems allows us annotate group analyze data systematically learning past problems essential effective service management postmortems see postmortem culture learning failure provide detailed information individual outages part answer written incidents large impact issues individually small impact frequent widespread fall within scope similarly postmortems tend provide useful insights improving single service set services may miss opportunities would small effect individual cases opportunities poor costbenefit ratio would large horizontal impact also get useful information questions many alerts per oncall shift team get ratio actionablenonactionable alerts last quarter even simply services team manages creates toil escalator google alert notifications sre share central replicated system tracks whether human acknowledged receipt notification acknowledgment received configured interval system escalates next configured destination eg primary oncall secondary system called escalator initially designed largely transparent tool received copies emails sent oncall aliases functionality allowed escalator easily integrate existing workflows without requiring change user behavior time monitoring system behavior outalator following escalator example added useful features existing infrastructure created system would deal individual escalating notifications next layer abstraction outages outalator lets users view timeinterleaved list notifications multiple queues instead requiring user switch queues manually figure shows multiple queues appear outalator queue view functionality handy frequently single sre team primary point contact services distinct secondary escalation targets usually developer teams figure outalator queue view outalator stores copy original notification allows annotating incidents convenience silently receives saves copy email replies well followups less helpful others example replyall sent sole purpose adding recipients cc list annotations marked important annotation important parts message collapsed interface cut clutter together provides context referring incident possibly fragmented email thread multiple escalating notifications alerts combined single entity incident outalator notifications may related single incident may otherwise unrelated uninteresting auditable events privileged database access may spurious monitoring failures grouping functionality shown figure unclutters overview displays allows separate analysis incidents per day versus alerts per day figure outalator view incident building outalator many organizations use messaging systems like slack hipchat even irc internal communication andor updating status dashboards systems great places hook system like outalator aggregation single event may often trigger multiple alerts example network failures cause timeouts unreachable backend services everyone affected teams receive alerts including owners backend services meanwhile network operations center klaxons ringing however even smaller issues affecting single service may trigger multiple alerts due multiple error conditions diagnosed worthwhile attempt minimize number alerts triggered single event triggering multiple alerts unavoidable tradeoff calculations false positives false negatives ability group multiple alerts together single incident critical dealing duplication sending email saying thing thing symptoms incident works given alert prevent duplication debugging panic sending email alert practical scalable solution handling duplicate alerts within team let alone teams longer periods time tagging course every alerting event incident falsepositive alerts occur well test events mistargeted emails humans outalator distinguish events allows generalpurpose tagging add metadata notifications level tags mostly freeform single words colons however interpreted semantic separators subtly promotes use hierarchical namespaces allows automatic treatment namespacing supported suggested tag prefixes primarily cause action list teamspecific generated based historical usage example cause network might sufficient information teams whereas another team might opt specific tags cause network switch versus cause network cable teams may frequently use customer style tags customer would suggested teams others tags parsed turned convenient link bug links bug tracking system tags single word bogus widely used false positives course tags typos cause netwrok tags particularly helpful problemwentaway avoiding predetermined list allowing teams find preferences standards result useful tool better data overall tags remarkably powerful tool teams obtain provide overview given service pain points even without much even formal analysis trivial tagging appears probably one outalator useful unique features analysis course sre much react incidents historical data useful one responding incidentthe question last time always good starting point historical information far useful concerns systemic periodic wider problems may exist enabling analysis one important functions outage tracking tool bottom layer analysis encompasses counting basic aggregate statistics reporting details depend team include information incidents per weekmonthquarter alerts per incident next layer important easy provide comparison teamsservices time identify first patterns trends layer allows teams determine whether given alert load normal relative track record services third time week good bad knowing whether used happen five times per day five times per month allows interpretation next step data analysis finding wider issues raw counts require semantic analysis example identifying infrastructure component causing incidents therefore potential benefit increasing stability performance component assumes straightforward way provide information alongside incident records simple example different teams servicespecific alert conditions stale data high latency conditions may caused network congestion leading database replication delays need intervention could within nominal service level objective failing meet higher expectations users examining information across multiple teams allows us identify systemic problems choose correct solution especially solution may introduction artificial failures stop overperforming reporting communication immediate use frontline sres ability select zero outalations include subjects tags important annotations email next oncall engineer arbitrary cc list order pass recent state shifts periodic reviews production services occur weekly teams outalator also supports report mode important annotations expanded inline main list order provide quick overview lowlights unexpected benefits able identify alert flood alerts coincides given outage obvious benefits increases speed diagnosis reduces load teams acknowledging indeed incident additional nonobvious benefits use bigtable example service disruption due apparent bigtable incident see bigtable sre team alerted manually alerting team probably good idea improved crossteam visibility make big difference incident resolution least incident mitigation teams across company gone far set dummy escalator configurations human receives notifications sent notifications appear outalator tagged annotated reviewed one example system record use log audit use privileged role accounts though must noted functionality basic used technical rather legal audits another use record automatically annotate runs periodic jobs may idempotent example automatic application schema changes version control database systems for example might take significant engineering effort make particular change bigtable small mitigating effect one outage however mitigation available across many events engineering effort may well worthwhile on one hand incidents caused good starting point reducing number alerts triggered improving overall system hand metric may simply artifact oversensitive monitoring small set client systems misbehaving running outside agreed service level gripping hand number incidents alone gives indication difficulty fix severity impact chapter testing reliability written alex perry max luebbe edited diane bates nt tried assume s broken unknown one key responsibility site reliability engineers quantify confidence systems maintain sres perform task adapting classical software testing techniques systems scale confidence measured past reliability future reliability former captured analyzing data provided monitoring historic system behavior latter quantified making predictions data past system behavior order predictions strong enough useful one following conditions must hold site remains completely unchanged time software releases changes server fleet means future behavior similar past behavior confidently describe changes site order analysis allow uncertainty incurred changes testing mechanism use demonstrate specific areas equivalence changes occur test passes change reduces uncertainty analysis needs allow thorough testing helps us predict future reliability given site enough detail practically useful amount testing need conduct depends reliability requirements system percentage codebase covered tests increases reduce uncertainty potential decrease reliability change adequate testing coverage means make changes reliability falls acceptable level make many changes quickly predicted reliability approaches acceptability limit point may want stop making changes new monitoring data accumulates accumulating data supplements tested coverage validates reliability asserted revised execution paths assuming served clients randomly distributed woo sampling statistics extrapolate monitored metrics whether aggregate behavior making use new paths statistics identify areas need better testing retrofitting relationships testing mean time repair passing test series tests necessarily prove reliability however tests failing generally prove absence reliability monitoring system uncover bugs quickly reporting pipeline react mean time repair mttr measures long takes operations team fix bug either rollback another action possible testing system identify bug zero mttr zero mttr occurs systemlevel test applied subsystem test detects exact problem monitoring would detect test enables push blocked bug never reaches production though still needs repaired source code repairing zero mttr bugs blocking push quick convenient bugs find zero mttr higher mean time failures mtbf experienced users mtbf increases response better testing developers encouraged release features faster features course bugs new bugs result opposite adjustment release velocity bugs found fixed authors writing software testing largely agree coverage needed conflicts opinion stem conflicting terminology differing emphasis impact testing software lifecycle phases particularities systems conducted testing discussion testing google general see whi following sections specify software testingrelated terminology used chapter types software testing software tests broadly fall two categories traditional production traditional tests common software development evaluate correctness software offline development production tests performed live web service evaluate whether deployed software system working correctly traditional tests shown figure traditional software testing begins unit tests testing complex functionality layered atop unit tests figure hierarchy traditional tests unit tests unit tests unit test smallest simplest form software testing tests employed assess separable unit software class function correctness independent larger software system contains unit unit tests also employed form specification ensure function module exactly performs behavior required system unit tests commonly used introduce testdriven development concepts integration tests software components pass individual unit tests assembled larger components engineers run integration test assembled component verify functions correctly dependency injection performed tools dagger extremely powerful technique creating mocks complex dependencies engineer cleanly test component common example dependency injection replace stateful database lightweight mock precisely specified behavior system tests system test largest scale test engineers run undeployed system modules belonging specific component server passed integration tests assembled system engineer tests endtoend functionality system system tests come many different flavors smoke tests smoke tests engineers test simple critical behavior among simplest type system tests smoke tests also known sanity testing serve shortcircuit additional expensive testing performance tests basic correctness established via smoke test common next step write another variant system test ensure performance system stays acceptable duration lifecycle response times dependencies resource requirements may change dramatically course development system needs tested make sure become incrementally slower without anyone noticing gets released users example given program may evolve need gb memory formerly needed gb ms response time might turn ms ms performance test ensures time system degrade become expensive regression tests another type system test involves preventing bugs sneaking back codebase regression tests analogized gallery rogue bugs historically caused system fail produce incorrect results documenting bugs tests system integration level engineers refactoring codebase sure accidentally introduce bugs already invested time effort eliminate important note tests cost terms time computational resources one extreme unit tests cheap dimensions usually completed milliseconds resources available laptop end spectrum bringing complete server required dependencies mock equivalents run related tests take significantly timefrom several minutes multiple hoursand possibly require dedicated computing resources mindfulness costs essential developer productivity also encourages efficient use testing resources production tests production tests interact live production system opposed system hermetic testing environment tests many ways similar blackbox monitoring see monitoring distributed systems therefore sometimes called blackbox testing production tests essential running reliable production service rollouts entangle tests often said testing performed hermetic environment nar statement implies production hermetic course production usually hermetic rollout cadences make live changes production environment small wellunderstood chunks manage uncertainty hide risk users changes might pushed live order added source control rollouts often happen stages using mechanisms gradually shuffle users around addition monitoring stage ensure new environment hitting anticipated yet unexpected problems result entire production environment intentionally representative given version binary checked source control possible source control one version binary associated configuration file waiting made live scenario cause problems tests conducted live environment example test might use latest version configuration file located source control along older version binary live might test older version configuration file find bug fixed newer version file similarly system test use configuration files assemble modules running test test passes version one configuration test discussed following section fails result test valid hermetically operationally outcome inconvenient configuration test google web service configurations described files stored version control system configuration file separate configuration test examines production see particular binary actually configured reports discrepancies file tests inherently hermetic operate outside test infrastructure sandbox configuration tests built tested specific version checkedin configuration file comparing version test passing relation goal version automation implicitly indicates far actual production currently lags behind ongoing engineering work nonhermetic configuration tests tend especially valuable part distributed monitoring solution since pattern passesfails across production identify paths service stack sensible combinations local configurations monitoring solution rules try match paths actual user requests trace logs set undesirable paths matches found rules become alerts ongoing releases andor pushes proceeding safely remedial action needed configuration tests simple production deployment uses actual file content offers realtime query retrieve copy content case test code simply issues query diffs response file tests become complex configuration one following implicitly incorporates defaults built binary meaning tests separately versioned result passes preprocessor bash commandline flags rendering tests subject expansion rules specifies behavioral context shared runtime making tests depend runtime release schedule stress test order safely operate system sres need understand limits system components many cases individual components gracefully degrade beyond certain pointinstead catastrophically fail engineers use stress tests find limits web service stress tests answer questions full database get writes start fail many queries second sent application server becomes overloaded causing requests fail canary test canary test conspicuously absent list production tests term canary comes phrase canary coal mine refers practice using live bird detect toxic gases humans poisoned conduct canary test subset servers upgraded new version configuration left incubation period unexpected variances occur release continues rest servers upgraded progressive fashion anything go awry modified servers quickly reverted known good state commonly refer incubation period upgraded servers baking binary canary test really test rather structured user acceptance whereas configuration stress tests confirm existence specific condition deterministic software canary test ad hoc exposes code test less predictable live production traffic thus perfect always catch newly introduced faults provide concrete example canary might proceed consider given underlying fault relatively rarely impacts user traffic deployed upgrade rollout exponential expect growing cumulative number reported variances cu rk r rate reports u order fault defined later k period traffic grows factor e order avoid user impact rollout triggers undesirable variances needs quickly rolled back prior configuration short time takes automation observe variances respond likely several additional reports generated dust settled reports estimate cumulative number c rate r dividing correcting k gives estimate u order underlying fault examples u user request encountered code simply broken u user request randomly damages data future user request may see u randomly damaged data also valid identifier previous request bugs order one scale linearly amount user traffic per generally track bugs converting logs requests unusual responses new regression tests strategy work higherorder bugs request repeatedly fails preceding requests attempted order suddenly pass requests omitted important catch higherorder bugs release otherwise operational workload increase quickly keeping dynamics higher versus lowerorder bugs mind using exponential rollout strategy necessary attempt achieve fairness among fractions user traffic long method establishing fraction uses k interval estimate u valid even though yet determine method instrumental illuminating fault using many methods sequentially permitting overlap keeps value k small strategy minimizes total number uservisible variances c still allowing early estimate u hoping course creating test build environment wonderful think types tests failure scenarios day one project frequently sres join developer team project already well underwayonce team project validates research model library proves project underlying algorithm scalable perhaps user interface mocks finally acceptable team codebase still prototype comprehensive testing yet designed deployed situations testing efforts begin conducting unit tests every key function class completely overwhelming prospect current test coverage low nonexistent instead start testing delivers impact least effort start approach asking following questions prioritize codebase way borrow technique feature development project management every task high priority none tasks high priority stackrank components system testing measure importance particular functions classes absolutely missioncritical businesscritical example code involves billing commonly businesscritical billing code also frequently cleanly separable parts system apis teams integrating even kind breakage never makes past release testing user extremely harmful confuses another developer team causing write wrong even suboptimal clients api shipping software obviously broken among cardinal sins developer takes little effort create series smoke tests run every release type loweffort highimpact first step lead highly tested reliable software one way establish strong testing culture start documenting reported bugs test cases every bug converted test test supposed initially fail bug yet fixed engineers fix bugs software passes testing road developing comprehensive regression test suite another key task creating welltested software set testing infrastructure foundation strong testing infrastructure versioned source control system tracks every change codebase source control place add continuous build system builds software runs tests every time code submitted found optimal build system notifies engineers moment change breaks software project risk sounding obvious essential latest version software project source control working completely build system notifies engineers broken code drop tasks prioritize fixing problem appropriate treat defects seriously reasons usually harder fix broken changes codebase defect introduced broken software slows team must work around breakage release cadences nightly weekly builds lose value ability team respond request emergency release example response security vulnerability disclosure becomes much complex difficult concepts stability agility traditionally tension world sre last bullet point provides interesting case stability actually drives agility build predictably solid reliable developers iterate faster build systems like bazel valuable features afford precise control testing example bazel creates dependency graphs software projects change made file bazel rebuilds part software depends file systems provide reproducible builds instead running tests every submit tests run changed code result tests execute cheaper faster variety tools help quantify visualize level test coverage need cra use tools shape focus testing approach prospect creating highly tested code engineering project rather philosophical mental exercise instead repeating ambiguous refrain need tests set explicit goals deadlines remember software created equal lifecritical revenuecritical systems demand substantially higher levels test quality coverage nonproduction script short shelf life testing scale covered fundamentals testing let examine sre takes systems perspective testing order drive reliability scale small unit test might short list dependencies one source file testing library runtime libraries compiler local hardware running tests robust testing environment dictates dependencies test coverage tests specifically address use cases parts environment expect implementation unit test depends code path inside runtime library test coverage unrelated change environment lead unit test consistently pass testing regardless faults code test contrast release test might depend many parts transitive dependency every object code repository test depends clean copy production environment principle every small patch requires performing full disaster recovery iteration practical testing environments try select branch points among versions merges resolves maximum amount dependent uncertainty minimum number iterations course area uncertainty resolves fault need select additional branch points testing scalable tools testing scalable tools pieces software sre tools also need testing sredeveloped tools might perform tasks following retrieving propagating database performance metrics predicting usage metrics plan capacity risks refactoring data within service replica user accessible changing files server sre tools share two characteristics side effects remain within tested mainstream api isolated userfacing production existing validation release barrier barrier defenses risky software software bypasses usual heavily tested api even good cause could wreak havoc live service example database engine implementation might allow administrators temporarily turn transactions order shorten maintenance windows implementation used batch update software userfacing isolation may lost utility ever accidentally launched userfacing replica avoid risk havoc design use separate tool place barrier replication configuration replica pass health check result replica released users configure risky software check barrier upon startup allow risky software access unhealthy replicas use replica health validating tool use blackbox monitoring remove barrier automation tools also software risk footprint appears outofband different layer service testing needs subtle automation tools perform tasks like following database index selection load balancing datacenters shuffling relay logs fast remastering automation tools share two characteristics actual operation performed robust predictable welltested api purpose operation side effect invisible discontinuity another api client testing demonstrate desired behavior service layer change often possible test whether internal state seen api constant across operation example databases pursue correct answers even suitable index available query hand documented api invariants dns cache holding ttl may hold across operation example runlevel change replaces local nameserver caching proxy choices promise retain completed lookups many seconds unlikely cache state handed one given automation tools imply additional release tests binaries handle environmental transients define environment automation tools run automation shuffling containers improve usage likely try shuffle point also runs container would embarrassing new release internal algorithm yielded dirty memory pages quickly network bandwidth associated mirroring ended preventing code finalizing live migration even integration test binary intentionally shuffles around test likely use productionsized model container fleet almost certainly allowed use scarce highlatency intercontinental bandwidth testing races even amusingly one automation tool might changing environment another automation tool runs tools might changing environment automation tool simultaneously example fleet upgrading tool likely consumes resources pushing upgrades result container rebalancing would tempted move tool turn container rebalancing tool occasionally needs upgrading circular dependency fine associated apis restart semantics someone remembered implement test coverage semantics checkpoint health assured independently testing disaster many disaster recovery tools carefully designed operate offline tools following compute checkpoint state equivalent cleanly stopping service push checkpoint state loadable existing nondisaster validation tools support usual release barrier tools trigger clean start procedure many cases implement phases associated tests easy write offer excellent coverage constraints offline checkpoint loadable barrier clean start must broken much harder show confidence associated tool implementation work time short notice online repair tools inherently operate outside mainstream api therefore become interesting test one challenge face distributed system determining normal behavior may eventually consistent nature interact badly repair example consider race condition attempt analyze using offline tools offline tool generally written expect instant consistency opposed eventual consistency instant consistency less challenging test situation becomes complicated repair binary generally built separately serving production binary racing consequently might need build unified instrumented binary run within tests tools observe transactions using statistical tests statistical techniques lemon ana fuzzing chaos monkey jepsen distributed state necessarily repeatable tests simply rerunning tests code change definitively prove observed fault fixed however techniques useful provide log randomly selected actions taken given runsometimes simply logging random number generator seed log immediately refactored release test running times starting bug report often helpful rate nonfailure replay tells hard later assert fault fixed variations fault expressed help pinpoint suspicious areas code later runs may demonstrate failure situations severe original run response may want escalate bug severity impact need speed every version patch code repository every defined test provides pass fail indication indication may change repeated seemingly identical runs estimate actual likelihood test passing failing averaging many runs computing statistical uncertainty likelihood however performing calculation every test every version point computationally infeasible instead must form hypotheses many scenarios interest run appropriate number repeats test version allow reasonable inference scenarios benign code quality sense others actionable scenarios affect test attempts varying extents coupled reliably quickly obtaining list actionable hypotheses ie components actually broken means estimating scenarios time engineers use testing infrastructure want know codeusually tiny fraction source behind given test runis broken often broken implies observed failures blamed someone else code words engineer wants know code unanticipated race condition makes test flaky flaky test already due factors testing deadlines tests simple sense run selfcontained hermetic binary fits small compute container seconds tests give engineers interactive feedback mistakes engineer switches context next bug task tests require orchestration across many binaries andor across fleet many containers tend startup times measured seconds tests usually unable offer interactive feedback classified batch tests instead saying close editor tab engineer test failures saying code ready review code reviewer informal deadline test point engineer makes next context switch test results best given engineer switches context otherwise next context may involve xkcd compiling suppose engineer working service simple tests occasionally proposes patch service codebase test patch want compare vector passfail results codebase patch vector results codebase patch favorable comparison two vectors provisionally qualifies codebase releasable qualification creates incentive run many release integration tests well distributed binary tests examine scaling system case patch uses significantly local compute resources complexity case patch creates superlinear workload elsewhere rate incorrectly flag user patch damaging miscalculating environmental flakiness seems likely users would vehemently complain patches rejected rejection patch among perfect patches might go without comment means interested th root one defined test patch one defined test patch fraction patches accepted calculation suggests individual tests must run correctly time hmm pushing production pushing production production configuration management commonly kept source control repository configuration often separate developer source code similarly software testing infrastructure often see production configuration even two located repository changes configuration management made branches andor segregated directory tree test automation historically ignored legacy corporate environment software engineers develop binaries throw wall administrators update servers segregation testing infrastructure production configuration best annoying worst damage reliability agility segregation might also lead tool duplication nominally integrated ops environment segregation degrades resiliency creates subtle inconsistencies behavior two sets tools segregation also limits project velocity commit races versioning systems sre model impact segregating testing infrastructure production configuration appreciably worse prevents relating model describing production model describing application behavior discrepancy impacts engineers want find statistical inconsistencies expectations development time however segregation slow development much prevent system architecture changing way eliminate migration risk consider scenario unified versioning unified testing sre methodology applicable impact would failure distributed architecture migration fair amount testing probably occur far assumed software engineer would likely accept test system giving wrong answer time risk willing take migration know testing may return false negative situation could become really exciting really quickly clearly areas test coverage need higher level paranoia others distinction generalized test failures indicative larger impact risk test failures expect testing fail long ago software product might released per year binaries generated compiler toolchain many hours days testing performed humans manually written instructions release process inefficient little need automate release effort dominated documentation data migration user retraining factors mean time failure mtbf releases one year matter much testing took place many changes happened per release uservisible breakage bound hiding software effectively reliability data previous release irrelevant next release effective apiabi management tools interpreted languages scale large amounts code support building executing new software version every minutes principle sufficiently large army humans could complete testing new version using methods described earlier achieve quality bar incremental version even though ultimately tests applied code final software version higher quality resulting release ships annually addition annual versions intermediate versions code also tested using intermediates unambiguously map problems found testing back underlying causes confident whole issue limited symptom exposed fixed principle shorter feedback cycle equally effective applied automated test coverage let users try versions software year mtbf suffers opportunities uservisible breakage however also discover areas would benefit additional test coverage tests implemented improvement protects future failure careful reliability management combines limits uncertainty due test coverage limits uservisible faults order adjust release cadence combination maximizes knowledge gain operations end users gains drive test coverage turn product release velocity sre modifies configuration file adjusts automation tool strategy opposed implementing user feature engineering work matches conceptual model defining release cadence based reliability often makes sense segment reliability budget functionality conveniently team scenario feature engineering team aims achieve given uncertainty limit affects goal release cadence sre team separate budget associated uncertainty thus upper limit release rate order remain reliable avoid scaling number sres supporting service linearly production environment run mostly unattended remain unattended environment must resilient minor faults major event demands manual sre intervention occurs tools used sre must suitably tested otherwise intervention decreases confidence historical data applicable near future reduction confidence requires waiting analysis monitoring data order eliminate uncertainty incurred whereas previous discussion testing scalable tools focused meet opportunity test coverage sre tool see testing determines often appropriate use tool production configuration files generally exist changing configuration faster rebuilding tool low latency often factor keeping mttr low however files also changed frequently reasons need reduced latency viewed point view reliability configuration file exists keep mttr low modified failure release cadence slower mtbf fair amount uncertainty whether given manual edit actually truly optimal without edit impacting overall site reliability configuration file changes per userfacing application release example holds release state major risk changes treated application releases testing monitoring coverage configuration file considerably better user application file dominate site reliability negative way one method handling configuration files make sure every configuration file categorized one options preceding bulleted list somehow enforce rule take latter strategy make sure following configuration file enough test coverage support regular routine editing releases file edits somewhat delayed waiting release testing provide breakglass mechanism push file live completing testing since breaking glass impairs reliability generally good idea make break noisy example filing bug requesting robust resolution next time breakglass testing implement breakglass mechanism disable release testing means whoever makes hurried manual edit told mistakes real user impact reported monitoring better leave tests running associate early push event pending testing event soon possible backannotate push broken tests way flawed manual push quickly followed another hopefully less flawed manual push ideally breakglass mechanism automatically boosts priority release tests preempt routine incremental validation coverage workload test infrastructure already processing integration addition unit testing configuration file mitigate risk reliability also important consider integration testing configuration files contents configuration file testing purposes potentially hostile content interpreter reading configuration interpreted languages python commonly used configuration files interpreters embedded simple sandboxing available protect nonmalicious coding errors writing configuration files interpreted language risky approach fraught latent failures hard definitively address loading content actually consists executing program inherent upper limit inefficient loading addition testing pair type integration testing careful deadline checking integration test methods order label tests run completion reasonable amount time failed configuration instead written text custom syntax every category test needs separate coverage scratch using existing syntax yaml combination heavily tested parser like python safeload removes toil incurred configuration file careful choice syntax parser ensure hard upper limit long loading operation take however implementer needs address schema faults simple strategies upper bound runtime even worse strategies tend robustly unit tested benefit using protocol buffers schema defined advance automatically checked load time removing even toil yet still offering bounded runtime role sre generally includes writing systems engineering tools one else already writing adding robust validation test coverage tools behave unexpectedly due bugs caught testing defense depth advisable one tool behaves unexpectedly engineers need confident possible tools working correctly therefore mitigate resolve side effects misbehavior key element delivering site reliability finding anticipated form misbehavior making sure test another tool tested input validator reports misbehavior tool finds problem might able fix even stop least report problem catastrophic outage occurs example consider configured list users etcpasswd nonnetworked unixstyle machine imagine edit unintentionally causes parser stop parsing half file recently created users loaded machine likely continue run without problem many users may notice fault tool maintains home directories easily notice mismatch actual directories present implied partial user list urgently report discrepancy tool value lies reporting problem avoid attempting remediate deleting lots user data production probes given testing specifies acceptable behavior face known data monitoring confirms acceptable behavior face unknown user data would seem major sources riskboth known unknownare covered combination testing monitoring unfortunately actual risk complicated known good requests work known bad requests error implementing kinds coverage integration test generally good idea replay bank test requests release test splitting known good requests replayed production yields three sets requests known bad requests known good requests replayed production known good requests replayed production use set integration release tests tests also used monitoring probes would seem superfluous principle pointless deploy monitoring exact requests already tried two ways however two ways different reasons release test probably wrapped integrated server frontend fake backend probe test probably wrapped release binary load balancing frontend separate scalable persistent backend frontends backends probably independent release cycles likely schedules cycles occur different rates due adaptive release cadences therefore monitoring probe running production configuration previously tested probes never fail mean fail either frontend api load balancer backend api persistent store equivalent production release environments unless already know production release environments equivalent site likely broken production updater gradually replaces application also gradually replaces probes four combinations oldornew probes sending requests oldornew applications continuously generated updater detect one four combinations generating errors roll back last known good state usually updater expects newly started application instance unhealthy short time prepares start receiving lots user traffic probes already inspected part readiness check update safely fails indefinitely user traffic ever routed new version update remains paused engineers time inclination diagnose fault condition encourage production updater cleanly roll back production test probe indeed offer protection site plus clear feedback engineers earlier feedback given engineers useful also preferable test automated delivery warnings engineers scalable assume component older software version replaced newer version rolling soon newer version might talking old version peer forces use deprecated api older version might talking peer newer version using api time older version released work properly yet works honest better hope tests future compatibility running monitoring probes good api coverage fake backend versions implementing release tests fake backend often maintained peer service engineering team merely referenced build dependency hermetic test executed testing infrastructure always combines fake backend test frontend build point revision control history build dependency may providing runnable hermetic binary ideally engineering team maintaining cuts release fake backend binary time cut main backend application probes backend release available might worthwhile include hermetic frontend release tests without fake backend binary frontend release package monitoring aware release versions sides given service interface two peers setup ensures retrieving every combination two releases determining whether test still passes take much extra configuration monitoring happen continuouslyyou need run new combinations result either team cutting new release problems block new release hand rollout automation ideally block associated production rollout problematic combinations longer possible similarly peer team automation may consider draining upgrading replicas yet moved problematic combination conclusion testing one profitable investments engineers make improve reliability product testing activity happens twice lifecycle project continuous amount effort required write good tests substantial effort build maintain infrastructure promotes strong testing culture fix problem understand engineering understand problem measuring methodologies techniques chapter provide solid foundation measuring faults uncertainty software system help engineers reason reliability software written released users this chapter explains maximize value derived investing engineering effort testing engineer defines suitable tests given system generalized way remaining work common across sre teams thus may considered shared infrastructure infrastructure consists scheduler share budgeted resources across otherwise unrelated projects executors sandbox test binaries prevent considered trusted two infrastructure components considered ordinary sresupported service much like cluster scale storage therefore discussed for reading equivalence http stackoverflowcomquestionsequivalenceclasstestingvsboundaryvaluetesting see see https googlegithubiodagger a standard rule thumb start release impact user traffic scaling orders magnitude every hours varying geographic location servers upgraded day day day for instance assuming hour interval continuous exponential growth minutes seconds hours we using order sense big notation order complexity context see https enwikipediaorgwikibigonotation for topic highly recommend bla former coworker exgoogler mike bland see https githubcomgooglebazel for example code test wraps nontrivial api provide simpler backwardcompatible abstraction api used synchronous instead returns future calling argument errors still deliver exception future evaluated code test passes api result directly back caller many cases argument misuse may caught this section talks specifically tools used sre need scalable however sre also develops uses tools necessarily need scalable tools need scalable also need tested tools scope section therefore discussed risk footprint similar userfacing applications similar testing strategies applicable sredeveloped tools see https githubcomnetflixsimianarmywikichaosmonkey see https githubcomaphyrjepsen even test run repeated random seed task kills order serialization kills fake user traffic therefore guarantee actual previously observed code path exercised see http xkcdcom perhaps acquired mechanical turk similar services see https githubcomgoogleprotobuf not software engineers write tools cross technology verticals span abstraction layers tend weak associations many software teams slightly stronger association systems teams chapter software engineering sre written dave helstroom trisha weir evan leonard kurt delimon edited kavita guliani ask someone name google software engineering effort likely list consumerfacing product like gmail maps might even mention underlying infrastructure bigtable colossus truth massive amount behindthescenes software engineering consumers never see number products developed within sre google production environment isby measuresone complex machines humanity ever built sres firsthand experience intricacies production making uniquely well suited develop appropriate tools solve internal problems use cases related keeping production running majority tools related overall directive maintaining uptime keeping latency low take many forms examples include binary rollout mechanisms monitoring development environment built dynamic server composition overall sredeveloped tools fullfledged software engineering projects distinct oneoff solutions quick hacks sres develop adopted productbased mindset takes internal customers roadmap future plans account software engineering within sre important many ways vast scale google production necessitated internal software development thirdparty tools designed sufficient scale google needs company history successful software projects led us appreciate benefits developing directly within sre sres unique position effectively develop internal software number reasons breadth depth googlespecific production knowledge within sre organization allows engineers design create software appropriate considerations dimensions scalability graceful degradation failure ability easily interface infrastructure tools sres embedded subject matter easily understand needs requirements tool developed direct relationship intended userfellow sresresults frank highsignal user feedback releasing tool internal audience high familiarity problem space means development team launch iterate quickly internal users typically understanding comes minimal ui alpha product issues purely pragmatic standpoint google clearly benefits engineers sre experience developing software deliberate design growth rate sresupported services exceeds growth rate sre organization one sre guiding principles team size scale directly service growth achieving linear team growth face exponential service growth requires perpetual automation work efforts streamline tools processes aspects service introduce inefficiency daytoday operation production people direct experience running production systems developing tools ultimately contribute uptime latency goals makes lot sense flip side individual sres well broader sre organization also benefit sredriven software development fully fledged software development projects within sre provide career development opportunities sres well outlet engineers want coding skills get rusty longterm project work provides muchneeded balance interrupts oncall work provide job satisfaction engineers want careers maintain balance software engineering systems engineering beyond design automation tools efforts reduce workload engineers sre software development projects benefit sre organization attracting helping retain engineers broad variety skills desirability team diversity doubly true sre variety backgrounds problemsolving approaches help prevent blind spots end google always strives staff sre teams mix engineers traditional software development experience engineers systems engineering experience auxon case study project background problem space case study examines auxon powerful tool developed within sre automate capacity planning services running google production best understand auxon conceived problems addresses first examine problem space associated capacity planning difficulties traditional approaches task present services google across industry whole context google uses terms service cluster see production environment google viewpoint sre traditional capacity planning myriad tactics capacity planning compute resources see hixa majority approaches boil cycle approximated follows collect demand forecasts many resources needed resources needed uses best data available today plan future typically covers anywhere several quarters years devise build allocation plans given forecasted outlook best way meet demand additional supply resources much supply locations review sign plan forecast reasonable plan line budgetary productlevel technical considerations deploy configure resources resources eventually arrive potentially phases course defined period time services get use resources make typically lowerlevel resources cpu disk etc useful services bears stressing capacity planning neverending cycle assumptions change deployments slip budgets cut resulting revision upon revision plan revision trickledown effects must propagate throughout plans subsequent quarters example shortfall quarter must made future quarters traditional capacity planning uses demand key driver manually shapes supply fit demand response change brittle nature traditional capacity planning produces resource allocation plan disrupted seemingly minor change example service undergoes decrease efficiency needs resources expected serve demand customer adoption rates increase resulting increase projected demand delivery date new cluster compute resources slips product decision performance goal changes shape required service deployment service footprint amount required resources minor changes require crosschecking entire allocation plan make sure plan still feasible larger changes delayed resource delivery product strategy changes potentially require recreating plan scratch delivery slippage single cluster might impact redundancy latency requirements multiple services resource allocations clusters must increased make slippage changes would propagate throughout plan also consider capacity plan given quarter time frame based expected outcome capacity plans previous quarters meaning change one quarter results work update subsequent quarters laborious imprecise many teams process collecting data necessary generate demand forecasts slow errorprone time find capacity meet future demand resources equally suitable example latency requirements mean service must commit serve user demand continent user obtaining additional resources north america alleviate capacity shortfall asia every forecast constraints parameters around fulfilled constraints fundamentally related intent discussed next section mapping constrained resource requests allocations actual resources available capacity equally slow complex tedious bin pack requests limited space hand find solutions fit limited budget process may already paint grim picture make matters worse tools requires typically unreliable cumbersome spreadsheets suffer severely scalability problems limited errorchecking abilities data becomes stale tracking changes becomes difficult teams often forced make simplifying assumptions reduce complexity requirements simply render maintaining adequate capacity tractable problem service owners face challenges fitting series requests capacity various services resources available manner meets various constraints service may additional imprecision ensues bin packing nphard problem difficult human beings compute hand furthermore capacity request service generally inflexible set demand requirements x cores cluster reasons x cores cluster needed degrees freedom around parameters long lost time request reaches human trying fit list demands available supply net result massive expenditure human effort come bin packing approximate best process brittle change known bounds optimal solution solution intentbased capacity planning specify requirements implementation google many teams moved approach call intentbased capacity planning basic premise approach programmatically encode dependencies parameters intent service needs use encoding autogenerate allocation plan details resources go service cluster demand supply service requirements change simply autogenerate new plan response changed parameters new best distribution resources service true requirements flexibility captured capacity plan dramatically nimble face change reach optimal solution meets many parameters possible bin packing delegated computers human toil drastically reduced service owners focus highorder priorities like slos production dependencies service infrastructure requirements opposed lowlevel scrounging resources added benefit using computational optimization map intent implementation achieves much greater precision ultimately resulting cost savings organization bin packing still far solved problem certain types still considered nphard however today algorithms solve known optimal solution intentbased capacity planning intent rationale service owner wants run service moving concrete resource demands motivating reasons order arrive true capacity planning intent often requires several layers abstraction consider following chain abstraction want cores clusters x z service foo explicit resource request butwhy need many resources specifically particular clusters want core footprint clusters geographic region yyy service foo request introduces degrees freedom potentially easier fulfill although explain reasoning behind requirements butwhy need quantity resources footprints want meet service foo demand geographic region n redundancy suddenly greater flexibility introduced understand human level happens service foo receive resources butwhy need n service foo want run service foo nines reliability abstract requirement ramification requirement met becomes clear reliability suffer even greater flexibility perhaps running n actually sufficient optimal service deployment plan would suitable level intent used intentdriven capacity planning ideally levels intent supported together services benefiting shift specifying intent versus implementation google experience services tend achieve best wins cross step good degrees flexibility available ramifications request higherlevel understandable terms particularly sophisticated services may aim step precursors intent information need order capture service intent enter dependencies performance metrics prioritization dependencies services google depend many infrastructure userfacing services dependencies heavily influence service placed example imagine userfacing service foo depends upon bar infrastructure storage service foo expresses requirement bar must located within milliseconds network latency foo requirement important repercussions place foo bar intentdriven capacity planning must take constraints account furthermore production dependencies nested build upon preceding example imagine service bar dependencies baz lowerlevel distributed storage service qux application management service therefore place foo depends place bar baz qux given set production dependencies shared possibly different stipulations around intent performance metrics demand one service trickles result demand one services understanding chain dependencies helps formulate general scope bin packing problem still need information expected resource usage many compute resources service foo need serve n user queries every n queries service foo many mbps data expect service bar performance metrics glue dependencies convert one higherlevel resource type one lowerlevel resource type deriving appropriate performance metrics service involve load testing resource usage monitoring prioritization inevitably resource constraints result tradeoffs hard decisions many requirements services requirements sacrificed face insufficient capacity perhaps n redundancy service foo important n redundancy service bar perhaps feature launch x less important n redundancy service baz intentdriven planning forces decisions made transparently openly consistently resource constraints entail tradeoffs often prioritization ad hoc opaque service owners intentbased planning allows prioritization granular coarse needed introduction auxon auxon google implementation intentbased capacity planning resource allocation solution prime example sredesigned developed software engineering product built small group software engineers technical program manager within sre course two years auxon perfect case study demonstrate software development fostered within sre auxon actively used plan use many millions dollars machine resources google become critical component capacity planning several major divisions within google product auxon provides means collect intentbased descriptions service resource requirements dependencies user intents expressed requirements owner would like service provisioned requirements might specified request like service must n per continent frontend servers must ms away backend servers auxon collects information either via user configuration language via programmatic api thus translating human intent machineparseable constraints requirements prioritized feature useful resources insufficient meet requirements therefore tradeoffs must made requirementsthe intentare ultimately represented internally giant mixedinteger linear program auxon solves linear program uses resultant bin packing solution formulate allocation plan resources figure explanations follow outline auxon major components figure major components auxon performance data describes service scales every unit demand x cluster many units dependency z used scaling data may derived number ways depending maturity service question services load tested others infer scaling based upon past performance perservice demand forecast data describes usage trend forecasted demand signals services derive future usage demand forecasts a forecast queries per second broken continent services demand forecast services eg storage service like colossus derive demand purely services depend upon resource supply provides data availability baselevel fundamental resources example number machines expected available use particular point future linear program terminology resource supply acts upper bound limits services grow services placed ultimately want make best use resource supply intentbased description combined group services allows resource pricing provides data much baselevel fundamental resources cost instance cost machines may vary globally based upon spacepower charges given facility linear program terminology prices inform overall calculated costs act objective want minimize intent config key intentbased information fed auxon defines constitutes service services relate one another config ultimately acts configuration layer allows components wired together designed humanreadable configurable auxon configuration language engine acts based upon information receives intent config component formulates machinereadable request protocol buffer understood auxon solver applies light sanity checking configuration designed act gateway humanconfigurable intent definition machineparseable optimization request auxon solver brain tool formulates giant mixedinteger linear program based upon optimization request received configuration language engine designed scalable allows solver run parallel upon hundreds even thousands machines running within google clusters addition mixedinteger linear programming toolkits also components within auxon solver handle tasks scheduling managing pool workers descending decision trees allocation plan output auxon solver prescribes resources allocated services locations computed implementation details intentbased definition capacity planning problem requirements allocation plan also includes information requirements could satisfiedfor example requirement met due lack resources competing requirements otherwise strict requirements implementation successes lessons learned auxon first imagined sre technical program manager separately tasked respective teams capacity planning large portions google infrastructure performed manual capacity planning spreadsheets well positioned understand inefficiencies opportunities improvement automation features tool might require throughout auxon development sre team behind product continued deeply involved production world team maintained role oncall rotations several google services participated design discussions technical leadership services ongoing interactions team able stay grounded production world acted consumer developer product product failed team directly impacted feature requests informed team firsthand experiences firsthand experience problem space buy huge sense ownership product success also helped give product credibility legitimacy within sre approximation focus perfection purity solution especially bounds problem well known launch iterate sufficiently complex software engineering effort bound encounter uncertainty component designed problem tackled auxon met uncertainty early development linear programming world uncharted territory team members limitations linear programming seemed central part product would likely function well understood address team consternation insufficiently understood dependency opted initially build simplified solver engine socalled stupid solver applied simple heuristics services arranged based upon user specified requirements stupid solver would never yield truly optimal solution gave team sense vision auxon achievable even build something perfect day one deploying approximation help speed development important undertake work way allows team make future enhancements revisit approximation case stupid solver entire solver interface abstracted away within auxon solver internals could swapped later date eventually built confidence unified linear programming model simple operation switch stupid solver something well smarter auxon product requirements also unknowns building software fuzzy requirements frustrating challenge degree uncertainty need showstopper use fuzziness incentive ensure software designed general modular instance one aims auxon project integrate automation systems within google allow allocation plan directly enacted production assigning resources turning upturning downresizing services appropriate however time world automation systems great deal flux huge variety approaches use rather try design unique solutions allow auxon work individual tool instead shaped allocation plan universally useful automation systems could work integration points agnostic approach became key auxon process onboarding new customers allowed customers begin using auxon without switching particular turnup automation tool forecasting tool performance data tool also leveraged modular designs deal fuzzy requirements building model machine performance within auxon data future machine platform performance eg cpu scarce users wanted way model various scenarios machine power abstracted away machine data behind single interface allowing user swap different models future machine performance later extended modularity based increasingly defined requirements provide simple machine performance modeling library worked within interface one theme draw auxon case study old motto launch iterate particularly relevant sre software development projects wait perfect design rather keep overall vision mind moving ahead design development encounter areas uncertainty design software flexible enough process strategy changes higher level incur huge rework cost time stay grounded making sure general solutions realworldspecific implementation demonstrates utility design raising awareness driving adoption product sredeveloped software must designed knowledge users requirements needs drive adoption utility performance demonstrated ability benefit google production reliability goals better lives sres process socializing product achieving buyin across organization key project success underestimate effort required raise awareness interest software producta single presentation email announcement enough socializing internal software tools large audience demands following consistent coherent approach user advocacy sponsorship senior engineers management demonstrate utility product important consider perspective customer making product usable engineer might time inclination dig source code figure use tool although internal customers generally tolerant rough edges early alphas external customers still necessary provide documentation sres busy solution difficult confusing write solution set expectations engineer years familiarity problem space begins designing product easy imagine utopian endstate work however important differentiate aspirational goals product minimum success criteria minimum viable product projects lose credibility fail promising much soon time product promise sufficiently rewarding outcome difficult overcome necessary activation energy convince internal teams try something new demonstrating steady incremental progress via small releases raises user confidence team ability deliver useful software case auxon struck balance planning longterm roadmap alongside shortterm fixes teams promised onboarding configuration efforts would provide immediate benefit alleviating pain manually bin packing shortterm resource requests additional features developed auxon configuration files would carry provide new much broader longterm cost savings benefits project road map enabled services quickly determine use cases required features implemented early versions meanwhile auxon iterative development approach fed development priorities new milestones road map identify appropriate customers team developing auxon realized onesize solution might fit many larger teams already homegrown solutions capacity planning worked passably well custom tools perfect teams experience sufficient pain capacity planning process try new tool especially alpha release rough edges initial versions auxon intentionally targeted teams existing capacity planning processes place teams would invest configuration effort whether adopted existing tool new approach interested adopting newest tool early successes auxon achieved teams demonstrated utility project turned customers advocates tool quantifying usefulness product proved beneficial onboarded one google business areas team authored case study detailing process comparing results time savings reduction human toil alone presented huge incentive teams give auxon try customer service even though software developed within sre targets audience tpms engineers high technical proficiency sufficiently innovative software still presents learning curve new users afraid provide white glove customer support early adopters help onboarding process sometimes automation also entails host emotional concerns fear someone job replaced shell script working oneonone early users address fears personally demonstrate rather owning toil performing tedious task manually team instead owns configurations processes ultimate results technical work later adopters convinced happy examples early adopters furthermore google sre teams distributed across globe earlyadopter advocates project particularly beneficial serve local experts teams interested trying project designing right level idea termed agnosticismwriting software generalized allow myriad data sources inputwas key principle auxon design agnosticism meant customers required commit one tool order use auxon framework approach allowed auxon remain sufficient general utility even teams divergent use cases began use approached potential users message come work got avoiding overcustomizing one two big users achieved broader adoption across organization lowered barrier entry new services also consciously endeavored avoid pitfall defining success adoption across organization many cases diminishing returns closing last mile enable feature set sufficient every service long tail google team dynamics selecting engineers work sre software development product found great benefit creating seed team combines generalists able get speed quickly new topic engineers possessing breadth knowledge experience diversity experiences covers blind spots well pitfalls assuming every team use case essential team establish working relationship necessary specialists engineers comfortable working new problem space sre teams companies venturing new problem space requires outsourcing tasks working consultants sre teams larger organizations may able partner inhouse experts initial phases conceptualizing designing auxon presented design document google inhouse teams specialize operations research quantitative analysis order draw upon expertise field bootstrap auxon team knowledge capacity planning project development continued auxon feature set grew broad complex team acquired members backgrounds statistics mathematical optimization smaller company might akin bringing outside consultant inhouse new team members able identify areas improvement project basic functionality complete adding finesse become top priority right time engage specialists course vary project project rough guideline project successfully ground demonstrably successful skills current team would significantly bolstered additional expertise fostering software engineering sre makes project good candidate take leap oneoff tool fully fledged software engineering effort strong positive signals include engineers firsthand experience relative domain interested working project target user base highly technical therefore able provide highsignal bug reports early phases development project provide noticeable benefits reducing toil sres improving existing piece infrastructure streamlining complex process important project fit overall set objectives organization engineering leaders weigh potential impact subsequently advocate project reporting teams teams might interface teams crossorganizational socialization review help prevent disjoint overlapping efforts product easily established furthering departmentwide objective easier staff support makes poor candidate project many red flags might instinctively identify software project software touches many moving parts software design requires allornothing approach prevents iterative development google sre teams currently organized around services run sredeveloped projects particularly risk overly specific work benefits small percentage organization team incentives aligned primarily provide great experience users one particular service projects often fail generalize broader use case standardization across sre teams comes second place opposite end spectrum overly generic frameworks equally problematic tool strives flexible universal runs risk quite fitting use case therefore insufficient value projects grand scope abstract goals often require significant development effort lack concrete use cases required deliver enduser benefit reasonable time frame example broad use case layer load balancer developed google sres proved successful years repurposed customerfacing product offering via google cloud load balancer eis successfully building software engineering culture sre staffing development time sres often generalists desire learn breadthfirst instead depthfirst lends well understanding bigger picture pictures bigger intricate inner workings modern technical infrastructure engineers often strong coding software development skills may traditional swe experience part product team think customer feature requests quote engineer early sre software development project sums conventional sre approach software design doc need requirements partnering engineers tpms pms familiar userfacing software development help build team software development culture brings together best software product development handson production experience dedicated noninterrupted project work time essential software development effort dedicated project time necessary enable progress project nearly impossible write codemuch less concentrate larger impactful projectswhen thrashing several tasks course hour therefore ability work software project without interrupts often attractive reason engineers begin working development project time must aggressively defended majority software products developed within sre begin side projects whose utility leads grow become formalized point product may branch one several possible directions remain grassroots effort developed engineers spare time become established formal project structured processes see getting gain executive sponsorship within sre leadership expand fully staffed software development effort however scenariosand point worth stressingit essential sres involved development effort continue working sres instead becoming fulltime developers embedded sre organization immersion world production gives sres performing development work invaluable perspective creator customer product getting like idea organized software development sre probably wondering introduce software development model sre organization focused production support first recognize goal much organizational change technical challenge sres used working closely teammates quickly analyzing reacting problems therefore working natural instinct sre quickly write code meet immediate needs sre team small approach may problematic however organization grows ad hoc approach scale instead resulting largely functional yet narrow singlepurpose software solutions shared inevitably lead duplicated efforts wasted time next think want achieve developing software sre want foster better software development practices within team interested software development produces results used across teams possibly standard organization larger established organizations latter change take time possibly spanning multiple years change needs tackled multiple fronts higher payback following guidelines google experience create communicate clear message important define communicate strategy plans andmost importantlythe benefits sre gains effort sres skeptical lot fact skepticism trait specifically hire sre initial response effort likely sounds like much overhead never work start making compelling case strategy help sre example consistent supported software solutions speed rampup new sres reducing number ways perform task allows entire department benefit skills single team developed thus making knowledge effort portable across teams sres start ask questions strategy work rather strategy pursued know passed first hurdle evaluate organization capabilities sres many skills relatively common sre lack experience part team built shipped product set users order develop useful software effectively creating product team team includes required roles skills sre organization may formerly demanded someone play role product manager acting customer advocate tech lead project manager skills andor experience run agile development process begin filling gaps taking advantage skills already present company ask product development team help establish agile practices via training coaching solicit consulting time product manager help define product requirements prioritize feature work given large enough softwaredevelopment opportunity may case hire dedicated people roles making case hire roles easier positive experiment results launch iterate initiate sre software development program efforts followed many watchful eyes important establish credibility delivering product value reasonable amount time first round products aim relatively straightforward achievable targetsones without controversy existing solutions also found success pairing approach sixmonth rhythm product update releases provided additional useful features release cycle allowed teams focus identifying right set features build building features simultaneously learning productive software development team initial launch google teams moved pushongreen model even faster delivery feedback lower standards start develop software may tempted cut corners resist urge holding standards product development teams held example ask product created separate dev team would onboard product solution enjoys broad adoption may become critical sres order successfully perform jobs therefore reliability utmost importance proper code review practices place endtoend integration testing another sre team review product production readiness would onboarding service takes long time build credibility software development efforts short time lose credibility due misstep conclusions software engineering projects within google sre flourished organization grown many cases lessons learned successful execution earlier software development projects paved way subsequent endeavors unique handson production experience sres bring developing tools lead innovative approaches ageold problems seen development auxon address complex problem capacity planning sredriven software projects also noticeably beneficial company developing sustainable model supporting services scale sres often develop software streamline inefficient processes automate common tasks projects mean sre team scale linearly size services support ultimately benefits sres devoting time software development reaped company sre organization sres chapter load balancing frontend written piotr lewandowski edited sarah chavis serve many millions requests every second may already guessed use single computer handle demand even supercomputer somehow able handle requests imagine network connectivity configuration would require still employ strategy relied upon single point failure dealing largescale systems putting eggs one basket recipe disaster chapter focuses highlevel load balancinghow balance user traffic datacenters following chapter zooms explore implement load balancing inside datacenter power answer sake argument let assume unbelievably powerful machine network never fails would configuration sufficient meet google needs even configuration would still limited physical constraints associated networking infrastructure example speed light limiting factor communication speeds fiber optic cable creates upper bound quickly serve data based upon distance travel even ideal world relying infrastructure single point failure bad idea reality google thousands machines even users many issue multiple requests time traffic load balancing decide many many machines datacenters serve particular request ideally traffic distributed across multiple network links datacenters machines optimal fashion optimal mean context actually single answer optimal solution depends heavily variety factors hierarchical level evaluate problem global versus local technical level evaluate problem hardware versus software nature traffic dealing let start reviewing two common traffic scenarios basic search request video upload request users want get query results quickly important variable search request latency hand users expect video uploads take nonnegligible amount time also want requests succeed first time important variable video upload throughput differing needs two requests play role determine optimal distribution request global level search request sent nearest available datacenteras measured roundtrip time rtt because want minimize latency request video upload stream routed via different pathperhaps link currently underutilizedto maximize throughput expense latency local level inside given datacenter often assume machines within building equally distant user connected network therefore optimal distribution load focuses optimal resource utilization protecting single server overloading course example presents vastly simplified picture reality many considerations factor optimal load distribution requests may directed datacenter slightly farther away order keep caches warm noninteractive traffic may routed completely different region avoid network congestion load balancing especially large systems anything straightforward static google approached problem load balancing multiple levels two described following sections sake presenting concrete discussion consider http requests sent tcp load balancing stateless services like dns udp differs slightly mechanisms described applicable stateless services well load balancing using dns client even send http request often look ip address using dns provides perfect opportunity introduce first layer load balancing dns load balancing simplest solution return multiple aaaa records dns reply let client pick ip address arbitrarily conceptually simple trivial implement solution poses multiple challenges first problem provides little control client behavior records selected randomly attract roughly equal amount traffic mitigate problem theory could use srv records specify record weights priorities srv records yet adopted http another potential problem stems fact usually client determine closest address mitigate scenario using anycast address authoritative nameservers leverage fact dns queries flow closest address reply server return addresses routed closest datacenter improvement builds map networks approximate physical locations serves dns replies based mapping however solution comes cost much complex dns server implementation maintaining pipeline keep location mapping date course none solutions trivial due fundamental characteristic dns end users rarely talk authoritative nameservers directly instead recursive dns server usually lies somewhere end users nameservers server proxies queries user server often provides caching layer dns middleman three important implications traffic management recursive resolution ip addresses nondeterministic reply paths additional caching complications recursive resolution ip addresses problematic ip address seen authoritative nameserver belong user instead recursive resolver serious limitation allows reply optimization shortest distance resolver nameserver possible solution use edns extension proposed con includes information client subnet dns query sent recursive resolver way authoritative nameserver returns response optimal user perspective rather resolver perspective yet official standard obvious advantages led biggest dns resolvers opendns google support already difficult find optimal ip address return nameserver given user request nameserver may responsible serving thousands millions users across regions varying single office entire continent instance large national isp might run nameservers entire network one datacenter yet network interconnects metropolitan area isp nameservers would return response ip address best suited datacenter despite better network paths users finally recursive resolvers typically cache responses forward responses within limits indicated timetolive ttl field dns record end result estimating impact given reply difficult single authoritative reply may reach single user multiple thousands users solve problem two ways analyze traffic changes continuously update list known dns resolvers approximate size user base behind given resolver allows us track potential impact given resolver estimate geographical distribution users behind tracked resolver increase chance direct users best location estimating geographic distribution particularly tricky user base distributed across large regions cases make tradeoffs select best location optimize experience majority users best location really mean context dns load balancing obvious answer location closest user however determining users locations difficult additional criteria dns load balancer needs make sure datacenter selects enough capacity serve requests users likely receive reply also needs know selected datacenter network connectivity good shape directing user requests datacenter experiencing power networking problems ideal fortunately integrate authoritative dns server global control systems track traffic capacity state infrastructure third implication dns middleman related caching given authoritative nameservers flush resolvers caches dns records need relatively low ttl effectively sets lower bound quickly dns changes propagated users unfortunately little keep mind make load balancing decisions despite problems dns still simplest effective way balance load user connection even starts hand clear load balancing dns sufficient keep mind dns replies served fit within byte limit set rfc moc limit sets upper bound number addresses squeeze single dns reply number almost certainly less number servers really solve problem frontend load balancing initial level dns load balancing followed level takes advantage virtual ip addresses load balancing virtual ip address virtual ip addresses vips assigned particular network interface instead usually shared across many devices however user perspective vip remains single regular ip address theory practice allows us hide implementation details number machines behind particular vip facilitates maintenance schedule upgrades add machines pool without user knowing practice important part vip implementation device called network load balancer balancer receives packets forwards one machines behind vip backends process request several possible approaches balancer take deciding backend receive request first perhaps intuitive approach always prefer least loaded backend theory approach result best enduser experience requests always routed least busy machine unfortunately logic breaks quickly case stateful protocols must use backend duration request requirement means balancer must keep track connections sent order make sure subsequent packets sent correct backend alternative use parts packet create connection id possibly using hash function information packet use connection id select backend example connection id could expressed id packet mod n id function takes packet input produces connection id n number configured backends avoids storing state packets belonging single connection always forwarded backend success quite yet happens one backend fails needs removed backend list suddenly n becomes n id packet mod n becomes id packet mod n almost every packet suddenly maps different backend backends share state remapping forces reset almost existing connections scenario definitely best user experience even events infrequent fortunately alternate solution require keeping state every connection memory force connections reset single machine goes consistent hashing proposed consistent hashing kar describes way provide mapping algorithm remains relatively stable even new backends added removed list approach minimizes disruption existing connections pool backends changes result usually use simple connection tracking fall back consistent hashing system pressure eg ongoing denial service attack returning larger question exactly network load balancer forward packets selected vip backend one solution perform network address translation however requires keeping entry every single connection tracking table precludes completely stateless fallback mechanism another solution modify information data link layer layer osi networking model changing destination mac address forwarded packet balancer leave information upper layers intact backend receives original source destination ip addresses backend send reply directly original sendera technique known direct server response dsr user requests small replies large eg http requests dsr provides tremendous savings small fraction traffic need traverse load balancer even better dsr require us keep state load balancer device unfortunately using layer internal load balancing incur serious disadvantages deployed scale machines ie load balancers backends must able reach data link layer issue connectivity supported network number machines grow excessively machines need reside single broadcast domain may imagine google outgrew solution quite time ago find alternate approach current vip load balancing solution eis uses packet encapsulation network load balancer puts forwarded packet another ip packet generic routing encapsulation gre han uses backend address destination backend receiving packet strips outer ipgre layer processes inner ip packet delivered directly network interface network load balancer backend longer need exist broadcast domain even separate continents long route two exists packet encapsulation powerful mechanism provides great flexibility way networks designed evolve unfortunately encapsulation also comes price inflated packet size encapsulation introduces overhead bytes case ipvgre precise cause packet exceed available maximum transmission unit mtu size require fragmentation packet reaches datacenter fragmentation avoided using larger mtu within datacenter however approach requires network supports large protocol data units many things scale load balancing sounds simple surfaceload balance early load balance oftenbut difficulty details frontend load balancing handling packets reach datacenter see https groupsgooglecomforum topicpublicdns announceoxfjsleum sadly dns resolvers respect ttl value set authoritative nameservers otherwise users must establish tcp connection get list ip addresses chapter load balancing datacenter written alejandro forero cuervo edited sarah chavis chapter focuses load balancing within datacenter specifically discusses algorithms distributing work within given datacenter stream queries cover applicationlevel policies routing requests individual servers process lowerlevel networking principles eg switches packet routing datacenter selection outside scope chapter assume stream queries arriving datacenterthese could coming datacenter remote datacenters mix bothat rate exceed resources datacenter process exceeds short amounts time also assume services within datacenter queries operate services implemented many homogeneous interchangeable server processes mostly running different machines smallest services typically least three processes using fewer processes means losing capacity lose single machine largest may processes depending datacenter size typical case services composed processes call processes backend tasks backends tasks known client tasks hold connections backend tasks incoming query client task must decide backend task handle query clients communicate backends using protocol implemented top combination tcp udp note google datacenters house vastly diverse set services implement different combinations policies discussed chapter working example described fit one service directly generalized scenario allows us discuss various techniques found useful various services techniques may less applicable specific use cases techniques designed implemented several google engineers span many years techniques applied many parts stack example external http requests reach gfe google frontend http reverse proxying system gfe uses algorithms along algorithms described load balancing frontend route request payloads metadata individual processes running applications process information based configuration maps various url patterns individual applications control different teams order produce response payloads return gfe returned back browsers applications often use algorithms turn communicate infrastructure complementary services depend sometimes stack dependencies get relatively deep single incoming http request trigger long transitive chain dependent requests several systems potentially high fanout various points ideal case ideal case load given service spread perfectly backend tasks given point time least loaded backend tasks consume exactly amount cpu send traffic datacenter point loaded task reaches capacity limit depicted figure two scenarios time interval time crossdatacenter load balancing algorithm must avoid sending additional traffic datacenter risks overloading tasks figure two scenarios pertask load distribution time shown lefthand graph figure significant amount capacity wasted idle capacity every task except loaded task figure histogram cpu used wasted two scenarios formally let cpu cpu rate consumed task given point time suppose task loaded task case large spread wasting sum differences cpu task cpu sum tasks cpu cpu wasted case wasted means reserved unused example illustrates poor indatacenter load balancing practices artificially limit resource availability may reserving cpus service given datacenter unable actually use say cpus identifying bad tasks flow control lame ducks decide backend task receive client request need identifyand avoidunhealthy tasks pool backends simple approach unhealthy tasks flow control assume client tasks track number active requests sent connection backend task activerequest count reaches configured limit client treats backend unhealthy longer sends requests backends reasonable limit average case requests tend finish fast enough rare number active requests given client reach limit normal operating conditions basic form flow control also serves simplistic form load balancing given backend task becomes overloaded requests start piling clients avoid backend workload spreads organically among backend tasks unfortunately simplistic approach protects backend tasks extreme forms overload easy backends become overloaded well limit ever reached converse also true cases clients may reach limit backends still plenty spare resources example backends may longlived requests prohibit quick responses seen cases default limit backfired causing backend tasks become unreachable requests blocked clients time fail raising activerequest limit avoid situation solve underlying problem knowing task truly unhealthy simply slow respond robust approach unhealthy tasks lame duck state client perspective given backend task following states healthy backend task initialized correctly processing requests refusing connections backend task unresponsive happen task starting shutting backend abnormal state though would rare backend stop listening port shutting lame duck backend task listening port serve explicitly asking clients stop sending requests task enters lame duck state broadcasts fact active clients inactive clients google rpc implementation inactive clients ie clients active tcp connections still send periodic udp health checks result lame duck information propagated quickly clientstypically rttregardless current state main advantage allowing task exist quasioperational lame duck state simplifies clean shutdown avoids serving errors unlucky requests happened active backend tasks shutting bringing backend task active requests without serving errors facilitates code pushes maintenance activities machine failures may require restarting related tasks shutdown would follow general steps job scheduler sends sigterm signal backend task backend task enters lame duck state asks clients send new requests backend tasks done api call rpc implementation explicitly called sigterm handler ongoing request started backend task entered lame duck state entered lame duck state client detected executes normally responses flow back clients number active requests backend gradually decreases zero configured interval backend task either exits cleanly job scheduler kills interval set large enough value typical requests sufficient time finish value service dependent good rule thumb s s depending client complexity strategy also allows client establish connections backend tasks performing potentially longlived initialization procedures thus yet ready start serving backend tasks could otherwise start listening connections ready serve would delay negotiation connections unnecessarily soon backend task ready start serving signals explicitly clients limiting connections pool subsetting addition health management another consideration load balancing subsetting limiting pool potential backend tasks client task interacts client rpc system maintains pool longlived connections backends uses send new requests connections typically established early client starting usually remain open requests flowing client death alternative model would establish tear connection request model significant resource latency costs corner case connection remains idle long time rpc implementation optimization switches connection cheap inactive mode example frequency health checks reduced underlying tcp connection dropped favor udp every connection requires memory cpu due periodic health checking ends overhead small theory quickly become significant occurs across many machines subsetting avoids situation single client connects large number backend tasks single backend task receives connections large number client tasks cases potentially waste large amount resources little gain picking right subset picking right subset comes choosing many backend tasks client connects tothe subset sizeand selection algorithm typically use subset size backend tasks right subset size system depends heavily typical behavior service example may want use larger subset size number clients significantly smaller number backends case want number backends per client large enough end backend tasks never receive traffic frequent load imbalances within client jobs ie one client task sends requests others scenario typical situations clients occasionally send bursts requests case clients receive requests clients occasionally large fanout eg read information followers given user burst requests concentrated client assigned subset need larger subset size ensure load spread evenly across larger set available backend tasks subset size determined need algorithm define subset backend tasks client task use may seem like simple task becomes complex quickly working largescale systems efficient provisioning crucial system restarts guaranteed selection algorithm clients assign backends uniformly optimize resource provisioning example subsetting overloads one backend whole set backends needs overprovisioned algorithm also handle restarts failures gracefully robustly continuing load backends uniformly possible minimizing churn case churn relates backend replacement selection example backend task becomes unavailable clients may need temporarily pick replacement backend replacement backend selected clients must create new tcp connections likely perform applicationlevel negotiation creates additional overhead similarly client task restarts needs reopen connections backends algorithm also handle resizes number clients andor number backends minimal connection churn without knowing numbers advance functionality particularly important tricky entire set client backend tasks restarted one time eg push new version backends pushed want clients continue serving transparently little connection churn possible subset selection algorithm random subsetting naive implementation subset selection algorithm might client randomly shuffle list backends fill subset selecting resolvablehealthy backends list shuffling picking backends start list handles restarts failures robustly eg relatively little churn explicitly limits consideration however found strategy actually works poorly practical scenarios spreads load unevenly initial work load balancing implemented random subsetting calculated expected load various cases example consider clients backends subset size client connects backends figure shows least loaded backend average load connections average connections loaded connections cases subset size already larger would want use practice calculated load distribution changes every time run simulation general pattern remains figure connection distribution clients backends subset size unfortunately smaller subset sizes lead even worse imbalances example figure depicts results subset size reduced backends per client case least loaded backend receives average load connections loaded receives connections figure connection distribution clients backends subset size concluded random subsetting spread load relatively evenly across available tasks would need subset sizes large subset large simply impractical variance number clients connecting task large consider random subsetting good subset selection policy scale subset selection algorithm deterministic subsetting google solution limitations random subsetting deterministic subsetting following code implements algorithm described detail next def subset backends clientid subsetsize subsetcount len backends subsetsize group clients rounds round uses shuffled list round clientid subsetcount randomseed round randomshuffle backends subset id corresponding current client subsetid clientid subsetcount start subsetid subsetsize return backends start start subsetsize divide client tasks rounds round consists subsetcount consecutive client tasks starting task subsetcount subsetcount number subsets ie number backend tasks divided desired subset size within round backend assigned exactly one client except possibly last round may contain enough clients backends may assigned example backend tasks desired subset size rounds containing clients subsetcount clients preceding algorithm could yield following shuffledbackends round round round key point notice round assigns backend entire list one client except last run clients example every backend gets assigned exactly two three clients list shuffled otherwise clients assigned group consecutive backend tasks may become temporarily unavailable example backend job updated gradually order first task last different rounds use different seed shuffling backend fails load receiving spread among remaining backends subset additional backends subset fail effect compounds situation quickly worsen significantly n backends subset corresponding load spread remaining subsetsize n backends much better approach spread load remaining backends using different shuffle round use different shuffle round clients round start shuffled list clients across rounds different shuffled lists algorithm builds subset definitions based upon shuffled list backends desired subset size example subset shuffledbackends shuffledbackends subset shuffledbackends shuffledbackends subset shuffledbackends shuffledbackends subset shuffledbackends shuffledbackends shuffledbackend shuffled list created client assign subset client task take subset corresponds position within round eg client four subsets client client client use subset client client client use subset client client client use subset client client client use subset clients across rounds use different value shuffledbackends thus subset clients within rounds use different subsets connection load spread uniformly cases total number backends divisible desired subset size allow subsets slightly larger others cases number clients assigned backend differ figure shows distribution former example clients connecting backends yields good results backend receives exactly number connections figure connection distribution clients deterministic subsetting backends load balancing policies established groundwork given client task maintains set connections known healthy let examine load balancing policies mechanisms used client tasks select backend task subset receives client request many complexities load balancing policies stem distributed nature decisionmaking process clients need decide real time partial andor stale backend state information backend used request load balancing policies simple take account information state backends eg round robin act information backends eg leastloaded round robin weighted round robin simple round robin one simple approach load balancing client send requests roundrobin fashion backend task subset successfully connect lame duck state many years common approach still used many services unfortunately round robin advantage simple performing significantly better selecting backend tasks randomly results policy poor actual numbers depend many factors varying query cost machine diversity found round robin result spread x cpu consumption least loaded task spread extremely wasteful occurs number reasons including small subsetting varying query costs machine diversity unpredictable performance factors small subsetting one simplest reasons round robin distributes load poorly clients may issue requests rate different rates requests among clients especially likely vastly different processes share backends case especially using relatively small subset sizes backends subsets clients generating traffic naturally tend loaded varying query costs many services handle requests require vastly different amounts resources processing practice found semantics many services google expensive requests consume x cpu cheapest requests load balancing using round robin even difficult query cost predicted advance example query return emails received user xyz last day could cheap user received little email course day extremely expensive load balancing system large discrepancies potential query cost problematic become necessary adjust service interfaces functionally cap amount work done per request example case email query described previously could introduce pagination interface change semantics request return recent emails fewer received user xyz last day unfortunately often difficult introduce semantic changes require changes client code also entails additional consistency considerations example user may receiving new emails deleting emails client fetches emails pagebypage use case client naively iterates results concatenates responses rather paginating based fixed view data likely produce inconsistent view repeating messages andor skipping others keep interfaces implementations simple services often defined allow expensive requests consume even times resources cheapest requests however varying resource requirements perrequest naturally mean backend tasks unlucky occasionally receive expensive requests others extent situation affects load balancing depends expensive expensive requests example one java backends queries consume around ms cpu average queries easily require seconds task backend reserves multiple cpu cores reduces latency allowing computations happen parallel despite reserved cores backend receives one large queries load increases significantly seconds poorly behaved task may run memory even stop responding entirely eg due memory thrashing even normal case ie backend sufficient resources load normalizes large query completes latency requests suffers due resource competition expensive request machine diversity another challenge simple round robin fact machines datacenter necessarily given datacenter may machines cpus varying performance therefore request may represent significantly different amount work different machines dealing machine diversitywithout requiring strict homogeneitywas challenge many years google theory solution working heterogeneous resource capacity fleet simple scale cpu reservations depending processormachine type however practice rolling solution required significant effort required job scheduler account resource equivalencies based average machine performance across sampling services example cpu units machine x slow machine equivalent cpu units machine fast machine information job scheduler required adjust cpu reservations process based upon equivalence factor type machine process scheduled attempt mitigate complexity created virtual unit cpu rate called gcu google compute units gcus became standard modeling cpu rates used maintain mapping cpu architecture datacenters corresponding gcu based upon performance unpredictable performance factors perhaps largest complicating factor simple round robin machines or accurately performance backend tasksmay differ vastly due several unpredictable aspects accounted statically two many unpredictable factors contribute performance include antagonistic neighbors processes often completely unrelated run different teams significant impact performance processes seen differences performance nature difference mostly stems competition shared resources space memory caches bandwidth ways may directly obvious example latency outgoing requests backend task grows competition network resources antagonistic neighbor number active requests also grow may trigger increased garbage collection task restarts task gets restarted often requires significantly resources minutes one example seen condition affect platforms java optimize code dynamically others response actually added logic server codewe keep servers lame duck state prewarm triggering optimizations period time start performance nominal effect task restarts become sizable problem consider update many servers eg push new builds requires restarting tasks every day load balancing policy adapt unforeseen performance limitations inherently end suboptimal load distribution working scale leastloaded round robin alternative approach simple round robin client task keep track number active requests backend task subset use round robin among set tasks minimal number active requests example suppose client uses subset backend tasks t t currently following number active requests backend t t t t t t t t t t new request client would filter list potential backend tasks tasks least number connections t t t t t choose backend list let assume picks t client connection state table would look like following t t t t t t t t t t assuming none current requests completed next request backend candidate pool becomes t t t t let fastforward issued four new requests still assuming request finishes meantime connection state table would look like following t t t t t t t t t t point set backend candidates tasks except t t however request task t finishes current state becomes active requests new request assigned t implementation actually uses round robin applied across set tasks minimal active requests without filtering policy might able spread requests well enough avoid situation portion available backend tasks goes unused idea behind leastloaded policy loaded tasks tend higher latency spare capacity strategy naturally take load away loaded tasks said learned hard way one dangerous pitfall leastloaded round robin approach task seriously unhealthy might start serving errors depending nature errors may low latency frequently significantly faster return unhealthy error actually process request result clients might start sending large amount traffic unhealthy task erroneously thinking task available opposed fastfailing say unhealthy task sinkholing traffic fortunately pitfall solved relatively easily modifying policy count recent errors active requests way backend task becomes unhealthy load balancing policy begins divert load way would divert load overburdened task leastloaded round robin two important limitations count active requests may good proxy capability given backend many requests spend significant portion life waiting response network ie waiting responses requests initiate backends little time actual processing example one backend task may able process twice many requests another eg running machine cpu twice fast rest latency requests may still roughly latency requests task requests spend life waiting network respond case blocking io often consumes zero cpu little ram bandwidth still want send twice many requests faster backend however leastloaded round robin consider backend tasks equally loaded count active requests client include requests clients backends client task limited view state backend tasks view requests practice found large services using leastloaded round robin see loaded backend task using twice much cpu least loaded performing poorly round robin weighted round robin weighted round robin important load balancing policy improves simple leastloaded round robin incorporating backendprovided information decision process weighted round robin fairly simple principle client task keeps capability score backend subset requests distributed roundrobin fashion clients weigh distributions requests backends proportionally response including responses health checks backends include current observed rates queries errors per second addition utilization typically cpu usage clients adjust capability scores periodically pick backend tasks based upon current number successful requests handled utilization cost failed requests result penalty affects future decisions practice weighted round robin worked well significantly reduced difference least utilized tasks figure shows cpu rates random subset backend tasks around time clients switched leastloaded weighted round robin spread least loaded tasks decreased drastically figure cpu distribution enabling weighted round robin chapter handling overload written alejandro forero cuervo edited sarah chavis avoiding overload goal load balancing policies matter efficient load balancing policy eventually part system become overloaded gracefully handling overload conditions fundamental running reliable serving system one option handling overload serve degraded responses responses accurate contain less data normal responses easier compute example instead searching entire corpus provide best available results search query search small percentage candidate set rely local copy results may fully date cheaper use going canonical storage however extreme overload service might even able compute serve degraded responses point may immediate option serve errors one way mitigate scenario balance traffic across datacenters datacenter receives traffic capacity process example datacenter runs backend tasks task process requests per second load balancing algorithm allow queries per second sent datacenter however even constraint prove insufficient avoid overload re operating scale end day s best build clients backends handle resource restrictions gracefully redirect possible serve degraded results necessary handle resource errors transparently else fails pitfalls queries per second different queries vastly different resource requirements query s cost vary based arbitrary factors code client issues services many different clients even time day eg home users versus work users interactive enduser traffic versus batch traffic learned lesson hard way modeling capacity queries per second using static features requests believed proxy resources consume eg many keys requests reading often makes poor metric even metrics perform adequately one point time ratios change sometimes change gradual sometimes change drastic eg new version software suddenly made features requests require significantly fewer resources moving target makes poor metric designing implementing load balancing better solution measure capacity directly available resources example may total cpu cores tb memory reserved given service given datacenter naturally works much better use numbers directly model datacenter s capacity often speak cost request refer normalized measure much cpu time consumed different cpu architectures consideration performance differences majority cases although certainly ve found simply using cpu consumption signal provisioning works well following reasons platforms garbage collection memory pressure naturally translates increased cpu consumption platforms s possible provision remaining resources way re unlikely run cpu runs cases overprovisioning noncpu resources prohibitively expensive take system resource account separately considering resource consumption percustomer limits one component dealing overload deciding case global overload perfect world teams coordinate launches carefully owners backend dependencies global overload never happens backend services always enough capacity serve customers unfortunately nt live perfect world reality global overload occurs quite frequently especially internal services tend many clients run many teams global overload occur s vital service delivers error responses misbehaving customers customers remain unaffected achieve outcome service owners provision capacity based negotiated usage customers define percustomer quotas according agreements example backend service cpus allocated worldwide various datacenters percustomer limits might look something like following gmail allowed consume cpu seconds per second calendar allowed consume cpu seconds per second android allowed consume cpu seconds per second google allowed consume cpu seconds per second every user allowed consume cpu seconds per second note numbers may add cpus allocated backend service service owner relying fact s unlikely customers hit resource limits simultaneously aggregate global usage information real time backend tasks use data push effective limits individual backend tasks closer look system implements logic outside scope discussion ve written significant code implement backend tasks interesting part puzzle computing real time amount resources specifically cpuconsumed individual request computation particularly tricky servers nt implement threadperrequest model pool threads executes different parts requests come using nonblocking apis clientside throttling customer quota backend task reject requests quickly expectation returning customer quota error consumes significantly fewer resources actually processing request serving back correct response however logic nt hold true services example s almost equally expensive reject request requires simple ram lookup overhead requestresponse protocol handling significantly larger overhead producing response accept run request even case rejecting requests saves significant resources requests still consume resources amount rejected requests significant numbers add quickly cases backend become overloaded even though vast majority cpu spent rejecting requests clientside throttling addresses problem client detects significant portion recent requests rejected due quota errors starts selfregulating caps amount outgoing traffic generates requests cap fail locally without even reaching network implemented clientside throttling technique call adaptive throttling specifically client task keeps following information last two minutes history requests number requests attempted application layer client top adaptive throttling system accepts number requests accepted backend normal conditions two values equal backend starts rejecting traffic number accepts becomes smaller number requests clients continue issue requests backend requests k times large accepts cutoff reached client begins selfregulate new requests rejected locally ie client probability calculated client request rejection probability client request rejection probability client starts rejecting requests requests continue exceed accepts may seem counterintuitive given locally rejected requests nt actually propagated backend preferred behavior rate application attempts requests client grows relative rate backend accepts want increase probability dropping new requests services cost processing request close cost rejecting request allowing roughly half backend resources consumed rejected requests unacceptable case solution simple modify accepts multiplier k eg client request rejection probability client request rejection probability way reducing multiplier make adaptive throttling behave aggressively increasing multiplier make adaptive throttling behave less aggressively example instead client selfregulate requests accepts selfregulate requests accepts reducing modifier means one request rejected backend every requests accepted generally prefer x multiplier allowing requests reach backend expected actually allowed waste resources backend also speed propagation state backend clients example backend decides stop rejecting traffic client tasks delay client tasks detected change state shorter ve found adaptive throttling work well practice leading stable rates requests overall even large overload situations backends end rejecting one request request actually process one large advantage approach decision made client task based entirely local information using relatively simple implementation additional dependencies latency penalties one additional consideration clientside throttling may work well clients sporadically send requests backends case view client state backend reduced drastically approaches increment visibility tend expensive criticality criticality another notion ve found useful context global quotas throttling request made backend associated one four possible criticality values depending critical consider request criticalplus reserved critical requests result serious uservisible impact fail critical default value requests sent production jobs requests result uservisible impact impact may less severe criticalplus services expected provision enough capacity expected critical criticalplus traffic sheddableplus traffic partial unavailability expected default batch jobs retry requests minutes even hours later sheddable traffic frequent partial unavailability occasional full unavailability expected found four values sufficiently robust model almost every service ve various discussions proposals add values would allow us classify requests finely however defining additional values would require resources operate various criticalityaware systems ve made criticality firstclass notion rpc system ve worked hard integrate many control mechanisms taken account reacting overload situations example customer runs global quota backend task reject requests given criticality s already rejecting requests lower criticalities fact percustomer limits system supports described earlier set per criticality task overloaded reject requests lower criticalities sooner adaptive throttling system also keeps separate stats criticality criticality request orthogonal latency requirements thus underlying network quality service qos used example system displays search results suggestions user typing search query underlying requests highly sheddable system overloaded s acceptable display results tend stringent latency requirements ve also significantly extended rpc system propagate criticality automatically backend receives request part executing request issues outgoing request b request c backends request b request c use criticality request default past many systems google evolved ad hoc notions criticality often incompatible across services standardizing propagating criticality part rpc system able consistently set criticality specific points means confident overloaded dependencies abide desired highlevel criticality reject traffic regardless deep rpc stack practice thus set criticality close possible browsers mobile clientstypically http frontends produce html returned and override criticality specific cases makes sense specific points stack utilization signals implementation tasklevel overload protection based notion utilization many cases utilization measurement cpu rate ie current cpu rate divided total cpus reserved task cases also factor measurements portion memory reserved currently used utilization approaches configured thresholds start rejecting requests based criticality higher thresholds higher criticalities utilization signals use based state local task since goal signals protect task implementations various signals generally useful signal based load process determined using system call executor load average find executor load average count number active threads process case active refers threads currently running ready run waiting free processor smooth value exponential decay begin rejecting requests number active threads grows beyond number processors available task means incoming request large fanout ie one schedules burst large number shortlived operations cause load spike briefly smoothing mostly swallow spike however operations shortlived ie load increases remains high significant amount time task start rejecting requests executor load average proven useful signal system plug utilization signal particular backend may need example might use memory pressurewhich indicates whether memory usage backend task grown beyond normal operational parametersas another possible utilization signal system also configured combine multiple signals reject requests would surpass combined individual target utilization thresholds handling overload errors addition handling load gracefully ve put significant amount thought clients react receive loadrelated error response case overload errors distinguish two possible situations large subset backend tasks datacenter overloaded crossdatacenter load balancing system working perfectly ie propagate state react instantaneously shifts traffic condition occur small subset backend tasks datacenter overloaded situation typically caused imperfections load balancing inside datacenter example task may recently received expensive request case likely datacenter remaining capacity tasks handle request large subset backend tasks datacenter overloaded requests retried errors bubble way caller eg returning error end user s much typical small portion tasks become overloaded case preferred response retry request immediately general crossdatacenter load balancing system tries direct traffic clients nearest available backend datacenters cases nearest datacenter far away eg client may nearest available backend different continent usually manage situate clients close backends way additional latency retrying requestjust network round tripstends negligible point view load balancing policies retries requests indistinguishable new requests nt use explicit logic ensure retry actually goes different backend task rely likely probability retry land different backend task simply virtue number participating backends subset ensuring retries actually go different task would incur complexity apis worthwhile even backend slightly overloaded client request often better served backend rejects retry new requests equally quickly requests retried immediately different backend task may spare resources consequence treating retries new requests identically backend retrying requests different tasks becomes form organic load balancing redirects load tasks may better suited requests deciding retry client receives task overloaded error response needs decide whether retry request mechanisms place avoid retries significant portion tasks cluster overloaded first implement perrequest retry budget three attempts request already failed three times let failure bubble caller rationale request already landed overloaded tasks three times s relatively unlikely attempting help whole datacenter likely overloaded secondly implement perclient retry budget client keeps track ratio requests correspond retries request retried long ratio rationale small subset tasks overloaded relatively little need retry concrete example worstcase scenario let s assume datacenter accepting small amount requests rejecting large portion requests let x total rate requests attempted datacenter according clientside logic due number retries occur number requests grow significantly somewhere x although ve effectively capped growth caused retries threefold increase requests significant especially cost rejecting versus processing request considerable however layering perclient retry budget retry ratio reduces growth x general casea significant improvement third approach clients include counter many times request already tried request metadata instance counter starts first attempt incremented every retry reaches point perrequest budget causes stop retried backends keep histograms values recent history backend needs reject request consults histograms determine likelihood backend tasks also overloaded histograms reveal significant amount retries indicating backend tasks likely also overloaded return overloaded nt retry error response instead standard task overloaded error triggers retries figure shows number attempts request received given backend task various example situations sliding window corresponding initial requests counting retries simplicity perclient retry budget ignored ie numbers assume limit retries retry budget three attempts per request subsetting could alter numbers somewhat figure histograms attempts various conditions larger services tend deep stacks systems may turn dependencies architecture requests retried layer immediately layer rejecting decide given request ca nt served nt retried use overloaded nt retry error thus avoid combinatorial retry explosion consider example figure practice stacks often significantly complex imagine db frontend currently overloaded rejects request case backend b retry request according preceding guidelines however backend b determines request db frontend ca nt served example request already attempted rejected three times backend b return backend either overloaded nt retry error degraded response assuming produce moderately useful response even request db frontend failed backend exactly options request received frontend proceeds accordingly figure stack dependencies key point failed request db frontend retried backend b layer immediately multiple layers retried d combinatorial explosion load connections load associated connections one last factor worth mentioning sometimes take account load backends caused directly requests receive one problems approaches model load based upon queries per second however overlooks cpu memory costs maintaining large pool connections cost fast rate churn connections issues negligible small systems quickly become problematic running largescale rpc systems mentioned previously rpc protocol requires inactive clients perform periodic health checks connection idle configurable amount time client drops tcp connection switches udp health checking unfortunately behavior problematic large number client tasks issue low rate requests health checking connections require resources actually serving requests approaches carefully tuning connection parameters eg significantly decreasing frequency health checks even creating destroying connections dynamically significantly improve situation handling bursts new connection requests second related problem ve seen bursts type happen case large batch jobs create large number worker client tasks need negotiate maintain excessive number new connections simultaneously easily overload group backends experience couple strategies help mitigate load expose load crossdatacenter load balancing algorithm eg base load balancing utilization cluster rather number requests case load requests effectively rebalanced away datacenters spare capacity mandate batch client jobs use separate set batch proxy backend tasks nothing forward requests underlying backends hand responses back clients controlled way therefore instead batch client backend batch client batch proxy backend case large job starts batch proxy job suffers shielding actual backends higherpriority clients effectively batch proxy acts like fuse another advantage using proxy typically reduces number connections backend improve load balancing backend eg proxy tasks use bigger subsets probably better view state backend tasks conclusions chapter load balancing datacenter discussed various techniques deterministic subsetting weighted round robin clientside throttling customer quotas etc help spread load tasks datacenter relatively evenly however mechanisms depend propagation state distributed system perform reasonably well general case realworld application resulted small number situations work imperfectly result consider critical ensure individual tasks protected overload state simply backend task provisioned serve certain traffic rate continue serve traffic rate without significant impact latency regardless much excess traffic thrown task corollary backend task fall crash load statements hold true certain rate traffic somewhere x even x task provisioned process accept might certain point system begins break raising threshold breakdown occurs becomes relatively difficult achieve key take degradation conditions seriously degradation conditions ignored many systems exhibit terrible behavior work piles tasks eventually run memory crash end burning almost cpu memory thrashing latency suffers traffic dropped tasks compete resources left unchecked failure subset system individual backend task might trigger failure system components potentially causing entire system considerable subset fail impact kind cascading failure severe s critical system operating scale protect see addressing cascading failures s common mistake assume overloaded backend turn stop accepting traffic however assumption actually goes counter goal robust load balancing actually want backend continue accepting much traffic possible accept load capacity frees wellbehaved backend supported robust load balancing policies accept requests process reject rest gracefully vast array tools implement good load balancing overload protections magic bullet load balancing often requires deep understanding system semantics requests techniques described chapter evolved along needs many systems google likely continue evolve nature systems continues change for example see doorman provides cooperative distributed client side throttling system chapter addressing cascading failures written mike ulrich first nt succeed back exponentially dan sandler google software engineer people always forget need add little jitter ade oshineye google developer advocate cascading failure failure grows time result positive feedback occur portion overall system fails increasing probability portions system fail example single replica service fail due overload increasing load remaining replicas increasing probability failing causing domino effect takes replicas service use shakespeare search service discussed shakespeare sample service example throughout chapter production configuration might look something like figure figure example production configuration shakespeare search service causes cascading failures designing avoid wellthoughtout system design take account typical scenarios account majority cascading failures server overload common cause cascading failures overload cascading failures described either directly due server overload due extensions variations scenario suppose frontend cluster handling requests per second qps figure figure normal server load distribution clusters b cluster b fails figure requests cluster increase qps frontends able handle requests qps therefore start running resources causes crash miss deadlines otherwise misbehave result rate successfully handled requests dips well qps figure cluster b fails sending traffic cluster reduction rate useful work done spread failure domains potentially spreading globally example local overload one cluster may lead servers crashing response load balancing controller sends requests clusters overloading servers leading servicewide overload failure may take long events transpire eg order couple minutes load balancer task scheduling systems involved may act quickly resource exhaustion running resource result higher latency elevated error rates substitution lowerquality results fact desired effects running resources something eventually needs give load increases beyond server handle depending resource becomes exhausted server server built resource exhaustion render server less efficient cause server crash prompting load balancer distribute resource problems servers happens rate successfully handled requests drop possibly send cluster entire service cascade failure different types resources exhausted resulting varying effects servers cpu insufficient cpu handle request load typically requests become slower scenario result various secondary effects including following increased number inflight requests requests take longer handle requests handled concurrently possible maximum capacity queuing may occur affects almost resources including memory number active threads threadperrequest server model number file descriptors backend resources turn effects excessively long queue lengths insufficient capacity handle requests steady state server saturate queues means latency increases requests queued longer amounts time queue uses memory see queue management discussion mitigation strategies thread starvation thread make progress waiting lock health checks may fail health check endpoint served time cpu request starvation internal watchdogs server detect server making progress causing servers crash due cpu starvation due request starvation watchdog events triggered remotely processed part request queue missed rpc deadlines server becomes overloaded responses rpcs clients arrive later may exceed deadlines clients set work server respond wasted clients may retry rpcs leading even overload reduced cpu caching benefits cpu used chance spilling cores increases resulting decreased usage local caches decreased cpu efficiency memory nothing else inflight requests consume ram allocating request response rpc objects memory exhaustion cause following effects dying tasks example task might evicted container manager vm otherwise exceeding available resource limits applicationspecific crashes may cause tasks die increased rate garbage collection gc java resulting increased cpu usage vicious cycle occur scenario less cpu available resulting slower requests resulting increased ram usage resulting gc resulting even lower availability cputhis known colloquially gc death spiral reduction cache hit rates reduction available ram reduce applicationlevel cache hit rates resulting rpcs backends possibly cause backends become overloaded threads thread starvation directly cause errors lead health check failures server adds threads needed thread overhead use much ram extreme cases thread starvation also cause run process ids file descriptors running file descriptors lead inability initialize network connections turn cause health checks fail dependencies among resources note many resource exhaustion scenarios feed one anothera service experiencing overload often host secondary symptoms look like root cause making debugging difficult example imagine following scenario java frontend poorly tuned garbage collection gc parameters high expected load frontend runs cpu due gc cpu exhaustion slows completion requests increased number inprogress requests causes ram used process requests memory pressure due requests combination fixed memory allocation frontend process whole leaves less ram available caching reduced cache size means fewer entries cache addition lower hit rate increase cache misses means requests fall backend servicing backend turn runs cpu threads finally lack cpu causes basic health checks fail starting cascading failure situations complex preceding scenario unlikely causal chain fully diagnosed outage might hard determine backend crash caused decrease cache rate frontend particularly frontend backend components different owners service unavailability resource exhaustion lead servers crashing example servers might crash much ram allocated container couple servers crash overload load remaining servers increase causing crash well problem tends snowball soon servers begin crashloop often difficult escape scenario soon servers come back online bombarded extremely high rate requests fail almost immediately example service healthy qps started cascading failure due crashes qps dropping load qps almost certainly stop crashes service handling increased demand reduced capacity small fraction servers usually healthy enough handle requests fraction servers healthy depends factors quickly system able start tasks quickly binary start serving full capacity long freshly started task able survive load example servers healthy enough handle requests request rate would need drop qps order system stabilize recover similarly servers appear unhealthy load balancing layer resulting reduced load balancing capacity servers may go lame duck state see robust approach unhealthy tasks lame duck state fail health checks without crashing effect similar crashing servers appear unhealthy healthy servers tend accept requests brief period time becoming unhealthy fewer servers participate handling requests load balancing policies avoid servers served errors exacerbate problems furthera backends serve errors contribute available capacity service increases load remaining servers starting snowball effect preventing server overload following list presents strategies avoiding server overload rough priority order load test server capacity limits test failure mode overload important exercise conduct order prevent server overload unless test realistic environment hard predict exactly resource exhausted resource exhaustion manifest details see testing cascading failures serve degraded results serve lowerquality cheapertocompute results user strategy servicespecificsee load shedding graceful degradation instrument server reject requests overloaded servers protect becoming overloaded crashingwhen overloaded either frontend backend layers fail early cheaply details see load shedding graceful degradation instrument higherlevel systems reject requests rather overloading servers note rate limiting often take overall service health account may able stop failure already begunsimple ratelimiting implementations also likely leave capacity unused rate limiting implemented number places reverse proxies limiting volume requests criteria ip address mitigate attempted denialofservice attacks abusive clients load balancers dropping requests service enters global overload depending nature complexity service rate limiting indiscriminate drop traffic x requests per second selective drop requests users recently interacted service drop requests lowpriority operations like background synchronization keep serving interactive user sessions individual tasks prevent random fluctuations load balancing overwhelming server perform capacity planning good capacity planning reduce probability cascading failure occurcapacity planning coupled performance testing determine load service failfor instance every cluster breaking point qps load evenly spread across clusters service peak load qps approximately six clusters needed run service n capacity planning reduces probability triggering cascading failure sufficient protect service cascading failures lose major parts infrastructure planned unplanned event amount capacity planning may sufficient prevent cascading failures load balancing problems network partitions unexpected traffic increases create pockets high load beyond planned systems grow number tasks service demand may prevent overload however proper capacity planning still needed queue management threadperrequest servers use queue front thread pool handle requestsrequests come sit queue threads pick requests queue perform actual work whatever actions required server usually queue full server reject new requests request rate latency given task constant reason queue requests constant number threads occupied idealized scenario requests queued steady state rate incoming requests exceeds rate server process requests results saturation thread pool queue queued requests consume memory increase latency example queue size x number threads time handle request thread milliseconds queue full request take seconds handle time spent queue system fairly steady traffic time usually better small queue lengths relative thread pool size eg less results server rejecting requests early sustain rate incoming requests example gmail often uses queueless servers relying instead failover server tasks threads full end spectrum systems bursty load traffic patterns fluctuate drastically may better queue size based current number threads use processing time request size frequency bursts load shedding graceful degradation load shedding drops proportion load dropping traffic server approaches overload conditions goal keep server running ram failing health checks serving extremely high latency symptoms associated overload still much useful work one straightforward way shed load pertask throttling based cpu memory queue length limiting queue length discussed queue management form strategy example one effective approach return http service unavailable incoming request given number client requests flight changing queuing method standard firstin firstout fifo lastin firstout lifo using controlled delay codel algorithm nic similar approaches reduce load removing requests unlikely worth processing mau user web search slow rpc queued seconds good chance user given refreshed browser issuing another request point responding first one since ignored strategy works well combined propagating rpc deadlines throughout stack described latency deadlines sophisticated approaches include identifying clients selective work dropped picking requests important prioritizingsuch strategies likely needed shared services graceful degradation takes concept load shedding one step reducing amount work needs performed applications possible significantly decrease amount work time needed decreasing quality responses instance search application might search subset data stored inmemory cache rather full ondisk database use lessaccurate faster ranking algorithm overloaded evaluating load shedding graceful degradation options service consider following metrics use determine load shedding graceful degradation kick eg cpu usage latency queue length number threads used whether service enters degraded mode automatically manual intervention necessary actions taken server degraded mode layer load shedding graceful degradation implemented make sense implement strategies every layer stack sufficient highlevel chokepoint evaluate options deploy keep following mind graceful degradation trigger oftenusually cases capacity planning failure unexpected load shift keep system simple understandable particularly used often remember code path never use code path often work steadystate operation graceful degradation mode used implying much less operational experience mode quirks increases level risk make sure graceful degradation stays working regularly running small subset servers near overload order exercise code path monitor alert many servers enter modes complex load shedding graceful degradation cause problems themselvesexcessive complexity may cause server trip degraded mode desired enter feedback cycles undesired times design way quickly turn complex graceful degradation tune parameters needed storing configuration consistent system server watch changes chubby increase deployment speed also introduces risks synchronized failure retries suppose code frontend talks backend implements retries naivelyit retries encountering failure caps number backend rpcs per logical request consider code frontend using grpc go func examplerpccall client pbexampleclient request pbrequest pbresponse set rpc timeout seconds opts grpcwithtimeout timesecond try times make rpc call attempts attempts conn err grpcdial serveraddr opts err nil something went wrong setting connection try attemptscontinue defer connclose create client stub make rpc call client pbnewbackendclient conn response err clientmakerequest contextbackground request err nil something went wrong making call try attemptscontinue return response grpclogfatalf ran attempts system cascade following way assume backend known limit qps per task point requests rejected attempt graceful degradation frontend calls makerequest constant rate qps overloads backend qps backend rejects failed qps retried makerequest every ms probably succeed retries adding requests sent backend receives qps qps failing due overload volume retries grows qps retries first second leads qps qps fewer fewer requests able succeed first attempt less useful work performed fraction requests backend backend task unable handle increase loadwhich consuming file descriptors memory cpu time backendit melt crash sheer load requests retries crash redistributes requests receiving across remaining backend tasks turn overloading tasks simplifying assumptions made illustrate scenario point remains retries destabilize system note temporary load spikes slow increases usage cause effect even rate calls makerequest decreases premeltdown levels qps example depending much returning failure costs backend problem might go away two factors play backend spends significant amount resources processing requests ultimately fail due overload retries may keeping backend overloaded mode backend servers may stable retries amplify effects seen server overload either conditions true order dig outage must dramatically reduce eliminate load frontends retries stop backends stabilize pattern contributed several cascading failures whether frontends backends communicate via rpc messages frontend client javascript code issuing xmlhttprequest calls endpoint retries failure retries originate offline sync protocol retries aggressively encounters failure issuing automatic retries keep mind following considerations backend protection strategies described preventing server overload apply particular testing system highlight problems graceful degradation reduce effect retries backend always use randomized exponential backoff scheduling retries see also exponential backoff jitter aws architecture blog bro retries randomly distributed retry window small perturbation eg network blip cause retry ripples schedule time amplify flo limit retries per request retry given request indefinitely consider serverwide retry budget example allow retries per minute process retry budget exceeded retry fail request strategy contain retry effect difference capacity planning failure leads dropped queries global cascading failure think service holistically decide really need perform retries given level particular avoid amplifying retries issuing retries multiple levels single request highest layer may produce number attempts large product number attempts layer lowest layer database service requests overloaded backend frontend javascript layers issue retries attempts single user action may create attempts databasethis behavior undesirable database returning errors overloaded use clear response codes consider different failure modes handled example separate retriable nonretriable error conditions retry permanent errors malformed requests client neither ever succeed return specific status overloaded clients layers back retry emergency may obvious outage due bad retry behavior graphs retry rates indication bad retry behavior may confused symptom instead compounding cause terms mitigation special case insufficient capacity problem additional caveat must either fix retry behavior usually requiring code push reduce load significantly cut requests entirely latency deadlines frontend sends rpc backend server frontend consumes resources waiting reply rpc deadlines define long request wait frontend gives limiting time backend may consume frontend resources picking deadline usually wise set deadline setting either deadline extremely high deadline may cause shortterm problems long since passed continue consume server resources server restarts high deadlines result resource consumption higher levels stack lower levels stack problems short deadlines cause expensive requests fail consistently balancing constraints pick good deadline something art missing deadlines common theme many cascading outages servers spend resources handling requests exceed deadlines client result resources spent progress made get credit late assignments rpcs suppose rpc second deadline set clientthe server overloaded result takes seconds move queue thread pool point client already given request circumstances would unwise server attempt handle request would work credit grantedthe client care work server deadline passed given request already handling request performed multiple stages eg callbacks rpc calls server check deadline left stage attempting perform work request example request split parsing backend request processing stages may make sense check enough time left handle request stage deadline propagation rather inventing deadline sending rpcs backends servers employ deadline propagation deadline propagation deadline set high stack eg frontend tree rpcs emanating initial request absolute deadline example server selects second deadline processes request seconds sending rpc server b rpc b second deadline server b takes seconds handle request sends rpc server c rpc b c second deadline ideally server request tree implements deadline propagation without deadline propagation following scenario may occur server sends rpc server b second deadline server b takes seconds start processing request sends rpc server c server b uses deadline propagation set second deadline suppose instead uses hardcoded second deadline rpc server c server c pulls request queue seconds server b used deadline propagation server c could immediately give request second deadline exceeded however scenario server c processes request thinking seconds spare useful work since request server server b already exceeded deadline may want reduce outgoing deadline bit eg hundred milliseconds account network transit times postprocessing client also consider setting upper bound outgoing deadlines may want limit long server waits outgoing rpcs noncritical backends rpcs backends typically complete short duration however sure understand traffic mix might otherwise inadvertently make particular types requests fail time eg requests large payloads requests require responding lot computation exceptions servers may wish continue processing request deadline elapsed example server receives request involves performing expensive catchup operation periodically checkpoints progress catchup would good idea check deadline writing checkpoint instead expensive operation cancellation propagation propagating cancellations reduces unneeded doomed work advising servers rpc call stack efforts longer necessary reduce latency systems use hedged requests dea send rpcs primary server time later send request instances service case primary slow responding client received response server sends messages servers cancel nowsuperfluous requests requests may transitively fan many servers cancellations propagated throughout entire stack approach also used avoid potential leakage occurs initial rpc long deadline subsequent critical rpcs deeper layers stack receive errors ca nt succeed retry short deadlines time using simple deadline propagation initial call continues use server resources eventually times despite doomed failure sending fatal errors timeouts stack cancelling rpcs call tree prevents unneeded work request whole ca nt fulfilled bimodal latency suppose frontend preceding example consists servers worker threads means frontend total threads capacityduring usual operation frontends perform qps requests complete ms means frontends usually worker threads occupied configured worker threads qps seconds suppose event causes requests never completethis could result unavailability bigtable row ranges renders requests corresponding bigtable keyspace unservable result requests hit deadline remaining requests take usual ms second deadline requests would consume threads qps seconds frontend many threads available assuming secondary effects frontend able handle requests threads available threads worth work resulting error rate therefore instead requests receiving error complete due keyspace unavailability requests receive error following guidelines help address class problems detecting problem hard particular may clear bimodal latency cause outage looking mean latency see latency increase try look distribution latencies addition averages problem avoided requests complete return error early rather waiting full deadline example backend unavailable usually best immediately return error backend rather consuming resources backend available rpc layer supports failfast option use deadlines several orders magnitude longer mean request latency usually bad preceding example small number requests initially hit deadline deadline three orders magnitude larger normal mean latency leading thread exhaustion using shared resources exhausted keyspace consider either limiting inflight requests keyspace using kinds abuse tracking suppose backend processes requests different clients wildly different performance request characteristics might consider allowing threads occupied one client order provide fairness face heavy load single client misbehaving slow startup cold caching processes often slower responding requests immediately starting steady state slowness caused either following required initialization setting connections upon receiving first request needs given backend runtime performance improvements languages particularly java justintime compilation hotspot optimization deferred class loading similarly binaries less efficient caches filled example case google services requests served caches requests miss cache significantly expensive steadystate operation warm cache cache misses occur cache completely empty requests costly services might employ caches keep user state ram might accomplished hard soft stickiness reverse proxies service frontends service provisioned handle requests cold cache greater risk outages take steps avoid following scenarios lead cold cache turning new cluster recently added cluster empty cache returning cluster service maintenance cache may stale restarts task cache recently restarted filling cache take time may worthwhile move caching server separate binary like memcache also allows cache sharing many servers albeit cost introducing another rpc slight additional latency caching significant effect service may want use one following strategies overprovision service important note distinction latency cache versus capacity cache latency cache employed service sustain expected load empty cache service using capacity cache sustain expected load empty cache service owners vigilant adding caches service make sure new caches either latency caches sufficiently well engineered safely function capacity caches sometimes caches added service improve performance actually wind hard dependencies employ general cascading failure prevention techniquesin particular servers reject requests overloaded enter degraded modes testing performed see service behaves events large restart adding load cluster slowly increase load initially small request rate warms cache cache warm traffic added good idea ensure clusters carry nominal load caches kept warm always go downward stack example shakespeare service frontend talks backend turn talks storage layer problem manifests storage layer cause problems servers talk fixing storage layer usually repair backend frontend layers however suppose backends crosscommunicate amongst example backends might proxy requests one another change owns user storage layer service request intralayer communication problematic several reasons communication susceptible distributed deadlock backends may use thread pool wait rpcs sent remote backends simultaneously receiving requests remote backends suppose backend thread pool full backend b sends request backend uses thread backend b backend thread pool clears behavior cause thread pool saturation spread intralayer communication increases response kind failure heavy load condition eg load rebalancing active high load intralayer communication quickly switch low high intralayer request mode load increases enough example suppose user primary backend predetermined hot standby secondary backend different cluster take user primary backend proxies requests secondary backend result errors lower layer response heavy load master entire system overloaded primary secondary proxying likely increase add even load system due additional cost parsing waiting request secondary primary depending criticality crosslayer bootstrapping system may become complex communication usually better avoid intralayer communicationie possible cycles communication pathin user request path instead client communication example frontend talks backend guesses wrong backend backend proxy correct backend instead backend tell frontend retry request correct backend triggering conditions cascading failures service susceptible cascading failures several possible disturbances initiate domino effect section identifies factors trigger cascading failures process death server tasks may die reducing amount available capacity tasks might die query death rpc whose contents trigger failure process cluster issues assertion failures number reasons small event eg couple crashes tasks rescheduled machines may cause service brink falling break process updates pushing new version binary updating configuration may initiate cascading failure large number tasks affected simultaneously prevent scenario either account necessary capacity overhead setting service update infrastructure push offpeak dynamically adjusting number inflight task updates based volume requests available capacity may workable approach new rollouts new binary configuration changes change underlying infrastructure stack result changes request profiles resource usage limits backends number system components trigger cascading failure cascading failure usually wise check recent changes consider reverting particularly changes affected capacity altered request profile service implement type change logging help quickly identify recent changes organic growth many cases cascading failure triggered specific service change growth usage accompanied adjustment capacity planned changes drains turndowns service multihomed capacity may unavailable maintenance outages cluster similarly one service critical dependencies may drained resulting reduction capacity upstream service due drain dependencies increase latency due send requests distant cluster request profile changes backend service may receive requests different clusters frontend service shifted traffic due load balancing configuration changes changes traffic mix cluster fullness also average cost handle individual payload may changed due frontend code configuration changes similarly data handled service may changed organically due increased differing usage existing users instance number size images per user photo storage service tend increase time resource limits cluster operating systems allow resource overcommitment cpu fungible resource often machines amount slack cpu available provides bit safety net cpu spikes availability slack cpu differs cells also machines within cell depending upon slack cpu safety net dangerous availability entirely dependent behavior jobs cluster might suddenly drop time example team starts mapreduce consumes lot cpu schedules many machines aggregate amount slack cpu suddenly decrease trigger cpu starvation conditions unrelated jobs performing load tests make sure remain within committed resource limits testing cascading failures specific ways service fail hard predict first principles section discusses testing strategies detect services susceptible cascading failures test service determine behaves heavy load order gain confidence enter cascading failure various circumstances test failure beyond understanding behavior service heavy load perhaps important first step avoiding cascading failures knowing system behaves overloaded helps identify engineering tasks important longterm fixes least knowledge may help bootstrap debugging process oncall engineers emergency arises load test components break load increases component typically handles requests successfully reaches point handle requests point component ideally start serving errors degraded results response additional load significantly reduce rate successfully handles requests component highly susceptible cascading failure start crashing serving high rate errors becomes overloaded better designed component instead able reject requests survive load testing also reveals breaking point knowledge fundamental capacity planning process enables test regressions provision worstcase thresholds trade utilization versus safety margins caching effects gradually ramping load may yield different results immediately increasing expected load levels therefore consider testing gradual impulse load patterns also test understand component behaves returns nominal load pushed well beyond load testing may answer questions component enters degraded mode heavy load capable exiting degraded mode without human intervention couple servers crash heavy load much load need drop order system stabilize load testing stateful service service employs caching load test track state multiple interactions check correctness high load often subtle concurrency bugs hit keep mind individual components may different breaking points load test component separately know advance component may hit wall first want know system behaves believe system proper protections overloaded consider performing failure tests small slice production find point components system fail real traffic limits may adequately reflected synthetic load test traffic real traffic tests may provide realistic results load tests risk causing uservisible pain careful testing real traffic make sure extra capacity available case automatic protections work need manually fail might consider following production tests reducing task counts quickly slowly time beyond expected traffic patterns rapidly losing cluster worth capacity blackholing various backends test popular clients understand large clients use servicefor example want know clients queue work service use randomized exponential backoff errors vulnerable external triggers create large amounts load eg externally triggered software update might clear offline client cache depending service may may control client code talks service however still good idea understanding large clients interact service behave principles apply large internal clients stage system failures largest clients see react ask internal clients access service mechanisms use handle backend failure test noncritical backends test noncritical backends make sure unavailability interfere critical components service example suppose frontend critical noncritical backends often given request includes critical components eg query results noncritical components eg spelling suggestions requests may significantly slow consume resources waiting noncritical backends finish addition testing behavior noncritical backend unavailable test frontend behaves noncritical backend never responds example blackholing requests backends advertised noncritical still cause problems frontends requests long deadlines frontend start rejecting lots requests running resources serving high latency noncritical backend blackholes immediate steps address cascading failures identified service experiencing cascading failure use different strategies remedy situationand course cascading failure good opportunity use incident management protocol managing incidents increase resources system running degraded capacity idle resources adding tasks expedient way recover outage however service entered death spiral sort adding resources may sufficient recover stop health check failuresdeaths cluster scheduling systems borg check health tasks job restart tasks unhealthy practice may create failure mode healthchecking makes service unhealthy example half tasks able accomplish work starting half soon killed overloaded failing health checks temporarily disabling health checks may permit system stabilize tasks running process health checking binary responding service health checking binary able respond class requests right two conceptually distinct operations process health checking relevant cluster scheduler whereas service health checking relevant load balancer clearly distinguishing two types health checks help avoid scenario restart servers restart servers servers somehow wedged making progress restarting may helptry restarting servers java servers gc death spiral inflight requests deadlines consuming resources leading block threads example servers deadlocked make sure identify source cascading failure restart servers make sure taking action simply shift around load canary change make slowly actions may amplify existing cascading failure outage actually due issue like cold cache drop traffic dropping load big hammer usually reserved situations true cascading failure hands fix means example heavy load causes servers crash soon become healthy get service running addressing initial triggering condition adding capacity example reducing load enough crashing stops consider aggressive hereif entire service crashlooping allow say traffic allowing majority servers become healthy gradually ramping load strategy allows caches warm connections established etc load returns normal levels obviously tactic cause lot uservisible harm whether able even drop traffic indiscriminately depends service configured mechanism drop less important traffic eg prefetching use mechanism first important keep mind strategy enables recover cascading outage underlying problem fixed issue started cascading failure fixed eg insufficient global capacity cascading failure may trigger shortly traffic returns therefore using strategy consider fixing least papering root cause triggering condition example service ran memory death spiral adding memory tasks first step enter degraded modes serve degraded results less work dropping unimportant traffic strategy must engineered service implemented know traffic degraded ability differentiate various payloads eliminate batch load services load important critical consider turning sources load example index updates data copies statistics gathering consume resources serving path consider turning sources load outage eliminate bad traffic queries creating heavy load crashes eg queries death consider blocking eliminating via means cascading failure shakespeare documentary shakespeare works airs japan explicitly points shakespeare service excellent place conduct research following broadcast traffic asian datacenter surges beyond service capacity capacity problem compounded major update shakespeare service simultaneously occurs datacenter fortunately number safeguards place help mitigate potential failure production readiness review process identified issues team already addressed example developers built graceful degradation service capacity becomes scarce service longer returns pictures alongside text small maps illustrating story takes place depending purpose rpc times either retried example case aforementioned pictures retried randomized exponential backoff despite safeguards tasks fail one one restarted borg drives number working tasks even result graphs service dashboard turn alarming shade red sre paged response sres temporarily add capacity asian datacenter increasing number tasks available shakespeare job able restore shakespeare service asian cluster afterward sre team writes postmortem detailing chain events went well could gone better number action items prevent scenario occurring example case service overload gslb load balancer redirect traffic neighboring datacenters also sre team turns autoscaling number tasks automatically increases traffic worry type issue closing remarks systems overloaded something needs give order remedy situation service passes breaking point better allow uservisible errors lowerquality results slip try fully serve every request understanding breaking points system behaves beyond critical service owners want avoid cascading failures without proper care system changes meant reduce background errors otherwise improve steady state expose service greater risk full outage retrying failures shifting load around unhealthy servers killing unhealthy servers adding caches improve performance reduce latency might implemented improve normal case improve chance causing largescale failure careful evaluating changes ensure one outage traded another see wikipedia positive https enwikipediaorgwikipositivefeedback feedback a watchdog often implemented thread wakes periodically see whether work done since last time checked assumes server stuck kills instance requests known type sent server regular intervals one received processed expected may indicate failureof server system sending requests intermediate network this often good assumption due geography see also job data organization an instructive exercise left reader write simple simulator see amount useful work backend varies much overloaded many retries permitted sometimes find meaningful proportion actual serving capacity function serving cache lost access cache actually able serve many queries similar observation holds latency cache help achieve latency goals lowering average response time query servable cache possibly meet without cache chapter managing critical state distributed consensus reliability written laura nolan edited tim harvey processes crash may need restarted hard drives fail natural disasters take several datacenters region site reliability engineers need anticipate sorts failures develop strategies keep systems running spite strategies usually entail running systems across multiple sites geographically distributing system relatively straightforward also introduces need maintain consistent view system state nuanced difficult undertaking groups processes may want reliably agree questions process leader group processes set processes group message successfully committed distributed queue process hold lease value datastore given key found distributed consensus effective building reliable highly available systems require consistent view system state distributed consensus problem deals reaching agreement among group processes connected unreliable communications network instance several processes distributed system may need able form consistent view critical piece configuration whether distributed lock held message queue processed one fundamental concepts distributed computing one rely virtually every service offer figure illustrates simple model group processes achieve consistent view system state distributed consensus figure distributed consensus agreement among group processes whenever see leader election critical shared state distributed locking recommend using distributed consensus systems formally proven tested thoroughly informal approaches solving problem lead outages insidiously subtle hardtofix data consistency problems may prolong outages system unnecessarily cap theorem cap theorem fox bre holds distributed system simultaneously three following properties consistent views data node availability data node tolerance network partitions gil logic intuitive two nodes communicate network partitioned system whole either stop serving requests nodes thus reducing availability serve requests usual results inconsistent views data node network partitions inevitable cables get cut packets get lost delayed due congestion hardware breaks networking components become misconfigured etc understanding distributed consensus really amounts understanding consistency availability work particular application commercial pressures often demand high levels availability many applications require consistent views data systems software engineers usually familiar traditional acid datastore semantics atomicity consistency isolation durability growing number distributed datastore technologies provide different set semantics known base basically available soft state eventual consistency datastores support base semantics useful applications certain kinds data handle large volumes data transactions would much costly perhaps altogether infeasible datastores support acid semantics systems support base semantics rely multimaster replication writes committed different processes concurrently mechanism resolve conflicts often simple latest timestamp wins approach usually known eventual consistency however eventual consistency lead surprising results lu particularly event clock drift inevitable distributed systems network partitioning kin also difficult developers design systems work well datastores support base semantics jeff shute shu example stated find developers spend significant fraction time building extremely complex errorprone mechanisms cope eventual consistency handle data may date think unacceptable burden place developers consistency problems solved database level system designers sacrifice correctness order achieve reliability performance particularly around critical state example consider system handles financial transactions reliability performance requirements provide much value financial data correct systems need able reliably synchronize critical state across multiple processes distributed consensus algorithms provide functionality motivating use consensus distributed systems coordination failure distributed systems complex subtle understand monitor troubleshoot engineers running systems often surprised behavior presence failures failures relatively rare events usual practice test systems conditions difficult reason system behavior failures network partitions particularly challenging a problem appears caused full partition may instead result slow network messages dropped throttle occurring one direction direction following sections provide examples problems occurred realworld distributed systems discuss leader election distributed consensus algorithms could used prevent issues case study splitbrain problem service content repository allows collaboration multiple users uses sets two replicated file servers different racks reliability service needs avoid writing data simultaneously file servers set could result data corruption possibly unrecoverable data pair file servers one leader one follower servers monitor via heartbeats one file server contact partner issues stonith shoot node head command partner node shut node takes mastership files practice industry standard method reducing splitbrain instances although shall see conceptually unsound happens network becomes slow starts dropping packets scenario file servers exceed heartbeat timeouts designed send stonith commands partner nodes take mastership however commands may delivered due compromised network file server pairs may state nodes expected active resource issued received stonith commands results either corruption unavailability data problem system trying solve leader election problem using simple timeouts leader election reformulation distributed asynchronous consensus problem solved correctly using heartbeats case study failover requires human intervention highly sharded database system primary shard replicates synchronously secondary another datacenter external system checks health primaries longer healthy promotes secondary primary primary determine health secondary makes unavailable escalates human order avoid splitbrain scenario seen case study solution risk data loss negatively impact availability data also unnecessarily increases operational load engineers run system human intervention scales poorly sort event primary secondary problems communicating highly likely occur case larger infrastructure problem responding engineers may already overloaded tasks network badly affected distributed consensus system elect master human likely better positioned case study faulty groupmembership algorithms system component performs indexing searching services starting nodes use gossip protocol discover join cluster cluster elects leader performs coordination case network partition splits cluster side incorrectly elects master accepts writes deletions leading splitbrain scenario data corruption problem determining consistent view group membership across group processes another instance distributed consensus problem fact many distributed systems problems turn different versions distributed consensus including master election group membership kinds distributed locking leasing reliable distributed queuing messaging maintenance kind critical shared state must viewed consistently across group processes problems solved using distributed consensus algorithms proven formally correct whose implementations tested extensively ad hoc means solving sorts problems heartbeats gossip protocols always reliability problems practice distributed consensus works consensus problem multiple variants dealing distributed software systems interested asynchronous distributed consensus applies environments potentially unbounded delays message passing synchronous consensus applies realtime systems dedicated hardware means messages always passed specific timing guarantees distributed consensus algorithms may crashfail assumes crashed nodes never return system crashrecover crashrecover algorithms much useful problems real systems transient nature due slow network restarts algorithms may deal byzantine nonbyzantine failures byzantine failure occurs process passes incorrect messages due bug malicious activity comparatively costly handle less often encountered technically solving asynchronous distributed consensus problem bounded time impossible proven dijkstra prizewinning flp impossibility result fis asynchronous distributed consensus algorithm guarantee progress presence unreliable network practice approach distributed consensus problem bounded time ensuring system sufficient healthy replicas network connectivity make progress reliably time addition system backoffs randomized delays setup prevents retries causing cascade effects avoids dueling proposers problem described later chapter protocols guarantee safety adequate redundancy system encourages liveness original solution distributed consensus problem lamport paxos protocol lam protocols exist solve problem including raft ong zab jun mencius mao paxos many variations intended increase performance zoo usually vary single detail giving special leader role one process streamline protocol paxos overview example protocol paxos operates sequence proposals may may accepted majority processes system proposal accepted fails proposal sequence number imposes strict ordering operations system first phase protocol proposer sends sequence number acceptors acceptor agree accept proposal yet seen proposal higher sequence number proposers try higher sequence number necessary proposers must use unique sequence numbers drawing disjoint sets incorporating hostname sequence number instance proposer receives agreement majority acceptors commit proposal sending commit message value strict sequencing proposals solves problems relating ordering messages system requirement majority commit means two different values committed proposal two majorities overlap least one node acceptors must write journal persistent storage whenever agree accept proposal acceptors need honor guarantees restarting paxos useful lets agree value proposal number quorum nodes need agree value given node may complete view set values agreed limitation true distributed consensus algorithms system architecture patterns distributed consensus distributed consensus algorithms lowlevel primitive simply allow set nodes agree value map well real design tasks makes distributed consensus useful addition higherlevel system components datastores configuration stores queues locking leader election services provide practical system functionality distributed consensus algorithms address using higherlevel components reduces complexity system designers also allows underlying distributed consensus algorithms changed necessary response changes environment system runs changes nonfunctional requirements many systems successfully use consensus algorithms actually clients service implements algorithms zookeeper consul etcd zookeeper hun first open source consensus system gain traction industry easy use even applications designed use distributed consensus chubby service fills similar niche google authors point bur providing consensus primitives service rather libraries engineers build applications frees application maintainers deploy systems way compatible highly available consensus service running right number replicas dealing group membership dealing performance etc reliable replicated state machines replicated state machine rsm system executes set operations order several processes rsms fundamental building block useful distributed systems components services data configuration storage locking leader election described detail later operations rsm ordered globally consensus algorithm powerful concept several papers agu kir sch show deterministic program implemented highly available replicated service implemented rsm shown figure replicated state machines system implemented logical layer consensus algorithm consensus algorithm deals agreement sequence operations rsm executes operations order every member consensus group necessarily member consensus quorum rsms may need synchronize state peers described kirsch amir kir use slidingwindow protocol reconcile state peer processes rsm figure relationship consensus algorithms replicated state machines reliable replicated datastores configuration stores reliable replicated datastores application replicated state machines replicated datastores use consensus algorithms critical path work thus performance throughput ability scale important type design datastores built underlying technologies consensusbased datastores provide variety consistency semantics read operations make huge difference datastore scales tradeoffs discussed distributed consensus performance nondistributedconsensusbased systems often simply rely timestamps provide bounds age data returned timestamps highly problematic distributed systems impossible guarantee clocks synchronized across multiple machines spanner cor addresses problem modeling worstcase uncertainty involved slowing processing necessary resolve uncertainty highly available processing using leader election leader election distributed systems equivalent problem distributed consensus replicated services use single leader perform specific type work system common single leader mechanism way ensuring mutual exclusion coarse level type design appropriate work service leader performed one process sharded system designers construct highly available service writing though simple program replicating process using leader election ensure one leader working point time shown figure often work leader coordinating pool workers system pattern used gfs ghe replaced colossus bigtable keyvalue store cha figure highly available system using replicated service master election type component unlike replicated datastore consensus algorithm critical path main work system throughput usually major concern distributed coordination locking services barrier distributed computation primitive blocks group processes proceeding condition met example parts one phase computation completed use barrier effectively splits distributed computation logical phases instance shown figure barrier could used implementing mapreduce dea model ensure entire map phase completed reduce part computation proceeds figure barriers process coordination mapreduce computation barrier could implemented single coordinator process implementation adds single point failure usually unacceptable barrier also implemented rsm zookeeper consensus service implement barrier pattern see hun zoo locks another useful coordination primitive implemented rsm consider distributed system worker processes atomically consume input files write results distributed locks used prevent multiple workers processing input file practice essential use renewable leases timeouts instead indefinite locks prevents locks held indefinitely processes crash distributed locking beyond scope chapter bear mind distributed locks lowlevel systems primitive used care applications use higherlevel system provides distributed transactions reliable distributed queuing messaging queues common data structure often used way distribute tasks number worker processes queuingbased systems tolerate failure loss worker nodes relatively easily however system must ensure claimed tasks successfully processed purpose lease system discussed earlier regard locks recommended instead outright removal queue downside queuingbased systems loss queue prevents entire system operating implementing queue rsm minimize risk make entire system far robust atomic broadcast distributed systems primitive messages received reliably order participants incredibly powerful distributed systems concept useful designing practical systems huge number publishsubscribe messaging infrastructures exist use system designers although provide atomic guarantees chandra toueg cha demonstrate equivalence atomic broadcast consensus queuingasworkdistribution pattern uses queue load balancing device shown figure considered pointtopoint messaging messaging systems usually also implement publishsubscribe queue messages may consumed many clients subscribe channel topic onetomany case messages queue stored persistent ordered list publishsubscribe systems used many types applications require clients subscribe receive notifications type event publishsubscribe systems also used implement coherent distributed caches figure queueoriented work distribution system using reliable consensusbased queuing component queuing messaging systems often need excellent throughput need extremely low latency due seldom directly userfacing however high latencies system like one described multiple workers claiming tasks queue could become problem percentage processing time task grew significantly distributed consensus performance conventional wisdom generally held consensus algorithms slow costly use many systems require high throughput low latency bol conception simply truewhile implementations slow number tricks improve performance distributed consensus algorithms core many google critical systems described ana bur cor shu proven extremely effective practice google scale advantage fact scale disadvantage introduces two main challenges datasets tend large systems run wide geographical distance larger datasets multiplied several replicas represent significant computing costs larger geographical distances increase latency replicas turn reduces performance one best distributed consensus state machine replication algorithm performance performance dependent number factors relating workload system performance objectives system deployed following sections present research aim increasing understanding possible achieve distributed consensus many systems described available use workloads vary many ways understanding vary critical discussing performance case consensus system workload may vary terms throughput number proposals made per unit time peak load type requests proportion operations change state consistency semantics required read operations request sizes size data payload vary deployment strategies vary example deployment local area wide area kinds quorum used majority processes system use sharding pipelining batching many consensus systems use distinguished leader process require requests go special node shown figure result performance system perceived clients different geographic locations may vary considerably simply distant nodes longer roundtrip times leader process figure effect distance server process perceived latency client multipaxos detailed message flow multipaxos protocol uses strong leader process unless leader yet elected failure occurs requires one round trip proposer quorum acceptors reach consensus using strong leader process optimal terms number messages passed typical many consensus protocols figure shows initial state new proposer executing first preparepromise phase protocol executing phase establishes new numbered view leader term subsequent executions protocol view remains first phase unnecessary proposer established view simply send accept messages consensus reached quorum responses received including proposer figure basic multipaxos message flow another process group assume proposer role propose messages time changing proposer performance cost necessitates extra round trip execute phase protocol importantly may cause dueling proposers situation proposals repeatedly interrupt proposals accepted shown figure scenario form livelock continue indefinitely figure dueling proposers multipaxos practical consensus systems address issue collisions usually either electing proposer process makes proposals system using rotating proposer allocates process particular slots proposals systems use leader process leader election process must tuned carefully balance system unavailability occurs leader present risk dueling proposers important implement right timeouts backoff strategies multiple processes detect leader attempt become leader time none processes likely succeed dueling proposers introducing randomness best approach raft ong example wellthoughtout method approaching leader election process scaling readheavy workloads scaling read workload often critical many workloads readheavy replicated datastores advantage data available multiple places meaning strong consistency required reads data could read replica technique reading replicas works well certain applications google photon system ana uses distributed consensus coordinate work multiple pipelines photon uses atomic compareandset operation state modification inspired atomic registers must absolutely consistent read operations may served replica stale data results extra work performed incorrect results gup tradeoff worthwhile order guarantee data read uptodate consistent changes made read performed necessary one following perform readonly consensus operation read data replica guaranteed uptodate system uses stable leader process many distributed consensus implementations leader provide guarantee use quorum leases replicas granted lease part data system allowing strongly consistent local reads cost write performance technique discussed detail following section quorum leases quorum leases mor recently developed distributed consensus performance optimization aimed reducing latency increasing throughput read operations previously mentioned case classic paxos distributed consensus protocols performing strongly consistent read ie one guaranteed uptodate view state requires either distributed consensus operation reads quorum replicas stable leader replica guaranteed seen recent state changing operations many systems read operations vastly outnumber writes reliance either distributed operation single replica harms latency system throughput quorum leasing technique simply grants read lease subset replicated datastore state quorum replicas lease specific usually brief period time operation changes state data must acknowledged replicas read quorum replicas becomes unavailable data modified lease expires quorum leases particularly useful readheavy workloads reads particular subsets data concentrated single geographic region distributed consensus performance network latency consensus systems face two major physical constraints performance committing state changes one network roundtrip time time takes write data persistent storage examined later network roundtrip times vary enormously depending source destination location impacted physical distance source destination amount congestion network within single datacenter roundtrip times machines order millisecond typical roundtriptime rtt within united states milliseconds new york london milliseconds consensus system performance local area network comparable asynchronous leaderfollower replication system bol many traditional databases use replication however much availability benefits distributed consensus systems require replicas distant order different failure domains many consensus systems use tcpip communication protocol tcpip connectionoriented provides strong reliability guarantees regarding fifo sequencing messages however setting new tcpip connection requires network round trip perform threeway handshake sets connection data sent received tcpip slow start initially limits bandwidth connection limits established initial tcpip window sizes range kb tcpip slow start probably issue processes form consensus group establish connections keep connections open reuse frequent communication however systems high number clients may practical clients keep persistent connection consensus clusters open open tcpip connections consume resources eg file descriptors addition generating keepalive traffic overhead may important issue applications use highly sharded consensusbased datastores containing thousands replicas even larger numbers clients solution use pool regional proxies shown figure hold persistent tcpip connections consensus group order avoid setup overhead long distances proxies may also good way encapsulate sharding load balancing strategies well discovery cluster members leaders figure using proxies reduce need clients open tcpip connections across regions reasoning performance fast paxos fast paxos lam version paxos algorithm designed improve performance wide area networks using fast paxos client send propose messages directly member group acceptors instead leader classic paxos multipaxos idea substitute one parallel message send client acceptors fast paxos two message send operations classic paxos one message client single proposer parallel message send operation proposer replicas intuitively seems though fast paxos always faster classic paxos however true client fast paxos system high rtt roundtrip time acceptors acceptors fast connections substituted n parallel messages across slower network links fast paxos one message across slower link plus n parallel messages across faster links classic paxos due latency tail effect majority time single round trip across slow link distribution latencies faster quorum shown jun therefore fast paxos slower classic paxos case many systems batch multiple operations single transaction acceptor increase throughput clients act proposers also makes much difficult batch proposals reason proposals arrive independently acceptors batch consistent way stable leaders seen multipaxos elects stable leader improve performance zab jun raft ong also examples protocols elect stable leader performance reasons approach allow read optimizations leader uptodate state also several problems operations change state must sent via leader requirement adds network latency clients located near leader leader process outgoing network bandwidth system bottleneck mao leader accept message contains data related proposal whereas messages contain acknowledgments numbered transaction data payload leader happens machine performance problems throughput entire system reduced almost distributed consensus systems designed performance mind use either single stable leader pattern system rotating leadership numbered distributed consensus algorithm preassigned replica usually simple modulus transaction id algorithms use approach include mencius mao egalitarian paxos mora wide area network clients spread geographically replicas consensus group located reasonably near clients leader election leads lower perceived latency clients network rtt nearest replica average smaller arbitrary leader batching batching described reasoning performance fast paxos increases system throughput still leaves replicas idle await replies messages sent inefficiencies presented idle replicas solved pipelining allows multiple proposals inflight optimization similar tcpip case protocol attempts keep pipe full using slidingwindow approach pipelining normally used combination batching batches requests pipeline still globally ordered view number transaction number method violate global ordering properties required run replicated state machine optimization method discussed bol san disk access logging persistent storage required node crashed returned cluster honors whatever previous commitments made regarding ongoing consensus transactions paxos protocol instance acceptors agree proposal already agreed proposal higher sequence number details agreed committed proposals logged persistent storage acceptor might violate protocol crashes restarted leading inconsistent state time required write entry log disk varies greatly depending hardware virtualized environment used likely take one several milliseconds message flow multipaxos discussed multipaxos detailed message flow section show protocol must log state changes disk disk write must happen whenever process makes commitment must honor performancecritical second phase multipaxos points occur acceptor sends accepted message response proposal proposer sends accept message accept message also implicit accepted message lam means latency single consensus operation involves following one disk write proposer parallel messages acceptors parallel disk writes acceptors return messages version multipaxos protocol useful cases disk write time dominates variant consider proposer accept message implicit accepted message instead proposer writes disk parallel processes sends explicit accept message latency becomes proportional time taken send two messages quorum processes execute synchronous write disk parallel latency performing small random write disk order milliseconds rate consensus operations limited approximately per second times assume network roundtrip times negligible proposer performs logging parallel acceptors seen already distributed consensus algorithms often used basis building replicated state machine rsms also need keep transaction logs recovery purposes reasons datastore consensus algorithm log rsm transaction log combined single log combining logs avoids need constantly alternate writing two different physical locations disk bol reducing time spent seek operations disks sustain operations per second therefore system whole perform transactions datastore disks purposes maintaining logs system state generally maintained disk log writes must flushed directly disk writes state changes written memory cache flushed disk later reordered use efficient schedule bol another possible optimization batching multiple client operations together one operation proposer ana bol cha jun mao mora amortizes fixed costs disk logging network latency larger number operations increasing throughput deploying distributed consensusbased systems critical decisions system designers must make deploying consensusbased system concern number replicas deployed location replicas number replicas general consensusbased systems operate using majority quorums ie group f replicas may tolerate f failures byzantine fault tolerance system resistant replicas returning incorrect results required f replicas may tolerate f failures cas nonbyzantine failures minimum number replicas deployed threeif two deployed tolerance failure process three replicas may tolerate one failure system downtime result planned maintenance ken three replicas allow system operate normally one replica maintenance assuming remaining two replicas handle system load acceptable performance unplanned failure occurs maintenance window consensus system becomes unavailable unavailability consensus system usually unacceptable five replicas run allowing system operate two failures intervention necessarily required four five replicas consensus system remain three left additional replica two added consensus system loses many replicas form quorum system theory unrecoverable state durable logs least one missing replicas accessed quorum remains possible decision seen missing replicas made administrators may able force change group membership add new replicas catch existing one order proceed possibility data loss always remainsa situation avoided possible disaster administrators decide whether perform forceful reconfiguration wait period time machines system state become available decisions made treatment system log addition monitoring becomes critical theoretical papers often point consensus used construct replicated log fail discuss deal replicas may fail recover thus miss sequence consensus decisions lag due slowness order maintain robustness system important replicas catch replicated log always firstclass citizen distributed consensus theory important aspect production systems raft describes method managing consistency replicated logs ong explicitly defining gaps replica log filled fiveinstance raft system loses members except leader leader still guaranteed full knowledge committed decisions hand missing majority members included leader strong guarantees made regarding uptodate remaining replicas relationship performance number replicas system need form part quorum minority slower replicas may lag behind allowing quorum betterperforming replicas run faster long leader performs well replica performance varies significantly every failure may reduce performance system overall slow outliers required form quorum failures lagging replicas system tolerate better system performance overall likely issue cost also considered managing replicas replica uses costly computing resources system question single cluster processes cost running replicas probably large consideration however cost replicas serious consideration systems photon ana uses sharded configuration shard full group processes running consensus algorithm number shards grows cost additional replica number processes equal number shards must added system decision number replicas system thus tradeoff following factors need reliability frequency planned maintenance affecting system risk performance cost calculation different system systems different service level objectives availability organizations perform maintenance regularly others organizations use hardware varying cost quality reliability location replicas decisions deploy processes comprise consensus cluster made based upon two factors tradeoff failure domains system handle latency requirements system multiple complex issues play deciding locate replicas failure domain set components system become unavailable result single failure example failure domains include following physical machine rack datacenter served single power supply several racks datacenter served one piece networking equipment datacenter could rendered unavailable fiber optic cable cut set datacenters single geographic area could affected single natural disaster hurricane general distance replicas increases roundtrip time replicas well size failure system able tolerate consensus systems increasing roundtrip time replicas also increase latency operations extent latency matters well ability survive failure particular domain systemdependent consensus system architectures require particularly high throughput low latency example consensus system exists order provide group membership leader election services highly available service probably heavily loaded consensus transaction time fraction leader lease time performance critical batchoriented systems also less affected latency operation batch sizes increased increase throughput always make sense continually increase size failure domain whose loss system withstand instance clients using consensus system running within particular failure domain say new york area deploying distributed consensusbased system across wider geographical area would allow remain serving outages failure domain say hurricane sandy worth probably system clients well system see traffic extra cost terms latency throughput computing resources would give benefit take disaster recovery account deciding locate replicas system stores critical data consensus replicas also essentially online copies system data however critical data stake important back regular snapshots elsewhere even case solid consensusbased systems deployed several diverse failure domains two failure domains never escape software human error part system administrators bugs software emerge unusual circumstances cause data loss system misconfiguration similar effects human operators also err perform sabotage causing data loss making decisions location replicas remember important measure performance client perception ideally network roundtrip time clients consensus system replicas minimized wide area network leaderless protocols like mencius egalitarian paxos may performance edge particularly consistency constraints application mean possible execute readonly operations system replica without performing consensus operation capacity load balancing capacity load balancing designing deployment must make sure sufficient capacity deal load case sharded deployments adjust capacity adjusting number shards however systems read consensus group members leader increase read capacity adding replicas adding replicas cost algorithm uses strong leader adding replicas imposes load leader process peertopeer protocol adding replicas imposes load processes however ample capacity write operations readheavy workload stressing system adding replicas may best approach noted adding replica majority quorum system potentially decrease system availability somewhat shown figure typical deployment zookeeper chubby uses five replicas majority quorum requires three replicas system still make progress two replicas unavailable six replicas quorum requires four replicas replicas unavailable system remain live considerations regarding failure domains therefore apply even strongly sixth replica added organization five datacenters generally runs consensus groups five processes one datacenter loss one datacenter still leaves one spare replica group sixth replica deployed one five datacenters outage datacenter removes spare replicas group thereby reducing capacity figure adding extra replica one region may reduce system availability colocating multiple replicas single datacenter may reduce system availability quorum without redundancy remaining clients dense particular geographic region best locate replicas close clients however deciding exactly locate replicas may require careful thought around load balancing system deals overload shown figure system simply routes client read requests nearest replica large spike load concentrated one region may overwhelm nearest replica nextclosest replica onthis cascading failure see addressing cascading failures type overload often happen result batch jobs beginning especially several begin time already seen reason many distributed consensus systems use leader process improve performance however important understand leader replicas use computational resources particularly outgoing network capacity leader sends proposal messages include proposed data replicas send smaller messages usually containing agreement particular consensus transaction id organizations run highly sharded consensus systems large number processes may find necessary ensure leader processes different shards balanced relatively evenly across different datacenters prevents system whole bottlenecked outgoing network capacity one datacenter makes much greater overall system capacity figure colocating leader processes leads uneven bandwidth utilization another downside deploying consensus groups multiple datacenters shown figure extreme change system occur datacenter hosting leaders suffers widespread failure power networking equipment failure fiber cut instance shown figure failure scenario leaders fail another datacenter either split evenly en masse one datacenter either case link two datacenters suddenly receive lot network traffic system would inopportune moment discover capacity link insufficient figure colocated leaders fail en masse patterns network utilization change dramatically however type deployment could easily unintended result automatic processes system bearing leaders chosen instance clients experience better latency operations handled via leader leader located closest algorithm attempts site leaders near bulk clients could take advantage insight algorithm might try locate leaders machines best performance pitfall approach one three datacenters houses faster machines disproportionate amount traffic sent datacenter resulting extreme traffic changes datacenter go offline avoid problem algorithm must also take account distribution balance machine capabilities selecting machines leader election algorithm might favor processes running longer longerrunning processes quite likely correlated location software releases performed perdatacenter basis quorum composition determining locate replicas consensus group important consider effect geographical distribution precisely network latencies replicas performance group one approach spread replicas evenly possible similar rtts replicas factors equal workload hardware network performance arrangement lead fairly consistent performance across regions regardless group leader located member consensus group leaderless protocol use geography greatly complicate approach particularly true intracontinental versus transpacific transatlantic traffic consider system spans north america europe impossible locate replicas equidistant always longer lag transatlantic traffic intracontinental traffic matter transactions one region need make transatlantic round trip order reach consensus however shown figure order try distribute traffic evenly possible systems designers might choose site five replicas two replicas roughly centrally us one east coast two europe distribution would mean average case consensus could achieved north america without waiting replies europe europe consensus achieved exchanging messages east coast replica east coast replica acts linchpin sorts two possible quorums overlap figure overlapping quorums one replica acting link shown figure loss replica means system latency likely change drastically instead largely influenced either central us east coast rtt eu east coast rtt latency based eu central rtt around higher eu east coast rtt geographic distance network rtt nearest possible quorum increases enormously figure loss link replica immediately leads longer rtt quorum scenario key weakness simple majority quorum applied groups composed replicas different rtts members cases hierarchical quorum approach may useful diagrammed figure nine replicas may deployed three groups three quorum may formed majority groups group may included quorum majority group members available means replica may lost central group without incurring large impact overall system performance central group may still vote transactions two three replicas however resource cost associated running higher number replicas highly sharded system readheavy workload largely fulfillable replicas might mitigate cost using fewer consensus groups strategy means overall number processes system may change figure hierarchical quorums used reduce reliance central replica monitoring distributed consensus systems already seen distributed consensus algorithms core many google critical systems ana bur cor shu important production systems need monitoring order detect outages problems troubleshooting experience shown us certain specific aspects distributed consensus systems warrant special attention number members running consensus group status process healthy healthy process may running unable make progress eg hardwarerelated reason persistently lagging replicas healthy members consensus group still potentially multiple different states group member may recovering state peers startup lagging behind quorum group may uptodate participating fully may leader whether leader exists system based algorithm multipaxos uses leader role must monitored ensure leader exists system leader totally unavailable number leader changes rapid changes leadership impair performance consensus systems use stable leader number leader changes monitored consensus algorithms usually mark leadership change new term view number number provides useful metric monitor rapid increase leader changes signals leader flapping perhaps due network connectivity issues decrease view number could signal serious bug consensus transaction number operators need know whether consensus system making progress consensus algorithms use increasing consensus transaction number indicate progress number seen increasing time system healthy number proposals seen number proposals agreed upon numbers indicate whether system operating correctly throughput latency although specific distributed consensus systems characteristics consensus system monitored understood administrators order understand system performance help troubleshoot performance issues might also monitor following latency distributions proposal acceptance distributions network latencies observed parts system different locations amount time acceptors spend durable logging overall bytes accepted per second system conclusion explored definition distributed consensus problem presented system architecture patterns distributedconsensus based systems well examining performance characteristics operational concerns around distributed consensusbased systems deliberately avoided indepth discussion specific algorithms protocols implementations chapter distributed coordination systems technologies underlying evolving quickly information would rapidly become date unlike fundamentals discussed however fundamentals along articles referenced throughout chapter enable use distributed coordination tools available today well future software remember nothing else chapter keep mind sorts problems distributed consensus used solve types problems arise ad hoc methods heartbeats used instead distributed consensus whenever see leader election critical shared state distributed locking think distributed consensus lesser approach ticking bomb waiting explode systems kyle kingsbury written extensive series articles distributed systems correctness contain many examples unexpected incorrect behavior kinds datastores see https aphyrcomtagsjepsen in particular performance original paxos algorithm ideal greatly improved years chapter distributed periodic scheduling cron written štěpán davidovič edited kavita guliani chapter describes google s implementation distributed cron service serves vast majority internal teams need periodic scheduling compute jobs throughout cron s existence learned many lessons design implement might seem like basic service discuss problems distributed crons face outline potential solutions cron common unix utility designed periodically launch arbitrary jobs userdefined times intervals first analyze base principles cron common implementations review application cron work large distributed environment order increase reliability system singlemachine failures describe distributed cron system deployed small number machines launch cron jobs across entire datacenter conjunction datacenter scheduling system like borg ver cron let s discuss cron typically used single machine case diving running crossdatacenter service introduction cron designed system administrators common users system specify commands run commands run cron executes various types jobs including garbage collection periodic data analysis common time specification format called crontab format supports simple intervals eg day noon every hour hour complex intervals every saturday also th day month also configured cron usually implemented using single component commonly referred crond crond daemon loads list scheduled cron jobs jobs launched according specified execution times reliability perspective several aspects cron service notable reliability perspective cron s failure domain essentially one machine machine running neither cron scheduler jobs launches run consider simple distributed case two machines cron scheduler launches jobs different worker machine example using ssh scenario presents two distinct failure domains could impact ability launch jobs either scheduler machine destination machine could fail state needs persist across crond restarts including machine reboots crontab configuration cron launches fireandforget crond makes attempt track launches anacron notable exception anacron attempts launch jobs would launched system relaunch attempts limited jobs run daily less frequently functionality useful running maintenance jobs workstations notebooks facilitated file retains timestamp last launch registered cron jobs cron jobs idempotency cron jobs designed perform periodic work beyond hard know advance function variety requirements diverse set cron jobs entails obviously impacts reliability requirements cron jobs garbage collection processes idempotent case system malfunction safe launch jobs multiple times cron jobs process sends email newsletter wide distribution launched make matters complicated failure launch acceptable cron jobs others example garbage collection cron job scheduled run every five minutes may able skip one launch payroll cron job scheduled run month skipped large variety cron jobs makes reasoning failure modes difficult system like cron service single answer fits every situation general favor skipping launches rather risking double launches much infrastructure allows recovering skipped launch tenable recovering double launch cron job owners monitor cron jobs example owner might cron service expose state managed cron jobs set independent monitoring effect cron jobs case skipped launch cron job owners take action appropriately matches nature cron job however undoing double launch previously mentioned newsletter example may difficult even entirely impossible therefore prefer fail closed avoid systemically creating bad state cron large scale moving away single machines toward largescale deployments requires fundamental rethinking make cron work well environment presenting details google cron solution ll discuss differences smallscale largescale deployment describe design changes largescale deployments necessitated extended infrastructure regular implementations cron limited single machine largescale system deployments extend cron solution multiple machines hosting cron service single machine could catastrophic terms reliability say machine located datacenter exactly machines failure th available machines could knock entire cron service obvious reasons implementation acceptable increase cron s reliability decouple processes machines want run service simply specify service requirements datacenter run datacenter scheduling system reliable determines machine machines deploy service addition handling machine deaths launching job datacenter effectively turns sending one rpcs datacenter scheduler process however instantaneous discovering dead machine entails health check timeouts rescheduling service onto different machine requires time install software start new process moving process different machine mean loss local state stored old machine unless live migration employed rescheduling time may exceed smallest scheduling interval one minute need procedures place mitigate data loss excessive time requirements retain local state old machine might simply persist state distributed filesystem gfs use filesystem startup identify jobs failed launch due rescheduling however solution falls short terms timeliness expectations run cron job every five minutes one twominute delay caused total overhead cron system rescheduling potentially unacceptably substantial case hot spares would able quickly jump resume operation significantly shorten time window extended requirements singlemachine systems typically colocate running processes limited isolation containers commonplace s necessary common use containers isolate every single component service s deployed single machine therefore cron deployed single machine crond cron jobs runs would likely isolated deployment datacenter scale commonly means deployment containers enforce isolation isolation necessary base expectation independent processes running datacenter negatively impact order enforce expectation know quantity resources need acquire front given process want runboth cron system jobs launches cron job may delayed datacenter resources available match demands cron job resource requirements addition user demand monitoring cron job launches means need track full state cron job launches scheduled launch termination decoupling process launches specific machines exposes cron system partial launch failure versatility cron job configurations also means launching new cron job datacenter may need multiple rpcs sometimes encounter scenario rpcs succeeded others example process sending rpcs died middle executing tasks cron recovery procedure must also account scenario terms failure mode datacenter substantially complex ecosystem single machine cron service began relatively simple binary single machine many obvious nonobvious dependencies deployed larger scale service basic cron want ensure even datacenter suffers partial failure example partial power outage problems storage services service still able function requiring datacenter scheduler locates replicas cron diverse locations within datacenter avoid scenario failure single power distribution unit takes processes cron service may possible deploy single cron service across globe deploying cron within single datacenter benefits service enjoys low latency shares fate datacenter scheduler cron s core dependency building cron google section address problems must resolved order provide largescale distributed deployment cron reliably also highlights important decisions made regards distributed cron google tracking state cron jobs discussed previous sections need hold amount state cron jobs able restore information quickly case failure moreover consistency state paramount recall many cron jobs like payroll run sending email newsletter idempotent two options track state cron jobs store data externally generally available distributed storage use system stores small volume state part cron service designing distributed cron chose second option made choice several reasons distributed filesystems gfs hdfs often cater use case large files example output web crawling programs whereas information need store cron jobs small small writes distributed filesystem expensive come high latency filesystem optimized types writes base services outages wide impact cron dependencies even parts datacenter go away cron service able function least amount time requirement mean storage part cron process directly storage handled essentially implementation detail however cron able operate independently downstream systems cater large number internal users use paxos deploy multiple replicas cron service use paxos distributed consensus algorithm see managing critical state distributed consensus reliability ensure consistent state long majority group members available distributed system whole successfully process new state changes despite failure bounded subsets infrastructure shown figure distributed cron uses single leader job replica modify shared state well replica launch cron jobs take advantage fact variant paxos use fast paxos lam uses leader replica internally optimizationthe fast paxos leader replica also acts cron service leader figure interactions distributed cron replicas leader replica dies healthchecking mechanism paxos group discovers event quickly within seconds another cron process already started available elect new leader soon new leader elected follow leader election protocol specific cron service responsible taking work left unfinished previous leader leader specific cron service paxos leader cron service needs take additional action upon promotion fast reaction time leader reelection allows us stay well within generally tolerable oneminute failover time important state keep paxos information regarding cron jobs launched synchronously inform quorum replicas beginning end scheduled launch cron job roles leader follower described use paxos deployment cron service two assigned roles leader follower following sections describe role leader leader replica replica actively launches cron jobs leader internal scheduler much like simple crond described beginning chapter maintains list cron jobs ordered scheduled launch time leader replica waits scheduled launch time first job upon reaching scheduled launch time leader replica announces start particular cron job s launch calculates new scheduled launch time like regular crond implementation would course regular crond cron job launch specification may changed since last execution launch specification must kept sync followers well simply identifying cron job enough also uniquely identify particular launch using start time otherwise ambiguity cron job launch tracking may occur ambiguity especially likely case highfrequency cron jobs running every minute seen figure communication performed paxos important paxos communication remain synchronous actual cron job launch proceed receives confirmation paxos quorum received launch notification cron service needs understand whether cron job launched order decide next course action case leader failover performing task synchronously could mean entire cron job launch happens leader without informing follower replicas case failover follower replicas might attempt perform launch nt aware launch already occurred figure illustration progress cron job launch leader s perspective completion cron job launch announced via paxos replicas synchronously note matter whether launch succeeded failed external reasons example datacenter scheduler unavailable simply keeping track fact cron service attempted launch given scheduled time also need able resolve failures cron system middle operation discussed following section another extremely important feature leader soon loses leadership reason must immediately stop interacting datacenter scheduler holding leadership guarantee mutual exclusion access datacenter scheduler absence condition mutual exclusion old new leaders might perform conflicting actions datacenter scheduler follower follower replicas keep track state world provided leader order take moment s notice needed state changes tracked follower replicas communicated via paxos leader replica much like leader followers also maintain list cron jobs system list must kept consistent among replicas use paxos upon receiving notification commenced launch follower replica updates local next scheduled launch time given cron job important state change performed synchronously ensures cron job schedules within system consistent keep track open launches launches begun completed leader replica dies otherwise malfunctions eg partitioned away replicas network follower elected new leader election must converge faster one minute order avoid risk missing unreasonably delaying cron job launch leader elected open launches ie partial failures must concluded process quite complicated imposing additional requirements cron system datacenter infrastructure following section discusses resolve partial failures type resolving partial failures mentioned interaction leader replica datacenter scheduler fail sending multiple rpcs describe single logical cron job launch systems able handle condition recall every cron job launch two synchronization points perform launch finished launch two points allow us delimit launch even launch consists single rpc know rpc actually sent consider case know scheduled launch started notified completion leader replica died order determine rpc actually sent one following conditions must met operations external systems may need continue upon reelection must idempotent ie safely perform operations must able look state operations external systems order unambiguously determine whether completed conditions imposes significant constraints may difficult implement able meet least one conditions fundamental accurate operation cron service distributed environment could suffer single several partial failures handling appropriately lead missed launches double launch cron job infrastructure launches logical jobs datacenters mesos example provides naming datacenter jobs making possible look state jobs stop jobs perform maintenance reasonable solution idempotency problem construct job names ahead time thereby avoiding causing mutating operations datacenter scheduler distribute names replicas cron service cron service leader dies launch new leader simply looks state precomputed names launches missing names note similar method identifying individual cron job launches name launch time important constructed job names datacenter scheduler include particular scheduled launch time information otherwise retrievable regular operation cron service fail quickly case leader failure quick failover nt always happen recall track scheduled launch time keeping internal state replicas similarly need disambiguate interaction datacenter scheduler also using scheduled launch time example consider shortlived frequently run cron job cron job launches launch communicated replicas leader crashes unusually long failoverlong enough cron job finishes successfully takes place new leader looks state cron job observes completion attempts launch job launch time included new leader would know job datacenter scheduler result particular cron job launch double launch would happened actual implementation complicated system state lookup driven implementation details underlying infrastructure however preceding description covers implementationindependent requirements system depending available infrastructure may also need consider tradeoff risking double launch risking skipping launch storing state using paxos achieve consensus one part problem handle state paxos essentially continuous log state changes appended synchronously state changes occur characteristic paxos two implications log needs compacted prevent growing infinitely log must stored somewhere order prevent infinite growth paxos log simply take snapshot current state means reconstruct state without needing replay state change log entries leading current state provide example state changes stored logs increment counter thousand iterations thousand log entries easily changed snapshot set counter case lost logs lose state since last snapshot snapshots fact critical stateif lose snapshots essentially start zero ve lost internal state losing logs hand causes bounded loss state sends cron system back time point latest snapshot taken two main options storing data externally generally available distributed storage system stores small volume state part cron service designing system combined elements options store paxos logs local disk machine cron service replicas scheduled three replicas default operation implies three copies logs store snapshots local disk well however critical also back onto distributed filesystem thus protecting failures affecting three machines store logs distributed filesystem consciously decided losing logs represent small amount recent state changes acceptable risk storing logs distributed filesystem entail substantial performance penalty caused frequent small writes simultaneous loss three machines unlikely simultaneous loss occur automatically restore snapshot thereby lose small amount logs taken since last snapshot perform configurable intervals course tradeoffs may different depending details infrastructure well requirements placed cron system addition logs snapshots stored local disk snapshot backups distributed filesystem freshly started replica fetch state snapshot logs already running replica network ability makes replica startup independent state local machine therefore rescheduling replica different machine upon restart machine death essentially nonissue reliability service running large cron smaller equally interesting implications running large cron deployment traditional cron small probably contains order tens cron jobs however run cron service thousands machines datacenter usage grow may run problems beware large wellknown problem distributed systems thundering herd based user configuration cron service cause substantial spikes datacenter usage people think daily cron job commonly configure job run midnight setup works fine cron job launches machine cron job spawn mapreduce thousands workers different teams decide run daily cron job like datacenter solve problem introduced extension crontab format ordinary crontab users specify minute hour day month week month cron job launch asterisk specify value running midnight daily would crontab specification ie zeroth minute zeroth hour every day week every month every day week also introduced use question mark means value acceptable cron system given freedom choose value users choose value hashing cron job configuration given time range eg hour therefore distributing launches evenly despite change load caused cron jobs still spiky graph figure illustrates aggregate global number launches cron jobs google graph highlights frequent spikes cron job launches often caused cron jobs need launched specific time example due temporal dependency external events figure number cron jobs launched globally summary cron service fundamental feature unix systems many decades industry move toward large distributed systems datacenter may smallest effective unit hardware requires changes large portions stack cron exception trend careful look required properties cron service requirements cron jobs drives google s new design discussed new constraints demanded distributedsystem environment possible design cron service based google s solution solution requires strong consistency guarantees distributed environment core distributed cron implementation therefore paxos commonplace algorithm reach consensus unreliable environment use paxos correct analysis new failure modes cron jobs largescale distributed environment allowed us build robust cron service heavily used google this chapter previously published part acm queue march vol issue failure individual jobs beyond scope analysis chapter data processing pipelines written dan dennisonedited tim harvey chapter focuses reallife challenges managing data processing pipelines depth complexity considers frequency continuum periodic pipelines run infrequently continuous pipelines never stop running discusses discontinuities produce significant operational problems fresh take leaderfollower model presented reliable betterscaling alternative periodic pipeline processing big data origin pipeline design pattern classic approach data processing write program reads data transforms desired way outputs new data typically program scheduled run control periodic scheduling program cron design pattern called data pipeline data pipelines go far back coroutines con dtss communication files bul unix pipe mci later etl pipelines pipelines gained increased attention rise big data datasets large complex traditional data processing applications inadequate initial effect big data simple pipeline pattern programs perform periodic continuous transformations big data usually referred simple onephase pipelines given scale processing complexity inherent big data programs typically organized chained series output one program becoming input next may varied rationales arrangement typically designed ease reasoning system usually geared toward operational efficiency programs organized way called multiphase pipelines program chain acts discrete data processing phase number programs chained together series measurement known depth pipeline thus shallow pipeline may one program corresponding pipeline depth measurement one whereas deep pipeline may pipeline depth tens hundreds programs challenges periodic pipeline pattern periodic pipelines generally stable sufficient workers volume data execution demand within computational capacity addition instabilities processing bottlenecks avoided number chained jobs relative throughput jobs remain uniform periodic pipelines useful practical run regular basis google written frameworks like mapreduce dea flume cha among others however collective sre experience periodic pipeline model fragile discovered periodic pipeline first installed worker sizing periodicity chunking technique parameters carefully tuned performance initially reliable however organic growth change inevitably begin stress system problems arise examples problems include jobs exceed run deadline resource exhaustion hanging processing chunks entail corresponding operational load trouble caused uneven work distribution key breakthrough big data widespread application embarrassingly parallel mol algorithms cut large workload chunks small enough fit onto individual machines sometimes chunks require uneven amount resources relative one another seldom initially obvious particular chunks require different amounts resources example workload partitioned customer data chunks customers may much larger others customer point indivisibility endtoend runtime thus capped runtime largest customer hanging chunk problem result resources assigned due differences machines cluster overallocation job problem arises due difficulty realtime operations streams sorting steaming data pattern typical user code wait total computation complete progressing next pipeline stage commonly sorting may involved requires data proceed significantly delay pipeline completion time completion blocked worstcase performance dictated chunking methodology use problem detected engineers cluster monitoring infrastructure response make matters worse example sensible default response hanging chunk immediately kill job allow job restart blockage may well result nondeterministic factors however pipeline implementations design usually include checkpointing work chunks restarted beginning thereby wasting time cpu cycles human effort invested previous cycle drawbacks periodic pipelines distributed environments big data periodic pipelines widely used google google cluster management solution includes alternative scheduling mechanism pipelines mechanism necessary unlike continuously running pipelines periodic pipelines typically run lowerpriority batch jobs lowerpriority designation works well case batch work sensitive latency way internetfacing web services addition order control cost maximizing machine workload borg google cluster management system ver assigns batch work available machines priority result degraded startup latency pipeline jobs potentially experience openended startup delays jobs invoked mechanism number natural limitations resulting various distinct behaviors example jobs scheduled gaps left userfacing web service jobs might impacted terms availability lowlatency resources pricing stability access resources execution cost inversely proportional requested startup delay directly proportional resources consumed although batch scheduling may work smoothly practice excessive use batch scheduler distributed periodic scheduling cron places jobs risk preemptions see section ver cluster load high users starved batch resources light risk tradeoffs running welltuned periodic pipeline successfully delicate balance high resource cost risk preemptions delays hours might well acceptable pipelines run daily however scheduled execution frequency increases minimum time executions quickly reach minimum average delay point placing lower bound latency periodic pipeline expect attain reducing job execution interval effective lower bound simply results undesirable behavior rather increased progress specific failure mode depends batch scheduling policy use example new run might stack cluster scheduler previous run complete even worse currently executing nearly finished run could killed next execution scheduled begin completely halting progress name increasing executions note downwardsloping idle interval line intersects scheduling delay figure scenario lowering execution interval much minutes minute job results potentially overlapping executions undesired consequences figure periodic pipeline execution interval versus idle time log scale solution problem secure sufficient server capacity proper operation however resource acquisition shared distributed environment subject supply demand expected development teams tend reluctant go processes acquiring resources resources must contributed common pool shared resolve distinction batch scheduling resources versus production priority resources made rationalize resource acquisition costs monitoring problems periodic pipelines pipelines sufficient execution duration realtime information runtime performance metrics important even important knowing overall metrics realtime data important providing operational support including emergency response practice standard monitoring model involves collecting metrics job execution reporting metrics upon completion job fails execution statistics provided continuous pipelines share problems tasks constantly running telemetry routinely designed realtime metrics available periodic pipelines inherent monitoring problems observed strong association thundering herd problems adding execution monitoring challenges thundering herd problem endemic distributed systems also discussed distributed periodic scheduling cron given large enough periodic pipeline cycle potentially thousands workers immediately start work many workers workers misconfigured invoked faulty retry logic servers run overwhelmed underlying shared cluster services networking infrastructure used also overwhelmed worsening situation retry logic implemented correctness problems result work dropped upon failure job retried retry logic present naive poorly implemented retry upon failure compound problem human intervention also contribute scenario engineers limited experience managing pipelines tend amplify problem adding workers pipeline job fails complete within desired period time regardless source thundering herd problem nothing harder cluster infrastructure sres responsible cluster various services buggy worker pipeline job moiré load pattern sometimes thundering herd problem may obvious spot isolation related problem call moiré load pattern occurs two pipelines run simultaneously execution sequences occasionally overlap causing simultaneously consume common shared resource problem occur even continuous pipelines although less common load arrives evenly moiré load patterns apparent plots pipeline usage shared resources example figure identifies resource usage three periodic pipelines figure stacked version data previous graph peak impact causing oncall pain occurs aggregate load nears m figure moiré load pattern separate infrastructure figure moiré load pattern shared infrastructure introduction google workflow inherently oneshot batch pipeline overwhelmed business demands continuously updated results pipeline development team usually considers either refactoring original design satisfy current demands moving continuous pipeline model unfortunately business demands usually occur least convenient time refactor pipeline system online continuous processing system newer larger customers faced forcing scaling issues typically also want include new features expect requirements adhere immovable deadlines anticipating challenge important ascertain several details outset designing system involving proposed data pipeline sure scope expected growth trajectory demand design modifications expected additional resources expected latency requirements business faced needs google developed system called workflow makes continuous processing available scale workflow uses leaderfollower workers distributed systems design pattern sha system prevalence design pattern combination enables largescale transactional data pipelines ensuring correctness exactlyonce semantics workflow modelviewcontroller pattern system prevalence works useful think workflow distributed systems equivalent modelviewcontroller pattern known user interface development shown figure design pattern divides given software application three interconnected parts separate internal representations information ways information presented accepted user figure modelviewcontroller pattern used user interface design adapting pattern workflow model held server called task master task master uses system prevalence pattern hold job states memory fast availability synchronously journaling mutations persistent disk view workers continually update system state transactionally master according perspective subcomponent pipeline although pipeline data may stored task master best performance usually achieved pointers work stored task master actual input output data stored common filesystem storage supporting analogy workers completely stateless discarded time controller optionally added third system component efficiently support number auxiliary system activities affect pipeline runtime scaling pipeline snapshotting workcycle state control rolling back pipeline state even performing global interdiction business continuity figure illustrates design pattern figure modelviewcontroller design pattern adapted google workflow stages execution workflow increase pipeline depth level inside workflow subdividing processing task groups held task master task group holds work corresponding pipeline stage perform arbitrary operations piece data relatively straightforward perform mapping shuffling sorting splitting merging operation stage stage usually worker type associated multiple concurrent instances given worker type workers selfscheduled sense look different types work choose type perform worker consumes work units previous stage produces output units output end point input processing stage within system easy guarantee work executed least reflected permanent state exactly workflow correctness guarantees practical store every detail pipeline state inside task master task master limited ram size however double correctness guarantee persists master holds collection pointers uniquely named data work unit uniquely held lease workers acquire work lease may commit work tasks currently possess valid lease avoid situation orphaned worker may continue working work unit thus destroying work current worker output file opened worker unique name way even orphaned workers continue writing independently master attempt commit upon attempting commit unable another worker holds lease work unit furthermore orphaned workers destroy work produced valid worker unique filename scheme ensures every worker writing distinct file way double correctness guarantee holds output files always unique pipeline state always correct virtue tasks leases double correctness guarantee enough workflow also versions tasks task updates task lease changes operation yields new unique task replacing previous one new id assigned task pipeline configuration workflow stored inside task master form work units order commit work worker must active lease reference task id number configuration used produce result configuration changed work unit flight workers type unable commit despite owning current leases thus work performed configuration change consistent new configuration cost work thrown away workers unfortunate enough hold old leases measures provide triple correctness guarantee configuration lease ownership filename uniqueness however even sufficient cases example task master network address changed different task master replaced address memory corruption altered ip address port number resulting another task master end even commonly someone mis configured task master setup inserting load balancer front set independent task masters workflow embeds server token unique identifier particular task master task metadata prevent rogue incorrectly configured task master corrupting pipeline client server check token operation avoiding subtle misconfiguration operations run smoothly task identifier collision occurs summarize four workflow correctness guarantees worker output configuration tasks creates barriers predicate work work committed requires currently valid lease held worker output files uniquely named workers client server validate task master checking server token every operation point may occur would simpler forgo specialized task master use spanner cor another database however workflow special task unique immutable twin properties prevent many potentially subtle issues widescale work distribution occurring example lease obtained worker part task requiring brand new task even lease changes database used directly transaction logs act like journal every read must part longrunning transaction configuration certainly possible terribly inefficient ensuring business continuity big data pipelines need continue processing despite failures types including fiber cuts weather events cascading power grid failures types failures disable entire datacenters addition pipelines employ system prevalence obtain strong guarantees job completion often disabled enter undefined state architecture gap makes brittle business continuity strategy entails costly mass duplication effort restore pipelines data workflow resolves problem conclusively continuous processing pipelines obtain global consistency task master stores journals spanner using globally available globally consistent lowthroughput filesystem determine task master write task master uses distributed lock service called chubby bur elect writer result persisted spanner finally clients look current task master using internal naming services spanner make highthroughput filesystem globally distributed workflows employ two local workflows running distinct clusters addition notion reference tasks stored global workflow units work tasks consumed pipeline equivalent reference tasks inserted global workflow binary labeled stage figure tasks finish reference tasks transactionally removed global workflow depicted stage n figure tasks removed global workflow local workflow block global workflow becomes available ensuring transactional correctness automate failover helper binary labeled stage figure runs inside local workflow local workflow otherwise unaltered described work box diagram helper binary acts controller mvc sense responsible creating reference tasks well updating special heartbeat task inside global workflow heartbeat task updated within timeout period remote workflow helper binary seizes work progress documented reference tasks pipeline continues unhindered whatever environment may work figure example distributed data process flow using workflow pipelines summary concluding remarks periodic pipelines valuable however data processing problem continuous organically grow become continuous use periodic pipeline instead use technology characteristics similar workflow found continuous data processing strong guarantees provided workflow performs scales well distributed cluster infrastructure routinely produces results users rely upon stable reliable system site reliability engineering team manage maintain wikipedia extract transform http enwikipediaorgwikiextract transform load load wikipedia big data http enwikipediaorgwikibigdata jeff dean lecture software engineering advice building large scale distributed systems excellent resource dea wikipedia system http enwikipediaorgwikisystemprevalence prevalence the modelviewcontroller pattern analogy distributed systems loosely borrowed smalltalk originally used describe design structure graphical user interfaces fow wikipedia modelviewcontroller http enwikipediaorgwikimodel e view e controller chapter data integrity read wrote written raymond blum rhandeev singhedited betsy beyer data integrity users come first data integrity whatever users think might say data integrity measure accessibility accuracy datastores needed provide users adequate level service definition insufficient instance user interface bug gmail displays empty mailbox long users might believe data lost thus even data actually lost world would question google ability act responsible steward data viability cloud computing would threatened gmail display error maintenance message long bit metadata repaired trust google users would similarly erode long long data unavailable demonstrated actual gmail incident hic four days long timeperhaps long subsequently believe hours good starting point establishing threshold long google apps similar reasoning applies applications like google photos drive cloud storage cloud datastore users necessarily draw distinction discrete products reasoning product still google google amazon whatever product still part cloud data loss data corruption extended unavailability typically indistinguishable users therefore data integrity applies types data across services considering data integrity matters services cloud remain accessible users user access data especially important data integrity strict requirements considering reliability needs given system may seem uptime service availability needs stricter data integrity example users may find hour email downtime unacceptable whereas may live grumpily fourday time window recover mailbox however appropriate way consider demands uptime versus data integrity slo uptime leaves room hour downtime whole year slo sets rather high bar likely exceeds expectations internet enterprise users contrast slo good bytes gb artifact would render documents executables databases corrupt kb garbled amount corruption catastrophic majority casesresulting executables random opcodes completely unloadable databases user perspective every service independent uptime data integrity requirements even requirements implicit worst time disagree users requirements demise data figure revise earlier definition data integrity might say data integrity means services cloud remain accessible users user access data especially important access remain perfect shape suppose artifact corrupted lost exactly year loss unrecoverable uptime affected artifact lost year likely means avoid loss proactive detection coupled rapid repair alternate universe suppose corruption immediately detected users affected artifact removed fixed returned service within half hour ignoring downtime minutes object would available year astonishingly least user perspective scenario data integrity still close accessible lifetime object demonstrated example secret superior data integrity proactive detection rapid repair recovery choosing strategy superior data integrity many possible strategies rapid detection repair recovery lost data strategies trade uptime data integrity respect affected users strategies work better others strategies require complex engineering investment others many options available strategies utilize answer depends computing paradigm cloud computing applications seek optimize combination uptime latency scale velocity privacy provide working definition terms uptime also referred availability proportion time service usable users latency responsive service appears users scale service volume users mixture workloads service handle latency suffers service falls apart velocity fast service innovate provide users superior value reasonable cost privacy concept imposes complex requirements simplification chapter limits scope discussing privacy data deletion data must destroyed within reasonable time users delete many cloud applications continually evolve atop mixture acid base apis meet demands five components base allows higher availability acid exchange softer distributed consistency guarantee specifically base guarantees piece data longer updated value eventually become consistent across potentially distributed storage locations following scenario provides example tradeoffs uptime latency scale velocity privacy might play velocity trumps requirements resulting applications rely arbitrary collection apis familiar particular developers working application example application may take advantage efficient blob storage api blobstore neglects distributed consistency favor scaling heavy workloads high uptime low latency low cost compensate application may entrust small amounts authoritative metadata pertaining blobs higher latency less available costly paxosbased service megastore bak lam certain clients application may cache metadata locally access blobs directly shaving latency still vantage point users another application may keep metadata bigtable sacrificing strong distributed consistency developers happened familiar bigtable cloud applications face variety data integrity challenges runtime referential integrity datastores preceding example blobstore megastore clientside caches vagaries high velocity dictate schema changes data migrations piling new features atop old features rewrites evolving integration points applications collude produce environment riddled complex relationships various pieces data single engineer fully groks prevent application data degrading users eyes system outofband checks balances needed within datastores third layer early detection discusses system addition application relies independent uncoordinated backups several datastores preceding example blobstore megastore ability make effective use restored data data recovery effort complicated variety relationships restored live data example application would sort distinguish restored blobs versus live megastore restored megastore versus live blobs restored blobs versus restored megastore interactions clientside caches consideration dependencies complications many resources invested data integrity efforts backups versus archives traditionally companies protect data loss investing backup strategies however real focus backup efforts data recovery distinguishes real backups archives sometimes observed one really wants make backups people really want restores backup really archive rather appropriate use disaster recovery figure important difference backups archives backups loaded back application archives therefore backups archives quite differing use cases archives safekeep data long periods time meet auditing discovery compliance needs data recovery purposes generally need complete within uptime requirements service example might need retain financial transaction data seven years achieve goal could move accumulated audit logs longterm archival storage offsite location month retrieving recovering logs monthlong financial audit may take week weeklong time window recovery may acceptable archive hand disaster strikes data must recovered real backups quickly preferably well within uptime needs service otherwise affected users left without useful access application onset data integrity issue completion recovery effort also important consider recent data risk safely backed may optimal schedule real backups opposed archives occur daily hourly frequently using full incremental continuous streaming approaches therefore formulating backup strategy consider quickly need able recover problem much recent data afford lose requirements cloud environment perspective cloud environments introduce unique combination technical challenges environment uses mixture transactional nontransactional backup restore solutions recovered data necessarily correct services must evolve without going maintenance different versions business logic may act data parallel interacting services versioned independently incompatible versions different services may interact momentarily increasing chance accidental data corruption data loss addition order maintain economy scale service providers must provide limited number apis apis must simple easy use vast majority applications customers use time apis must robust enough understand following data locality caching local global data distribution strong andor eventual consistency data durability backup recovery otherwise sophisticated customers migrate applications cloud simple applications grow complex large need complete rewrites order use different complex apis problems arise preceding api features used certain combinations service provider solve problems applications run challenges must identify solve independently google sre objectives maintaining data integrity availability sre goal maintaining integrity persistent data good vision thrive concrete objectives measurable indicators sre defines key metrics use set expectations capabilities systems processes tests track performance actual event data integrity means data availability goal data integrity refers accuracy consistency data throughout lifetime users need know information correct change unexpected way time first recorded last time observed assurance enough consider case email provider suffered weeklong data outage kinc space days users find temporary methods conducting business expectation soon return established email accounts identities accumulated histories worst possible news arrived provider announced despite earlier expectations trove past email contacts fact gone evaporated never seen seemed series mishaps managing data integrity conspired leave service provider usable backups furious users either stuck interim identities established new identities abandoning troubled former email provider wait several days declaration absolute loss provider announced users personal information could recovered data loss outage well except well user data preserved data accessible people needed long moral example user point view data integrity without expected regular data availability effectively data delivering recovery system rather backup system making backups classically neglected delegated deferred task system administration backups high priority anyonethey ongoing drain time resources yield immediate visible benefit reason lack diligence implementing backup strategy typically met sympathetic eye roll one might argue like measures protection lowrisk dangers attitude pragmatic fundamental problem lackadaisical strategy dangers entails may low risk also high impact service data unavailable response make break service product even company instead focusing thankless job taking backup much useful mention easier motivate participation taking backups concentrating task visible payoff restore backups tax one paid ongoing basis municipal service guaranteed data availability instead emphasizing tax draw attention service tax funds data availability make teams practice backups instead teams define service level objectives slos data availability variety failure modes team practices demonstrates ability meet slos types failures lead data loss illustrated figure high level distinct types failures factors occur combination consider potential failures designing data integrity program factors data integrity failure modes follows unrecoverable loss data may caused number factors user action operator error application bugs defects infrastructure faulty hardware site catastrophes scope losses widespread affecting many entities losses narrow directed deleting corrupting data specific small subset users rate data losses big bang event example million rows replaced rows single minute whereas data losses creeping example rows data deleted every minute course weeks figure factors data integrity failure modes effective restore plan must account failure modes occurring conceivable combination may perfectly effective strategy guarding data loss caused creeping application bug may help whatsoever colocation datacenter catches fire study data recovery efforts google found common uservisible data loss scenarios involved data deletion loss referential integrity caused software bugs challenging variants involved lowgrade corruption deletion discovered weeks months bugs first released production environment therefore safeguards google employs well suited prevent recover types loss recover scenarios large successful application needs retrieve data perhaps millions users spread across days weeks months application may also need recover affected artifact unique point time data recovery scenario called pointintime recovery outside google timetravel inside google backup recovery solution provides pointintime recovery application across acid base datastores meeting strict uptime latency scalability velocity cost goals chimera today solving problem engineers entails sacrificing velocity many projects compromise adopting tiered backup strategy without pointintime recovery instance apis beneath application may support variety data recovery mechanisms expensive local snapshots may provide limited protection application bugs offer quick restoration functionality might retain days local snapshots taken several hours apart costeffective full incremental copies every two days may retained longer pointintime recovery nice feature one strategies support consider data recovery options provided cloud apis use trade pointintime recovery tiered strategy necessary resort using either features use features features valuable point challenges maintaining data integrity deep wide designing data integrity program important recognize replication redundancy recoverability scaling issues fulls incrementals competing forces backups restores classic flawed response question backup something even better backupreplication replication provides many benefits including locality data protection sitespecific disaster protect many sources data loss datastores automatically sync multiple replicas guarantee corrupt database row errant delete pushed copies likely isolate problem address concern might make nonserving copies data format frequent database exports native file additional measure adds protection types errors replication protect againstuser errors applicationlayer bugsbut nothing guard losses introduced lower layer measure also introduces risk bugs data conversion directions storage native file addition possible mismatches semantics two formats imagine zeroday attack low level stack filesystem device driver copies rely compromised software component including database exports written filesystem backs database vulnerable thus see diversity key protecting failure layer x requires storing data diverse components layer media isolation protects media flaws bug attack disk device driver unlikely affect tape drives could make backup copies valuable data clay tablets forces data freshness restore completion compete comprehensive protection stack push snapshot data longer takes make copy means frequency copies decreases database level transaction may take order seconds replicate exporting database snapshot filesystem underneath may take minutes full backup underlying filesystem may take hours scenario may lose minutes recent data restore latest snapshot restore filesystem backup might incur hours missing transactions additionally restoring probably takes long backing actually loading data might take hours obviously like freshest data back quickly possible depending type failure freshest immediately available copy might option retention retentionhow long keep copies data aroundis yet another factor consider data recovery plans likely customers quickly notice sudden emptying entire database might take days gradual loss data attract right person attention restoring lost data latter scenario requires snapshots taken back time reaching back far likely want merge restored data current state significantly complicates restore process google sre faces challenges data integrity similar assumption google underlying systems prone failure assume protection mechanisms also subject forces fail ways inconvenient times maintaining guarantee data integrity large scale challenge complicated high rate change involved software systems requires number complementary uncoupled practices chosen offer high degree protection combinations data integrity failure modes given many ways data lost described previously silver bullet guards many combinations failure modes instead need defense depth defense depth comprises multiple layers successive layer defense conferring protection progressively less common data loss scenarios figure illustrates object journey soft deletion destruction data recovery strategies employed along journey ensure defense depth first layer soft deletion lazy deletion case developer api offerings proven effective defense inadvertent data deletion scenarios second line defense backups related recovery methods third final layer regular data validation covered third layer early detection across layers presence replication occasionally useful data recovery specific scenarios although data recovery plans rely upon replication figure object journey soft deletion destruction first layer soft deletion velocity high privacy matters bugs applications account vast majority data loss corruption events fact data deletion bugs may become common ability undelete data limited time becomes primary line defense majority otherwise permanent inadvertent data loss product upholds privacy users must allow users delete selected subsets andor data products incur support burden due accidental deletion giving users ability undelete data example via trash folder reduces completely eliminate support burden particularly service also supports thirdparty addons also delete data soft deletion dramatically reduce even completely eliminate support burden soft deletion means deleted data immediately marked rendering unusable application administrative code paths administrative code paths may include legal discovery hijacked account recovery enterprise administration user support problem troubleshooting related features conduct soft deletion user empties trash provide user support tool enables authorized administrators undelete items accidentally deleted users google implements strategy popular productivity applications otherwise user support engineering burden would untenable extend soft deletion strategy even offering users option recover deleted data example gmail trash bin allows users access messages deleted fewer days ago another common source unwanted data deletion occurs result account hijacking account hijacking scenarios hijacker commonly deletes original user data using account spamming unlawful purposes combine commonality accidental user deletion risk data deletion hijackers case programmatic soft deletion undeletion interface within andor beneath application becomes clear soft deletion implies data marked destroyed reasonable delay length delay depends upon organization policies applicable laws available storage resources cost product pricing market positioning especially cases involving much shortlived data common choices soft deletion delays days google experience majority account hijacking data integrity issues reported detected within days therefore case soft deleting data longer days may strong google also found devastating acute data deletion cases caused application developers unfamiliar existing code working deletionrelated code especially batch processing pipelines eg offline mapreduce hadoop pipeline advantageous design interfaces hinder developers unfamiliar code circumventing soft deletion features new code one effective way achieving implement cloud computing offerings include builtin soft deletion undeletion apis making sure enable said feature even best armor useless put soft deletion strategies cover data deletion features consumer products like gmail google drive support cloud computing offering instead assuming cloud computing offering already supports programmatic soft deletion undeletion feature reasonable defaults remaining accidental data deletion scenarios originate mistakes made internal developers developer customers cases useful introduce additional layer soft deletion refer lazy deletion think lazy deletion behind scenes purging controlled storage system whereas soft deletion controlled expressed client application service lazy deletion scenario data deleted cloud application becomes immediately inaccessible application preserved cloud service provider weeks destruction lazy deletion advisable defense depth strategies long lazy deletion period costly systems much shortlived data impractical systems must guarantee destruction deleted data within reasonable time frame ie offer privacy guarantees sum first layer defense depth trash folder allows users undelete data primary defense user error soft deletion primary defense developer error secondary defense user error developer offerings lazy deletion primary defense internal developer error secondary defense external developer error revision history products provide ability revert items previous states feature available users form trash available developers may may substitute soft deletion depending implementation google revision history proven useful recovering certain data corruption scenarios recovering data loss scenarios involving accidental deletion programmatic otherwise revision history implementations treat deletion special case previous states must removed opposed mutating item whose history may retained certain time period provide adequate protection unwanted deletion apply lazy andor soft deletion principles revision history also second layer backups related recovery methods backups data recovery second line defense soft deletion important principle layer backups matter matters recovery factors supporting successful recovery drive backup decisions way around words scenarios want backups help recover dictate following backup recovery methods use frequently establish restore points taking full incremental backups data store backups long retain backups much recent data afford lose recovery effort less data afford lose serious incremental backup strategy one google extreme cases used nearrealtime streaming backup strategy older version gmail even money limitation frequent full backups expensive ways notably impose compute burden live datastores service serving users driving service closer scalability performance limits ease burden take full backups offpeak hours series incremental backups service busier quickly need recover faster users need rescued local backups often google retains costly quicktorestore snapshots short periods time within storage instance stores less recent backups random access distributed storage within nearby datacenter slightly longer time strategy alone would protect sitelevel failures backups often transferred nearline offline locations longer time period expired favor newer backups far back backups reach backup strategy becomes costly back reach scenarios hope recover increase although increase subject diminishing returns google experience lowgrade data mutation deletion bugs within application code demand furthest reaches back time bugs noticed months first data loss began cases suggest like ability reach back time far possible flip side highvelocity development environment changes code schema may render older backups expensive impossible use furthermore challenging recover different subsets data different restore points would involve multiple backups yet exactly sort recovery effort demanded lowgrade data corruption deletion scenarios strategies described third layer early detection meant speed detection lowgrade data mutation deletion bugs within application code least partly warding need type complex recovery effort still confer reasonable protection know kinds issues detect google chose draw line days backups many services service falls within window depends tolerance data loss relative investments early detection sum advice guarding combinations data integrity failure modes addressing broad range scenarios reasonable cost demands tiered backup strategy first tier comprises many frequent quickly restored backups stored closest live datastores perhaps using similar storage technologies data sources confers protection majority scenarios involving software bugs developer error due relative expense backups retained tier anywhere hours singledigit days may take minutes restore second tier comprises fewer backups retained singledigit low doubledigit days random access distributed filesystems local site backups may take hours restore confer additional protection mishaps affecting particular storage technologies serving stack technologies used contain backups tier also protects bugs application detected late rely upon first tier backup strategy introducing new versions code production twice week may make sense retain backups least week two deleting subsequent tiers take advantage nearline storage dedicated tape libraries offsite storage backup media eg tapes disk drives backups tiers confer protection sitelevel issues datacenter power outage distributed filesystem corruption due bug expensive move large amounts data tiers hand storage capacity later tiers contend growth live production storage instances service result backups tiers tend taken less frequently retained longer overarching layer replication ideal world every storage instance including instances containing backups would replicated data recovery effort last thing want discover backups lost needed data datacenter containing useful backup maintenance volume data increases replication every storage instance always feasible cases makes sense stagger successive backups across different sites may fail independently write backups using redundancy method raid reedsolomon erasure codes gfsstyle replication choosing system redundancy rely upon infrequently used scheme whose tests efficacy infrequent data recovery attempts instead choose popular scheme common continual use many users t versus e bigger backup processes practices applied volumes data measured terabytes scale well data measured e exabytes validating copying performing roundtrip tests gigabytes structured data interesting problem however assuming sufficient knowledge schema transaction model exercise present special challenges typically need procure machine resources iterate data perform validation logic delegate enough storage hold copies data let ante instead gigabytes let try securing validating petabytes structured data assuming ideal sata performance mbs single task iterates data performs even basic validation checks take decades making full backups assuming media going take least long restore time postprocessing take even longer looking almost full century restore backup years old started restore obviously strategy needs rethought common largely effective technique used back massive amounts data establish trust points dataportions stored data verified rendered immutable usually passage time know given user profile transaction fixed subject change verify internal state make suitable copies recovery purposes make incremental backups include data modified added since last backup technique brings backup time line mainline processing time meaning frequent incremental backups save year monolithic verify copy job however remember care restores backups let say took full backup three years ago making daily incremental backups since full restore data serially process chain highly interdependent backups independent backup incurs additional risk failure mention logistical burden scheduling runtime cost jobs another way reduce wall time copying verification jobs distribute load shard data well possible run n tasks parallel task responsible copying verifying nth data requires forethought planning schema design physical deployment data order balance data correctly ensure independence shard avoid contention among concurrent sibling tasks distributing load horizontally restricting work vertical slices data demarcated time reduce eight decades wall time several orders magnitude rendering restores relevant third layer early detection third layer early detection bad data sit idly propagates references missing corrupt data copied links fan every update overall quality datastore goes subsequent dependent transactions potential data format changes make restoring given backup difficult clock ticks sooner know data loss easier complete recovery challenges faced cloud developers highvelocity environments cloud application infrastructure services face many data integrity challenges runtime referential integrity datastores schema changes aging code zerodowntime data migrations evolving integration points services without conscious engineering effort track emerging relationships data data quality successful growing service degrades time often novice cloud developer chooses distributed consistent storage api megastore delegates integrity application data distributed consistent algorithm implemented beneath api paxos see managing critical state distributed consensus reliability developer reasons selected api alone keep application data good shape result unify application data single storage solution guarantees distributed consistency avoiding referential integrity problems exchange reduced performance andor scale algorithms infallible theory implementations often riddled hacks optimizations bugs educated guesses example theory paxos ignores failed compute nodes make progress long quorum functioning nodes maintained practice however ignoring failed node may correspond timeouts retries failurehandling approaches beneath particular paxos implementation cha long paxos try contact unresponsive node timing particular machine fails perhaps intermittently certain way certain timing particular datacenter unpredictable behavior results larger scale application frequently application affected unbeknownst inconsistencies logic holds true even applied paxos implementations true google must true eventually consistent implementations bigtable also shown true affected applications way know data good check trust storage systems verify complicate problem order recover lowgrade data corruption deletion scenarios must recover different subsets data different restore points using different backups changes code schema may render older backups ineffective highvelocity environments outofband data validation prevent data quality degrading users eyes detect lowgrade data corruption data loss scenarios become unrecoverable system outofband checks balances needed within application datastores often data validation pipelines implemented collections mapreductions hadoop jobs frequently pipelines added afterthought services already popular successful sometimes pipelines first attempted services reach scalability limits rebuilt ground google built validators response situations shunting developers work data validation pipeline slow engineering velocity short term however devoting engineering resources data validation endows developers courage move faster long run engineers know data corruption bugs less likely sneak production unnoticed similar effects enjoyed units test introduced early project lifecycle data validation pipeline results overall acceleration software development projects cite specific example gmail sports number data validators detected actual data integrity problems production gmail developers derive comfort knowledge bugs introducing inconsistencies production data detected within hours shudder thought running data validators less often daily validators along culture unit regression testing best practices given gmail developers courage introduce code changes gmail production storage implementation frequently week outofband data validation tricky implement correctly strict even simple appropriate changes cause validation fail result engineers abandon data validation altogether data validation strict enough user experienceaffecting data corruption slip undetected find right balance validate invariants cause devastation users example google drive periodically validates file contents align listings drive folders two elements align files would missing dataa disastrous outcome drive infrastructure developers invested data integrity also enhanced validators automatically fix inconsistencies safeguard turned potential emergency allhandsondeckomigoshfilesaredisappearing data loss situation business usual let go home fix root cause monday situation transforming emergencies business usual validators improve engineering morale quality life predictability outofband validators expensive scale significant portion gmail compute resource footprint supports collection daily validators compound expense validators also lower serverside cache hit rates reducing serverside responsiveness experienced users mitigate hit responsiveness gmail provides variety knobs ratelimiting validators periodically refactors validators reduce disk contention one refactoring effort cut contention disk spindles without significantly reducing scope invariants covered majority gmail validators run daily workload largest validator divided shards one shard validated per day reasons scale google compute storage another example challenges scale entails data validation outofband validators could longer finish within day compute storage engineers devise efficient way verify metadata use brute force alone similar application data recovery tiered strategy also useful outofband data validation service scales sacrifice rigor daily validators make sure daily validators continue catch disastrous scenarios within hours continue rigorous validation reduced frequency contain costs latency troubleshooting failed validations take significant effort causes intermittent failed validation could vanish within minutes hours days therefore ability rapidly drill validation audit logs essential mature google services provide oncall engineers comprehensive documentation tools troubleshoot example oncall engineers gmail provided suite playbook entries describing respond validation failure alert bigquerylike investigation tool data validation dashboard effective outofband data validation demands following validation job management monitoring alerts dashboards ratelimiting features troubleshooting tools production playbooks data validation apis make validators easy add refactor majority small engineering teams operating high velocity afford design build maintain systems pressured result often fragile limited wasteful oneoffs fall quickly disrepair therefore structure engineering teams central infrastructure team provides data validation framework multiple product engineering teams central infrastructure team maintains outofband data validation framework product engineering teams maintain custom business logic heart validator keep pace evolving products knowing data recovery work light bulb break flicking switch fails turn light alwaysoften bulb already failed simply notice failure unresponsive flick switch room dark stubbed toe likewise recovery dependencies meaning mostly backup may latent broken state aware attempt recover data discover restore process broken need rely upon address vulnerability fall victim take another backup provision additional resources change slo take actions proactively first know needed detect vulnerabilities continuously test recovery process part normal operations set alerts fire recovery process fails provide heartbeat indication success go wrong recovery process anything everything test let sleep night full endtoend test let proof pudding even recently ran successful recovery parts recovery process still break take away one lesson chapter remember know recover recent state actually recovery tests manual staged event testing becomes unwelcome bit drudgery performed either deeply frequently enough deserve confidence therefore automate tests whenever possible run continuously aspects recovery plan confirm myriad backups valid complete empty sufficient machine resources run setup restore postprocessing tasks comprise recovery recovery process complete reasonable wall time able monitor state recovery process progresses free critical dependencies resources outside control access offsite media storage vault available testing discovered aforementioned failures well failures many components successful data recovery discovered failures regular teststhat came across failures needed recover user data real emergenciesit quite possible google successful products today may stood test time failures inevitable wait discover gun facing real data loss playing fire testing forces failures happen actual catastrophe strikes fix problems harm comes fruition case studies life imitates art case science predicted real life given us unfortunate inevitable opportunities put data recovery systems processes test realworld pressure two notable interesting opportunities discussed gmailfebruary restore gtape first recovery case study examine unique couple ways number failures coincided bring data loss fact largest use last line defense gtape offline backup system sunday february late evening gmail backup system pager triggered displaying phone number join conference call event long fearedindeed reason backup system existencehas come pass gmail lost significant amount user data despite system many safeguards internal checks redundancies data disappeared gmail first largescale use gtape global backup system gmail restore live customer data fortunately first restore similar situations previously simulated many times therefore able deliver estimate long would take restore majority affected user accounts restore accounts within several hours initial estimate recover data estimated completion time ability formulate estimate luck noour success fruit planning adherence best practices hard work cooperation glad see investment elements pay well google able restore lost data timely manner executing plan designed according best practices defense depth emergency preparedness google publicly revealed recovered data previously undisclosed tape backup system slo public reaction mix surprise amusement tape google lots disks fast network replicate data important course google resources principle defense depth dictates providing multiple layers protection guard breakdown compromise single protection mechanism backing online systems gmail provides defense depth two layers failure internal gmail redundancy backup subsystems wide failure zeroday vulnerability device driver filesystem affecting underlying storage medium disk particular failure resulted first scenariowhile gmail internal means recovering lost data loss went beyond internal means could recover one internally celebrated aspects gmail data recovery degree cooperation smooth coordination comprised recovery many teams completely unrelated gmail data recovery pitched help recovery succeeded smoothly without central plan choreograph widely distributed herculean effort plan product regular dress rehearsals dry runs google devotion emergency preparedness leads us view failures inevitable accepting inevitability hope bet avoid disasters anticipate occur thus need plan dealing foreseeable failures amount random undifferentiated breakage well short always knew adherence best practices important good see maxim proven true google musicmarch runaway deletion detection second failure examine entails challenges logistics unique scale datastore recovered store tapes efficiently even feasibly read much data offline media reasonable amount time tuesday march th midafternoon discovering problem google music user reports previously unproblematic tracks skipped team responsible interfacing google music users notifies google music engineers problem investigated possible media streaming issue march th investigating engineer discovers unplayable track metadata missing reference point actual audio data surprised obvious fix locate audio data reinstate reference data however google engineering prides culture fixing issues root engineer digs deeper finds cause data integrity lapse almost heart attack audio reference removed privacyprotecting data deletion pipeline part google music designed delete large numbers audio tracks record time assessing damage google privacy policy protects user personal data applied google music specifically privacy policy means music files relevant metadata removed within reasonable time users delete popularity google music soared amount data grew rapidly original deletion implementation needed redesigned efficient february th updated data deletion pipeline enjoyed maiden run remove relevant metadata nothing seemed amiss time second stage pipeline allowed remove associated audio data could engineer worst nightmare true immediately sounded alarm raising priority support case google urgent classification reporting issue engineering management site reliability engineering small team google music developers sres assembled tackle issue offending pipeline temporarily disabled stem tide external user casualties next manually checking metadata millions billions files organized across multiple datacenters would unthinkable team whipped hasty mapreduce job assess damage waited desperately job complete froze results came march th refactored data deletion pipeline removed approximately audio references removed affecting audio files users since hasty diagnosis pipeline made simplifications true extent damage could worse month since buggy data deletion pipeline first ran maiden run removed hundreds thousands audio tracks removed hope getting data back tracks recovered recovered fast enough google would face music users could noticed glitch resolving issue parallel bug identification recovery efforts first step resolving issue identify actual bug determine bug happened long root cause identified fixed recovery efforts would vain would pressure reenable pipeline respect requests users deleted audio tracks would hurt innocent users would continue lose storebought music worse painstakingly recorded audio files way escape catch fix issue root fix quickly yet time waste mounting recovery effort audio tracks backed tape unlike gmail case study encrypted backup tapes google music trucked offsite storage locations option offered space voluminous backups users audio data restore experience affected users quickly team decided troubleshoot root cause retrieving offsite backup tapes rather timeintensive restore option parallel engineers split two groups experienced sres worked recovery effort developers analyzed data deletion code attempted fix data loss bug root due incomplete knowledge root problem recovery would staged multiple passes first batch nearly half million audio tracks identified team maintained tape backup system notified emergency recovery effort pm pacific time march th recovery team one factor working favor recovery effort occurred weeks company annual disaster recovery testing exercise see kri tape backup team already knew capabilities limitations subsystems subjects dirt tests began dusting new tool tested dirt exercise using new tool combined recovery team began painstaking effort mapping hundreds thousands audio files backups registered tape backup system mapping files backups actual tapes way team determined initial recovery effort would involve recall backup tapes truck afterwards datacenter technicians would clear space tapes tape libraries long complex process registering tapes extracting data tapes would follow involving workarounds mitigations event bad tapes bad drives unexpected system interactions unfortunately approximately lost audio tracks found tape backups meant audio tracks eaten could backed recovery team decided figure recover missing tracks initiated recovery process tracks tape backups meanwhile root cause team pursued abandoned red herring initially thought storage service google music depended provided buggy data misled data deletion pipelines remove wrong audio data upon closer investigation theory proven false root cause team scratched heads continued search elusive bug first wave recovery recovery team identified backup tapes first recovery wave kicked march th requesting petabytes data distributed among thousands tapes offsite storage one matter extracting data tapes quite another custombuilt tape backup software stack designed handle single restore operation large size initial recovery split restore jobs would take human operator typing one restore command minute three days request many restores human operator would doubt make many mistakes requesting restore tape backup system needed sre develop programmatic solution midnight march th music sre finished requesting restores tape backup system began working magic four hours later spat list backup tapes recalled offsite locations another eight hours tapes arrived datacenter series truck deliveries trucks en route datacenter technicians took several tape libraries maintenance removed thousands tapes make way massive data recovery operation technicians began painstakingly loading tapes hand thousands tapes arrived wee hours morning past dirt exercises manual process proved hundreds times faster massive restores robotbased methods provided tape library vendors within three hours libraries back scanning tapes performing thousands restore jobs onto distributed compute storage despite team dirt experience massive petabyte recovery took longer two days estimated morning march th audio files successfully transferred recalled backup tapes distributed filesystem storage nearby compute cluster backup tapes omitted tape recall process vendor addition recovery process held bad tapes anticipation failure due bad tapes redundant encoding used write backup files additional truck deliveries set recall redundancy tapes along tapes omitted first offsite recall morning march th restore operation completed recall additional redundancy tapes remaining files progress although data safely distributed filesystems additional data recovery steps necessary order make accessible users google music team began exercising final steps data recovery process parallel small sample recovered audio files make sure process still worked expected moment google music production pagers sounded due unrelated critical useraffecting production failurea failure fully engaged google music team two days data recovery effort resumed march th audio tracks made accessible users short days petabytes audio data reinstated users help offsite tape backups days comprised actual data recovery effort second wave recovery first wave recovery process behind team shifted focus missing audio files deleted bug backed majority files storebought promotional tracks original store copies unaffected bug tracks quickly reinstated affected users could enjoy music however small portion audio files uploaded users google music team prompted servers request google music clients affected users reupload files dating march th onward process lasted week thus concluded complete recovery effort incident addressing root cause eventually google music team identified flaw refactored data deletion pipeline understand flaw first need context offline data processing systems evolve large scale large complex service comprising several subsystems storage services even task simple removing deleted data needs performed stages involving different datastores data processing finish quickly processing parallelized run across tens thousands machines exert large load various subsystems distribution slow service users cause service crash heavy load avoid undesirable scenarios cloud computing engineers often make shortlived copy data secondary storage data processing performed unless relative age secondary copies data carefully coordinated practice introduces race conditions instance two stages pipeline may designed run strict succession three hours apart second stage make simplifying assumption correctness inputs without simplifying assumption logic second stage may hard parallelize stages may take longer complete volume data grows eventually original design assumptions may longer hold certain pieces data needed second stage first race condition may occur tiny fraction data volume data increases larger larger fraction data risk triggering race condition scenario probabilisticthe pipeline works correctly vast majority data time race conditions occur data deletion pipeline wrong data deleted nondeterministically google music data deletion pipeline designed coordination large margins error place upstream stages pipeline began require increased time service grew performance optimizations put place google music could continue meet privacy requirements result probability inadvertent datadeleting race condition pipeline began increase pipeline refactored probability significantly increased point race conditions occurred regularly wake recovery effort google music redesigned data deletion pipeline eliminate type race condition addition enhanced production monitoring alerting systems detect similar largescale runaway deletion bugs aim detecting fixing issues users notice problems general principles sre applied data integrity general principles sre applied specifics data integrity cloud computing described section beginner mind largescale complex services inherent bugs fully grokked never think understand enough complex system say fail certain way trust verify apply defense depth note beginner mind suggest putting new hire charge data deletion pipeline trust verify api upon depend work perfectly time given regardless engineering quality rigor testing api defects check correctness critical elements data using outofband data validators even api semantics suggest need perfect algorithms may perfect implementations hope strategy system components continually exercised fail need prove data recovery works regular exercise data recovery work humans lack discipline continually exercise system components automation friend however staff automation efforts engineers competing priorities may end temporary stopgaps defense depth even bulletproof system susceptible bugs operator error order data integrity issues fixable services must detect issues quickly every strategy eventually fails changing environments best data integrity strategies multitieredmultiple strategies fall back one another address broad swath scenarios together reasonable cost revisit reexamine fact data safe yesterday going help tomorrow even today systems infrastructure change got prove assumptions processes remain relevant face progress consider following shakespeare service received quite bit positive press user base steadily increasing real attention paid data integrity service built course want serve bad bits index bigtable lost easily recreate original shakespeare texts mapreduce would take little time never made backups index new feature allows users make text annotations suddenly dataset longer easily recreated user data increasingly valuable users therefore need revisit replication optionswe replicating latency bandwidth data integrity well therefore need create test backup restore procedure procedure also periodically tested dirt exercise ensure restore users annotations backups within time set slo conclusion data availability must foremost concern datacentric system rather focusing means end google sre finds useful borrow page testdriven development proving systems maintain data availability predicted maximum time means mechanisms use achieve end goal necessary evils keeping eyes goal avoid falling trap operation success system died recognizing anything go wrong everything go wrong significant step toward preparation real emergency matrix possible combinations disasters plans address disasters permits sleep soundly least one night keeping recovery plans current exercised permits sleep nights year get better recovering breakage reasonable time n find ways whittle time rapid finergrained loss detection goal approaching n switch planning recovery planning prevention aim achieving holy grail data time achieve goal sleep beach welldeserved vacation atomicity consistency isolation durability see https enwikipediaorgwikiacid sql databases mysql postgresql strive achieve properties basically available soft state eventual consistency https enwikipediaorgwikieventualconsistency base systems bigtable megastore often also described nosql see like for reading acid base apis see gol bai binary large object see https enwikipediaorgwikibinarylargeobject see https enwikipediaorgwikizeroday computing clay tablets oldest known examples writing broader discussion preserving data long haul see con upon reading advice one might ask since offer api top datastore implement soft deletion stop soft deletion could offer many features protect accidental data deletion users take specific example google experience consider blobstore rather allow customers delete blob data metadata directly blob apis implement many safety features including default backup policies offline replicas endtoend checksums default tombstone lifetimes soft deletion turns multiple occasions soft deletion saved blobstore clients data loss could much much worse certainly many deletion protection features worth calling companies required data deletion deadlines soft deletion pertinent protection bugs accidental deletion case blobstore clients snapshot refers readonly static view storage instance snapshots sql databases snapshots often implemented using copyonwrite semantics storage efficiency expensive two reasons first contend storage capacity live datastores second faster data mutates less efficiency gained copyingonwrite for information gfsstyle replication see ghe information reedsolomon erasure codes see https enwikipediaorgwikireedsolomonerrorcorrection see http enwikipediaorgwikicatch logic in practice coming programmatic solution hurdle majority sres experienced software engineers case expectation experience makes sres notoriously hard find hire case study data points begin appreciate sre hires practicing software engineers see jon in experience cloud computing engineers often reluctant set production alerts data deletion rates due natural variation peruser data deletion rates time however since intent alert detect global rather local deletion rate anomalies would useful alert global data deletion rate aggregated across users crosses extreme threshold x observed th percentile opposed less useful peruser deletion rate alerts chapter reliable product launches scale written rhandeev singh sebastian kirsch vivek rauedited betsy beyer internet companies like google able launch new products features far rapid iterations traditional companies site reliability role process enable rapid pace change without compromising stability site created dedicated team launch coordination engineers consult engineering teams technical aspects successful launch team also curated launch checklist common questions ask launch recipes solve common issues checklist proved useful tool ensuring reproducibly reliable launches consider ordinary google servicefor example keyhole serves satellite imagery google maps google earth normal day keyhole serves several thousand satellite images per second christmas eve received times normal peak trafficupward one million requests per second caused massive surge traffic santa coming years ago google collaborated norad north american aerospace defense command host christmasthemed website tracked santa progress around world allowing users watch deliver presents real time part experience virtual flyover used satellite imagery track santa progress simulated world project like norad tracks santa may seem whimsical characteristics define difficult risky launch hard deadline google ask santa come week later site ready lot publicity audience millions steep traffic rampup everybody going watching site christmas eve never underestimate power millions kids anxious presentsthis project real possibility bringing google servers knees google site reliability engineering team worked hard prepare infrastructure launch making sure santa could deliver presents time watchful eyes expectant audience last thing wanted make children cry watch santa deliver presents fact dubbed various kill switches built experience protect services makechildrencry switches anticipating many different ways launch could go wrong coordinating different engineering groups involved launch fell special team within site reliability engineering launch coordination engineers lce launching new product feature moment truth every company point months years effort presented world traditional companies launch new products fairly low rate launch cycle internet companies markedly different launches rapid iterations far easier new features rolled server side rather requiring software rollout individual customer workstations google defines launch new code introduces externally visible change application depending launch characteristicsthe combination attributes timing number steps involved complexitythe launch process vary greatly according definition google sometimes performs launches per week rapid rate change provides rationale opportunity creating streamlined launch process company launches product every three years need detailed launch process time new launch occurs components previously developed launch process outdated traditional companies opportunity design detailed launch process accumulate enough experience performing launches generate robust mature process launch coordination engineering good software engineers great deal expertise coding design understand technology products well however engineers may unfamiliar challenges pitfalls launching product millions users simultaneously minimizing outages maximizing performance google approached challenges inherent launches creating dedicated consulting team within sre tasked technical side launching new product feature staffed software engineers systems engineerssome experience sre teamsthis team specializes guiding developers toward building reliable fast products meet google standards robustness scalability reliability consulting team launch coordination engineering lce facilitates smooth launch process ways auditing products services compliance google reliability standards best practices providing specific actions improve reliability acting liaison multiple teams involved launch driving technical aspects launch making sure tasks maintain momentum acting gatekeepers signing launches determined safe educating developers best practices integrate google services equipping internal documentation training resources speed learning members lce team audit services various times service lifecycle audits conducted new product service launches product development team performs launch without sre support lce provides appropriate domain knowledge ensure smooth launch even products already strong sre support often engage lce team critical launches challenges teams face launching new product substantially different daytoday operation reliable service task sre teams already excel lce team draw experience hundreds launches lce team also facilitates service audits new services first engage sre role launch coordination engineer launch coordination engineering team composed launch coordination engineers lces either hired directly role sres handson experience running google services lces held technical requirements sre also expected strong communication leadership skillsan lce brings disparate parties together work toward common goal mediates occasional conflicts guides coaches educates fellow engineers team dedicated coordinating launches offers following advantages breadth experience true crossproduct team members active across almost google product areas extensive crossproduct knowledge relationships many teams across company make lces excellent vehicles knowledge transfer crossfunctional perspective lces holistic view launch enables coordinate among disparate teams sre development product management holistic approach particularly important complicated launches span half dozen teams multiple time zones objectivity nonpartisan advisor lce plays balancing mediating role stakeholders including sre product developers product managers marketing launch coordination engineer sre role lces incentivized prioritize reliability concerns company share google reliability goals shares rapid rate change may choose different incentive structure setting launch process google honed launch process period years time identified number criteria characterize good launch process lightweight easy developers robust catches obvious errors thorough addresses important details consistently reproducibly scalable accommodates large number simple launches fewer complex launches adaptable works well common types launches example adding new ui language product new types launches example initial launch chrome browser google fiber see requirements obvious conflict example hard design process simultaneously lightweight thorough balancing criteria requires continuous work google successfully employed tactics help us achieve criteria simplicity get basics right plan every eventuality high touch approach experienced engineers customize process suit launch fast common paths identify classes launches always follow common pattern launching product new country provide simplified launch process class experience demonstrated engineers likely sidestep processes consider burdensome adding insufficient valueespecially team already crunch mode launch process seen another item blocking launch reason lce must optimize launch experience continuously strike right balance cost benefit launch checklist checklists used reduce failure ensure consistency completeness across variety disciplines common examples include aviation preflight checklists surgical checklists gaw similarly lce employs launch checklist launch qualification checklist launch coordination checklist helps lce assess launch provides launching team action items pointers information examples items checklist might include question need new domain name action item coordinate marketing desired domain name request registration domain link marketing form question storing persistent data action item make sure implement backups instructions implementing backups question could user potentially abuse service action item implement rate limiting quotas use following shared service practice nearinfinite number questions ask system easy checklist grow unmanageable size maintaining manageable burden developers requires careful curation checklist effort curb growth one point adding new questions google launch checklist required approval vice president lce uses following guidelines every question importance must substantiated ideally previous launch disaster every instruction must concrete practical reasonable developers accomplish checklist needs continuous attention order remain relevant uptodate recommendations change time internal systems replaced different systems areas concern previous launches become obsolete due new policies processes lces curate checklist continuously make small updates team members notice items need modified twice year team member reviews entire checklist identify obsolete items works service owners subject matter experts modernize sections checklist driving convergence simplification large organization engineers may aware available infrastructure common tasks rate limiting lacking proper guidance likely reimplement existing solutions converging set common infrastructure libraries avoids scenario provides obvious benefits company cuts duplicate effort makes knowledge easily transferable services results higher level engineering service quality due concentrated attention given infrastructure almost groups google participate common launch process makes launch checklist vehicle driving convergence common infrastructure rather implementing custom solution lce recommend existing infrastructure building blocksinfrastructure already hardened years experience help mitigate capacity performance scalability risks examples include common infrastructure rate limiting user quotas pushing new data servers releasing new versions binary type standardization helped radically simplify launch checklist example long sections checklist dealing requirements rate limiting could replaced single line stated implement rate limiting using system x due breadth experience across google products lces also unique position identify opportunities simplification working launch witness stumbling blocks firsthand parts launch causing struggle steps take disproportionate amount time problems get solved independently similar ways common infrastructure lacking duplication exists common infrastructure lces various ways streamline launch experience act advocates launching teams example lces might work owners particularly arduous approval process simplify criteria implement automatic approvals common cases lces also escalate pain points owners common infrastructure create dialogue customers leveraging experience gained course multiple previous launches lces devote attention individual concerns suggestions launching unexpected project enters new product space vertical lce may need create appropriate checklist scratch often involves synthesizing experience relevant domain experts drafting new checklist helpful structure checklist around broad themes reliability failure modes processes example launching android google rarely dealt mass consumer devices clientside logic directly control less easily fix bug gmail within hours days pushing new versions javascript browsers fixes option mobile devices therefore lces working mobile launches engaged mobile domain experts determine sections existing checklists apply new checklist questions needed conversations important keep intent question mind order avoid mindlessly applying concrete question action item relevant design unique product launched lce facing unusual launch must return abstract first principles execute safe launch respecialize make checklist concrete useful developers developing launch checklist checklist instrumental launching new services products reproducible reliability launch checklist grew time periodically curated members launch coordination engineering team details launch checklist different every company specifics must tailored company internal services infrastructure following sections extract number themes google lce checklists provide examples themes might fleshed architecture dependencies architecture review allows determine service using shared infrastructure correctly identifies owners shared infrastructure additional stakeholders launch google large number internal services used building blocks new products later stages capacity planning see hixa list dependencies identified section checklist used make sure every dependency correctly provisioned example checklist questions request flow user frontend backend different types requests different latency requirements example action items isolate userfacing requests non userfacing requests validate request volume assumptions one page view turn many requests integration many companies services run internal ecosystem entails guidelines set machines configure new services set monitoring integrate load balancing set dns addresses forth internal ecosystems usually grow time often idiosyncrasies pitfalls navigate thus section checklist vary widely company company example action items set new dns name service set load balancers talk service set monitoring new service capacity planning new features may exhibit temporary increase usage launch subsides within days type workload traffic mix launch spike could substantially different steady state throwing load test results public interest notoriously hard predict google products accommodate launch spikes times higher initially estimated launching initially one region country time helps develop confidence handle larger launches capacity interacts redundancy availability instance need three replicated deployments serve traffic peak need maintain four five deployments one two redundant order shield users maintenance unexpected malfunctions datacenter network resources often long lead time need requested far enough advance company obtain example checklist questions launch tied press release advertisement blog post form promotion much traffic rate growth expect launch obtained compute resources needed support traffic failure modes systematic look possible failure modes new service ensures high reliability start portion checklist examine component dependency identify impact failure service deal individual machine failures datacenter outages network failures deal bad input data prepared possibility denialofservice dos attack service continue serving degraded mode one dependencies fails deal unavailability dependency upon startup service runtime example checklist questions single points failure design mitigate unavailability dependencies example action items implement request deadlines avoid running resources longrunning requests implement load shedding reject new requests early overload situations client behavior traditional website rarely need take abusive behavior legitimate users account every request triggered user action click link request rates limited quickly users click double load number users would double axiom longer holds consider clients initiate actions without user inputfor example cell phone app periodically syncs data cloud website periodically refreshes either scenarios abusive client behavior easily threaten stability service also topic protecting service abusive traffic scrapers denialofservice attackswhich different designing safe behavior firstparty clients example checklist question autosaveautocompleteheartbeat functionality example action items make sure client backs exponentially failure make sure jitter automatic requests processes automation google encourages engineers use standard tools automate common processes however automation never perfect every service processes need executed human creating new release moving service different data center restoring data backups reliability reasons strive minimize single points failure include humans remaining processes documented launch ensure information translated engineer mind onto paper still fresh available emergency processes documented way team member execute given process emergency example checklist question manual processes required keep service running example action items document manual processes document process moving service new datacenter automate process building releasing new version development process google extensive user version control almost development processes deeply integrated version control system many best practices revolve around use version control system effectively example perform development mainline branch releases built separate branches per release setup makes easy fix bugs release without pulling unrelated changes mainline google also uses version control purposes storing configuration files many advantages version controlhistory tracking attributing changes individuals code reviewsapply configuration files well cases also propagate changes version control system live servers automatically engineer needs submit change make go live example action items check code configuration files version control system cut release new release branch external dependencies sometimes launch depends factors beyond company control identifying factors allows mitigate unpredictability entail instance dependency may code library maintained third parties service data provided another company vendor outage bug systematic error security issue unexpected scalability limit actually occurs prior planning enable avert mitigate damage users google history launches used filtering andor rewriting proxies data transcoding pipelines caches mitigate risks example checklist questions thirdparty code data services events service launch depend upon partners depend service need notified launch happens vendor meet hard launch deadline rollout planning large distributed systems events happen instantaneously reasons reliability immediacy usually ideal anyway complicated launch might require enabling individual features number different subsystems configuration changes might take hours complete working configuration test instance guarantee configuration rolled live instance sometimes complicated dance special functionality required make components launch cleanly correct order external requirements teams like marketing pr might add complications example team might need feature available time keynote conference need keep feature invisible keynote contingency measures another part rollout planning manage enable feature time keynote sometimes contingency measures simple preparing backup slide deck says launching feature next days rather launched feature example action items set launch plan identifies actions take launch service identify responsible item identify risk individual launch steps implement contingency measures selected techniques reliable launches described parts book google developed number techniques running reliable systems years techniques particularly well suited launching products safely also provide advantages regular operation service particularly important get right launch phase gradual staged rollouts one adage system administration never change running system change represents risk risk minimized order assure reliability system true small system doubly true highly replicated globally distributed systems like run google launches google pushbutton variety launch new product specific time entire world use time google developed number patterns allow us launch products features gradually thereby minimize risk see collection best practices production services almost updates google services proceed gradually according defined process appropriate verification steps interspersed new server might installed machines one datacenter observed defined period time looks well server installed machines one datacenter observed installed machines globally first stages rollout usually called canaries an allusion canaries carried miners coal mine detect dangerous gases canary servers detect dangerous effects behavior new software real user traffic canary testing concept embedded many google internal tools used make automated changes well systems change configuration files tools manage installation new software typically observe newly started server making sure server crash otherwise misbehave change pass validation period automatically rolled back concept gradual rollouts even applies software run google servers new versions android app rolled gradual manner updated version offered subset installs upgrade percentage upgraded instances gradually increases time reaches type rollout particularly helpful new version results additional traffic backend servers google datacenters way observe effect servers gradually roll new version detect problems early invite system another type gradual rollout frequently rather allowing free signups new service limited number users allowed sign per day ratelimited signups often coupled invite system user send limited number invites friends feature flag frameworks google often augments prelaunch testing strategies mitigate risk outage mechanism roll changes slowly allowing observation total system behavior real workloads pay engineering investment reliability engineering velocity time market mechanisms proven particularly useful cases realistic test environments impractical particularly complex launches effects hard predict furthermore changes equal sometimes simply want check whether small tweak user interface improves experience users small changes involve thousands lines code heavyweight launch process may want test hundreds changes parallel finally sometimes want find whether small sample users like using early prototype new hardtoimplement feature want spend months engineering effort harden new feature serve millions users find feature flop accommodate preceding scenarios several google products devised feature flag frameworks frameworks designed roll new features gradually users whenever product introduced framework framework hardened much possible applications would need lce involvement frameworks usually meet following requirements roll many changes parallel servers users entities datacenters gradually increase larger limited group users usually percent direct traffic different servers depending users sessions objects andor locations automatically handle failure new code paths design without affecting users independently revert change immediately event serious bugs side effects measure extent change improves user experience google feature flag frameworks fall two general classes primarily facilitate user interface improvements support arbitrary serverside business logic changes simplest feature flag framework user interface changes stateless service http payload rewriter frontend application servers limited subset cookies another similar http requestresponse attribute configuration mechanism may specify identifier associated new code paths scope change eg cookie hash mod range whitelists blacklists stateful services tend limit feature flags subset unique loggedin user identifiers actual product entities accessed id documents spreadsheets storage objects rather rewrite http payloads stateful services likely proxy reroute requests different servers depending change conferring ability test improved business logic complex new features dealing abusive client behavior simplest example abusive client behavior misjudgment update rates new client syncs every seconds opposed every seconds causes times load service retry behavior number pitfalls affect userinitiated requests well clientinitiated requests take example service overloaded therefore failing requests clients retry failed requests add load already overloaded service resulting retries even requests instead clients need reduce frequency retries usually adding exponentially increasing delay retries addition carefully considering types errors warrant retry example network error usually warrants retry xx http error indicates error client side usually intentional inadvertent synchronization automated requests thundering herd much like described chapters distributed periodic scheduling cron data processing pipelines another common example abusive client behavior phone app developer might decide am good time download updates user likely asleep inconvenienced download however design results barrage requests download server am every night almost requests time instead every client choose time type request randomly randomness also needs injected periodic processes return previously mentioned retries let take example client sends request encounters failure retries second seconds seconds without randomness brief request spike leads increased error rate could repeat due retries second seconds seconds order even synchronized events delay needs jittered adjusted random amount ability control behavior client server side proven important tool past app device control might mean instructing client check periodically server download configuration file file might enable disable certain features set parameters often client syncs often retries client configuration might even enable completely new userfacing functionality hosting code supports new functionality client application activate feature greatly reduce risk associated launch releasing new version becomes much easier need maintain parallel release tracks version new functionality versus without functionality holds particularly true dealing single piece new functionality set independent features might released different schedules would necessitate maintaining combinatorial explosion different versions sort dormant functionality also makes aborting launches easier adverse effects discovered rollout cases simply switch feature iterate release updated version app without type client configuration would provide new version app without feature update app users phones overload behavior load tests overload situations particularly complex failure mode therefore deserve additional attention runaway success usually welcome cause overload new service launches myriad causes including load balancing failures machine outages synchronized client behavior external attacks naive model assumes cpu usage machine providing particular service scales linearly load example number requests amount data processed available cpu exhausted processing simply becomes slower unfortunately services rarely behave ideal fashion real world many services much slower loaded usually due effect various kinds caches cpu caches jit caches servicespecific data caches load increases usually window cpu usage load service correspond linearly response times stay mostly constant point many services reach point nonlinearity approach overload benign cases response times simply begin increase resulting degraded user experience necessarily causing outage although slow dependency might cause uservisible errors stack due exceeded rpc deadlines drastic cases service locks completely response overload cite specific example overload behavior service logged debugging information response backend errors turned logging debugging information expensive handling backend response normal case therefore service became overloaded timed backend responses inside rpc stack service spent even cpu time logging responses timing requests meantime service ground complete halt services running java virtual machine jvm similar effect grinding halt sometimes called gc garbage collection thrashing scenario virtual machine internal memory management runs increasingly closer cycles trying free memory cpu time consumed memory management unfortunately hard predict first principles service react overload therefore load tests invaluable tool reliability reasons capacity planning load testing required launches development lce google formative years size engineering team doubled every year several years row fragmenting engineering department many small teams working many experimental new products features climate novice engineers run risk repeating mistakes predecessors especially comes launching new features products successfully mitigate repetition mistakes capturing lessons learned past launches small band experienced engineers called launch engineers volunteered act consulting team launch engineers developed checklists new product launches covering topics consult legal department select domain names register new domains without misconfiguring dns common engineering design production deployment pitfalls launch reviews launch engineers consulting sessions came called became common practice days weeks launch many new products within two years product deployment requirements launch checklist grew long complex combined increasing complexity google deployment environment became challenging product engineers stay uptodate make changes safely time sre organization growing quickly inexperienced sres sometimes overly cautious averse change google ran risk resulting negotiations two parties would reduce velocity productfeature launches mitigate scenario engineering perspective sre staffed small fulltime team lces responsible accelerating launches new products features time applying sre expertise ensure google shipped reliable products high availability low latency lces responsible making sure launches executing quickly without services falling launch fail take products lces also responsible keeping stakeholders informed nature likelihood failures whenever corners cut order accelerate time market consulting sessions formalized production reviews evolution lce checklist google environment grew complex launch coordination engineering checklist see launch coordination checklist volume launches years one lce ran launches lce checklist team averaged five engineers time period translates google launch throughput launches years question lce checklist simple much complexity built prompted question implications answer order fully understand degree complexity new lce hire requires six months training volume launches grew keeping pace annual doubling google engineering team lces sought ways streamline reviews lces identified categories lowrisk launches highly unlikely face cause mishaps example feature launch involving new server executables traffic increase would deemed low risk launches faced almost trivial checklist higherrisk launches underwent full gamut checks balances reviews considered lowrisk simultaneously google environment scaling removing constraints many launches instance acquisition youtube forced google build network utilize bandwidth efficiently meant many smaller products would fit within cracks avoiding complex network capacity planning provisioning processes thus accelerating launches google also began building large datacenters capable hosting several dependent services one roof development simplified launch new products needed large amounts capacity multiple preexisting services upon depended problems lce solve although lces tried keep bureaucracy reviews minimum efforts insufficient difficulties launching small new service google become legend services grew larger scale faced set problems launch coordination could solve scalability changes products successful far beyond early estimates usage increases two orders magnitude keeping pace load necessitates many design changes scalability changes combined ongoing feature additions often make product complex fragile difficult operate point original product architecture becomes unmanageable product needs completely rearchitected rearchitecting product migrating users old new architecture requires large investment time resources developers sres alike slowing rate new feature development period growing operational load running service launches operational load amount manual repetitive engineering needed keep system functioning tends grow time unless efforts made control load noisiness automated notifications complexity deployment procedures overhead manual maintenance work tend increase time consume increasing amounts service owner bandwidth leaving team less time feature development sre internally advertised goal keeping operational work maximum see eliminating toil staying maximum requires constant tracking sources operational work well directed effort remove sources infrastructure churn underlying infrastructure systems cluster management storage monitoring load balancing data transfer changing due active development infrastructure teams owners services running infrastructure must invest large amounts work simply keep infrastructure changes infrastructure features upon services rely deprecated replaced new features service owners must continually modify configurations rebuild executables consequently running fast stay place solution scenario enact type churn reduction policy prohibits infrastructure engineers releasing backwardincompatible features also automate migration clients new feature creating automated migration tools accompany new features minimizes work imposed service owners keep infrastructure churn solving problems requires companywide efforts far beyond scope lce combination better platform apis frameworks see evolving sre engagement model continuous build test automation better standardization automation across google production services conclusion companies undergoing rapid growth high rate change products services may benefit equivalent launch coordination engineering role team especially valuable company plans double product developers every one two years must scale services hundreds millions users reliability despite high rate change important users lce team google solution problem achieving safety without impeding change chapter introduced experiences accumulated unique lce role year period exactly circumstances hope approach help inspire others facing similar challenges respective organizations part iv management final selection topics covers working together team working teams sre island distinctive ways work organization aspires serious running effective sre arm needs consider training teaching sres think complicated fastchanging environment wellthoughtout wellexecuted training program promise instilling best practices within new hire first weeks months otherwise would take months years accumulate discuss strategies accelerating sres oncall beyond anyone operations world knows responsibility significant service comes lot interruptions production getting bad state people requesting updates favorite binary long queue consultation requestsmanaging interrupts turbulent conditions necessary skill discuss dealing interrupts turbulent conditions persisted long enough sre team needs start recovering operational overload flight plan embedding sre recover operational overload write communication collaboration sre different roles within sre crossteam crosssite crosscontinent communication running production meetings case studies sre collaborated well finally evolving sre engagement model examines cornerstone operation sre production readiness review prr crucial step onboarding new service discuss conduct prrs move beyond successful also limited model reading google sre building reliable systems requires carefully calibrated mix skills ranging software development arguably lesswellknown systems analysis engineering disciplines write latter disciplines systems engineering side site reliability engineering hixb hiring sres well critical highfunctioning reliability organization explored hiring site reliability engineers jon google hiring practices detailed texts like work rules boc hiring sres set particularities even google overall standards sre candidates difficult find even harder interview effectively chapter accelerating sres oncall beyond strap jetpack newbies keeping senior sres speed written andrew widdowsonedited shylaja nukala hired next sre hired new employees organization starting site reliability engineers train job investing front education technical orientation new sres shape better engineers training accelerate state proficiency faster making skill set robust balanced successful sre teams built trustin order maintain service consistently globally need trust fellow oncallers know system works diagnose atypical system behaviors comfortable reaching help react pressure save day essential sufficient think sre education lens newbie need learn go oncall given requirements regarding trust also need ask questions like existing oncallers assess readiness newbie oncall harness enthusiasm curiosity new hires make sure existing sres benefit activities commit team benefit everyone education everyone like students wide range learning preferences recognizing hire people mix preferences would shortsighted cater one style expense others thus style education works best train new sres certainly one magic formula work sre teams table lists recommended training practices corresponding antipatterns well known sre google practices represent wide range options available making team well educated sre concepts ongoing basis table sre education practices recommended patterns antipatterns deluging students menial work designing concrete sequential learning eg alertticket triage train experiences students follow trial fire encouraging reverse engineering statistical thinking working fundamental principles training strictly operator procedures checklists playbooks celebrating analysis failure treating outages secrets buried suggesting postmortems students order avoid blame read creating contained realistic first chance fix breakages students fix using real something occur student monitoring tooling already oncall roleplaying theoretical disasters creating experts team whose group intermingle team problem techniques knowledge solving approaches compartmentalized enabling students shadow oncall rotation early comparing notes oncaller pushing students primary oncall achieve holistic understanding service pairing students expert sres revise targeted sections oncall treating oncall training plans static untouchable except subject revise targeted sections oncall training plan untouchable except subject matter experts carving nontrivial project work awarding new project work students undertake allowing senior sres leaving junior sres gain partial ownership stack pick scraps rest chapter presents major themes found effective accelerating sres oncall beyond concepts visualized blueprint bootstrapping sres figure figure blueprint bootstrapping sre oncall beyond illustration captures best practices sre teams pick help bootstrap new members keeping senior talent fresh many tools pick choose activities best suit team illustration two axes xaxis represents spectrum different types work ranging abstract applied activities yaxis represents time read top new sres little knowledge systems services responsible postmortems detailing systems failed past good starting point new sres also try reverse engineer systems fundamentals since starting zero understand systems done handson work sres ready shadow oncall start mending incomplete outofdate documentation tips interpreting illustration going oncall milestone new sre career point learning becomes lot nebulous undefined selfdirectedhence dashed lines around activities happen sre goes oncall triangular shape project work ownership indicates project work starts small builds time becoming complex likely continuing well going oncall activities practices abstractpassive appliedactive activities mixes good variety learning modalities suit different learning styles maximum effect training activities practices appropriately paced appropriate undertake straightaway happen right sre officially goes oncall continual ongoing even seasoned sres concrete learning experiences happen entire time leading sre going oncall initial learning experiences case structure chaos discussed elsewhere book sre teams undertake natural mix proactive reactive work strong goal every sre team contain reduce reactive work ample proactivity approach take onboarding newbie exception consider following alltoocommon sadly suboptimal onboarding process john newest member fooserver sre team senior sres team tasked lot grunt work responding tickets dealing alerts performing tedious binary rollouts john first day job assigned new incoming tickets told ask member sre team help obtain background necessary decipher ticket sure lot upfront learning says john manager but eventually get much faster tickets one day click know lot tools use procedures follow systems maintain senior team member comments we throwing deep end pool trial fire method orienting one newbies often born team current environment opsdriven reactive sre teams train newest members making themwell react lucky engineers already good navigating ambiguity crawl hole put chances strategy alienated several capable engineers approach may eventually produce great operations employees results fall short mark trialbyfire approach also presumes many aspects team taught strictly rather reasoning set work one encounters tickets queue adequately provide training said job sre position sre students questions like following working much progress made activities accumulate enough experience go oncall making jump previous company university changing job roles traditional software engineer traditional systems administrator nebulous site reliability engineer role often enough knock students confidence several times introspective personalities especially regarding questions uncertainties incurred nebulous lessthanclear answers lead slower development retention problems instead consider approaches outlined following sections suggestions concrete ticket alert also sequential thus far rewarding learning paths cumulative orderly put amount learning order system new sres see path type training better random tickets interrupts make conscious effort combine right mix theory application abstract concepts recur multiple times newbie journey frontloaded education student also receive handson experience soon practically possible learning stack subsystem requires starting point consider whether makes sense group trainings together similarity purpose normalcase order execution example team responsible realtime userfacing serving stack consider curriculum order like following query enters system networking datacenter fundamentals frontend load balancing proxies etc frontend serving application frontend query logging user experience slo etc midtier services caches backend load balancing infrastructure backends infrastructure compute resources tying together debugging techniques escalation procedures emergency scenarios choose present learning opportunities informal whiteboard chats formal lectures handson discovery exercises sres helping structure design deliver training google search sre team structures learning document called oncall learning checklist simplified section oncall learning checklist might look like following results mixing server mixer frontended frontend server backends called results retrieval server geolocation server personalization database sre experts sally w dave k jen p know moving clusters mixer deployed roll back mixer release backends mixer considered critical path developer contacts jim resultsteam read understand following docs results mixing overview query execution section results mixing overview production section playbook roll new results mixing server performance analysis mixer comprehension questions q release schedule change company holiday occurs normal release build day q fix bad push geolocation dataset note preceding section directly encode procedures diagnostic steps playbooks instead relatively futureproof writeup focusing strictly enumerating expert contacts highlighting useful documentation resources establishing basic knowledge must gather internalize asking probing questions answered basic knowledge absorbed also provides concrete outcomes student knows kinds knowledge skills gained completing section learning checklist good idea interested parties get sense much information trainee retaining feedback mechanism perhaps need formal quiz good practice complete bits homework pose questions service work satisfactory answers checked student mentor sign learning continue next phase questions inner workings service might look similar following backends server considered critical path aspects server could simplified automated think first bottleneck architecture bottleneck saturated steps could take alleviate depending access permissions configured service also consider implementing tiered access model first tier access would allow student readonly access inner workings components later tier would permit mutate production state completing sections oncall learning checklist satisfactorily would earn student progressively deeper access system search sre team calls attained levels powerups route oncall trainees eventually added highest level systems access targeted project work menial work sres problem solvers give hearty problem solve starting even minor sense ownership team service wonders learning reverse ownership also make great inroads trust building among senior colleagues approach junior colleague learn new component processes early opportunities ownership standard across google general engineers given starter project meant provide tour infrastructure sufficient enable make small useful contribution early new sre split time learning project work also give sense purpose productivity would happen spent time learning project work several starter project patterns seem work well include making trivial uservisible feature change serving stack subsequently shepherding feature release way production understanding development toolchain binary release process encourages empathy developers adding monitoring service currently blind spots newbie reason monitoring logic reconciling understanding system actually mis behaves automating pain point quite painful enough automated already providing new sre appreciation value sres place removing toil daytoday operations creating stellar reverse engineers improvisational thinkers propose set guidelines train new sres train training material depend technologies used job important question kind engineers trying create scale complexity sres operate afford merely operationsfocused traditional system administrators addition largescale engineering mindset sres exhibit following characteristics course jobs come across systems never seen need strong reverse engineering skills scale anomalies hard detect need ability think statistically rather procedurally uncloak problems standard operating procedures break need able improvise fully let examine attributes understand equip sres skills behaviors reverse engineers figuring things work engineers curious systems never seen workor likely current versions systems used know quite well work baseline understanding systems work company along willingness dig deep debugging tools rpc boundaries logs binaries unearth flows sres become efficient homing unexpected problems unexpected system architectures teach sres diagnostic debugging surfaces applications practice drawing inferences information surfaces reveal behavior becomes reflexive dealing future outages statistical comparative thinkers stewards scientific method pressure think sre approach incident response largescale systems navigating massive decision tree unfolding front limited time window afforded demands incident response sre take actions hundreds goal mitigating outage either short term long term time often utmost importance sre effectively efficiently prune decision tree ability partially gained experience comes time exposure breadth production systems experience must paired careful construction hypotheses proven disproven even narrow decision space put another way tracking system breakages often akin playing game things like things might entail kernel version cpu architecture binary version stack regional traffic mix hundred factors architecturally team responsibility ensure factors controlled individually analyzed compared however also train newest sres become good analysts comparators earliest moments job improv artists unexpected happens try fix breakage work developer behind failing system nowhere found improvise learning multiple tools solve parts problem allows practice defense depth problemsolving behaviors procedural face outage thus forgetting analytical skills difference getting stuck finding root cause case boggeddown troubleshooting compounded sre brings many untested assumptions cause outage decision making demonstrating many analytical traps sres fall require zooming taking different approach resolution valuable lesson sres learn early given three aspirational attributes highperforming sres courses experiences provide new sres order send along path right direction need come course content embodies attributes addition attributes specific sre culture let consider one class believe hits aforementioned points tying together reverse engineering production service came time learn part google maps stack new sre asked rather passively someone explain service could herselflearning everything via reverse engineering class techniques rest us correct herfill blanks whatever missed got wrong result well probably correct useful would given talk oncall years paul cowan google site reliability engineer one popular class offer google called reverse engineering production service without help owners problem scenario presented appears simple first entire google news teamsre software engineers product management forthhas gone company trip cruise bermuda triangle heard team days students newly appointed google news sre team need figure serving stack works endtoend order commandeer keep running given scenario students led interactive purpose driven exercises trace inbound path web browser query google infrastructure stage process emphasize important learn multiple ways discover connectivity production servers connections missed middle class challenge students find another endpoint incoming traffic demonstrating initial assumption narrowly scoped challenge students find ways stack exploit highly instrumented nature production binaries selfreport rpc connectivity well available whitebox blackbox monitoring determine path users queries take along way build system diagram also discuss components shared infrastructure students likely see future end class students charged task student returns home team asks senior sre help select stack slice stack oncall using skills learned classes student diagrams stack presents findings senior sre undoubtedly student miss subtle details make good discussion also likely senior sre learn something exercise well exposing drifts prior understanding everchanging system rapid change production systems important team welcome chance refamiliarize system including learning newest rather oldest members team five practices aspiring oncallers oncall single important purpose sre production engineering responsibilities usually involve kind urgent notification coverage someone capable responsibly taking oncall someone understands system work reasonable depth breadth use able take oncall useful proxy knows enough figure rest hunger failure reading sharing postmortems remember past condemned repeat george santayana philosopher essayist postmortems see postmortem culture learning failure important part continuous improvement blamefree way getting many root causes significant visible outage writing postmortem keep mind appreciative audience might engineer yet hired without radical editing subtle changes made best postmortems make teachable postmortems even best postmortems helpful languish bottom virtual filing cabinet follows team collect curate valuable postmortems serve educational resources future newbies postmortems rote teachable postmortems provide insights structural novel failures largescale systems good gold new students ownership postmortems limited authorship point pride many teams survived documented largest outages collect best postmortems make prominently available newbies addition interested parties related andor integrating teamsto read ask related teams publish best postmortems access sre teams google run postmortem reading clubs fascinating insightful postmortems circulated preread discussed original author postmortem guest honor meeting teams organize tales fail gatherings postmortem author semiformally present recounting outage effectively driving discussion regular readings presentations outages including trigger conditions mitigation steps wonders building new sre mental map understanding production oncall response postmortems also excellent fuel future abstract disaster scenarios disaster role playing week meeting victim chosen spot front group scenariooften real one taken annals google historyis thrown victim think game show contestant tells game show host she would query understand solve problem host tells victim happens action observation s like sre zork maze twisty monitoring consoles alike must save innocent users slipping chasm excessive query latency save datacenters nearcertain meltdown spare us embarrassment erroneous google doodle display robert kennedy former site reliability engineer google search healthcaregov group sres wildly different experience levels bring together enable learn impress sre culture problemsolving nature team upon newbie also keeping grizzled veterans apprised new changes features stack google sre teams address challenges timehonored tradition regular disaster role playing among names exercise commonly referred wheel misfortune walk plank sense humorous danger titles lend exercise makes less intimidating freshly hired sres best exercises become weekly ritual every member group learns something formula straightforward bears resemblance tabletop rpg role playing game game master gm picks two team members primary secondary oncall two sres join gm front room incoming page announced oncall team responds would mitigate investigate outage gm carefully prepared scenario unfold scenario might based upon previous outage newer team members around older team members forgotten perhaps scenario foray hypothetical breakage new soontobelaunched feature stack rendering members room equally unprepared grapple situation better still coworker might find new novel breakage production today scenario expands new threat next minutes primary secondary oncallers attempt rootcause issue gm happily provides additional context problem unfolds perhaps informing oncallers audience graphs monitoring dashboard might look like outage incident requires escalation outside home team gm pretends member team purposes scenario virtual scenario perfect times gm may steer participants back track redirecting oncallers away red herrings introducing urgency clarity adding stimuli asking urgent pointed questions disaster rpg successful everyone learned something perhaps new tool trick different perspective solve problem especially gratifying new team members validation could solved week problem picked luck exercise inspire teammates eagerly look forward next week adventure ask become game master upcoming week break real things fix real things newbie learn much sre reading documentation postmortems taking trainings disaster role playing help get newbie mind game however experience derived handson experience breaking andor fixing real production systems even better plenty time handson experience newbie gone oncall learning happen new sre reaches point therefore provide handson experiences much earlier order develop student reflexive responses using company tooling monitoring approach developing outage realism paramount interactions ideally team stack multihomed provisioned way least one instance divert live traffic temporarily loan learning exercise alternatively might smaller still fully featured staging qa instance stack borrowed short time possible subject stack synthetic load approximates real userclient traffic addition resource consumption possible opportunities learning real production system synthetic load abundant senior sres experienced sorts troubles misconfigurations memory leaks performance regressions crashing queries storage bottlenecks forth realistic relatively riskfree environment proctors manipulate job set ways alter behavior stack forcing new sres find differences determine contributing factors ultimately repair systems restore appropriate behavior alternative overhead asking senior sre carefully plan specific type breakage new sre must repair also work opposite direction exercise may also increase participation entire team work known good configuration slowly impair stack selected bottlenecks observing upstream downstream efforts monitoring exercise valued google search sre team whose version exercise called let burn search cluster ground exercise proceeds follows group discuss observable performance characteristics might change cripple stack inflicting planned damage poll participants guesses reasoning predictions system react validate assumptions justify reasoning behind behaviors see exercise perform quarterly basis shakes new bugs eagerly fix systems always degrade gracefully would expect documentation apprenticeship many sre teams maintain oncall learning checklist organized reading comprehension list technologies concepts relevant system maintain list must internalized student eligible serve shadow oncaller take moment revisit example oncall learning checklist table learning checklist serves different purposes different people student doc helps establish boundaries system team supports studying list student gains sense systems important understand information therein move topics need learn rather dwelling learning esoteric details learned time mentors managers student progress learning checklist observed checklist answers questions sections working today sections confusing team members doc becomes social contract upon mastery student joins ranks oncall learning checklist sets standard team members aspire uphold rapidly changing environment documentation fall date quickly outdated documentation less problem senior sres already speed keep state world changes heads newbie sres much need uptodate documentation may feel empowered knowledgeable enough make changes designed right amount structure oncall documentation become adaptable body work harnesses newbie enthusiasm senior knowledge keep everyone fresh search sre anticipate arrival new team member reviewing oncall learning checklist sorting sections uptodate new team member arrives point overall learning checklist also task overhauling one two outdated sections see table label senior sre developer contacts technology encourage student make early connection subject matter experts might learn inner workings selected technology directly later become familiar scope tone learning checklist expected contribute revised learning checklist section must peerreviewed one senior sres listed experts shadow oncall early often ultimately amount hypothetical disaster exercises training mechanisms fully prepare sre going oncall end day tackling real outages always beneficial learning standpoint engaging hypotheticals yet unfair make newbies wait first real page chance learn retain knowledge student made way system fundamentals completing example oncall learning checklist consider configuring alerting system copy incoming pages newbie first business hours rely curiosity lead way shadow oncall shifts great way mentor gain visibility student progress student gain visibility responsibilities oncall arranging newbie shadow multiple members team team become increasingly comfortable thought person entering oncall rotation instilling confidence manner effective method building trust allowing senior members detach oncall thus helping avoid team burnout page comes new sre appointed oncaller condition removes time pressure student frontrow seat outage unfolds rather issue resolved may student primary oncaller share terminal session sit near readily compare notes time mutual convenience outage complete oncaller review reasoning processes followed student benefit exercise increase shadow oncaller retention actually occurred tip outage occur writing postmortem beneficial oncaller include newbie coauthor dump writeup solely student could mislearned postmortems somehow grunt work passed junior would mistake create impression teams also include final step experienced oncaller reverse shadow student newbie become primary oncall incoming escalations experienced oncaller lurk shadows independently diagnosing situation without modifying state experienced sre available provide active support help validation hints necessary oncall beyond rites passage practicing continuing education comprehension increases student reach point career capable reasoning stack comfortably improvise way rest point go oncall service teams create final exam sorts tests students one last time bestowing oncall powers responsibilities new sres submit completion oncall learning checklist evidence ready regardless gate milestone going oncall rite passage celebrated team learning stop student joins ranks oncall course remain vigilant sres team always need active aware changes come attention elsewhere portions stack may rearchitected extended leaving team operational knowledge historic best set regular learning series whole team overviews new upcoming changes stack given presentations sres shepherding changes copresent developers needed record presentations build training library future students practice gain much timely involvement sres within team developers work closely team keeping everyone minds fresh future venues educational engagement consider sres give talks developer counterparts better development peers understand work challenges team faces easier reach fully informed decisions later projects closing thoughts upfront investment sre training absolutely worthwhile students eager grasp production environment teams grateful welcome students ranks oncall use applicable practices outlined chapter create wellrounded sres faster sharpening team skills perpetuity apply practices charge clear sre scale humans faster scale machines good luck teams creating culture learning teaching and work examples proactive sre work include software automation design consulting launch coordination examples reactive sre work include debugging troubleshooting handling oncall escalations a nod video games yesteryear this follow rpc approach also works well batchpipeline systems start operation kicks system batch systems operation could data arriving needs processed transaction needs validated many events see life trenches healthcaregov for example getting paged another team brings information say for example losing money quickly could stop bleeding short term chapter dealing interrupts written dave connoredited diane bates operational load applied complex systems work must done maintain system functional state example car someone pay always end servicing putting gas regular maintenance keep performing function complex system imperfect creators managing operational load created systems remember creators also imperfect machines operational load applied managing complex systems takes many forms obvious others terminology may change operational load falls three general categories pages tickets ongoing operational activities pages concern production alerts fallout triggered response production emergencies sometimes monotonous recurring requiring little thought also engaging involve tactical indepth thought pages always expected response time slo sometimes measured minutes tickets concern customer requests require take action like pages tickets either simple boring require real thought simple ticket might request code review config team owns complex ticket might entail special unusual request help design capacity plan tickets may also slo response time likely measured hours days weeks ongoing operational responsibilities also known kicking road toil see eliminating toil include activities like teamowned code flag rollouts responses ad hoc timesensitive questions customers may defined slo tasks interrupt types operational load easily anticipated planned much load unplanned interrupt someone nonspecific time requiring person determine issue wait managing operational load google several methods managing type operational load team level pages commonly managed dedicated primary oncall engineer single person responds pages manages resulting incidents outages primary oncall engineer might also manage user support communications escalation product developers order minimize interruption page causes team avoid bystander effect google oncall shifts manned single engineer oncall engineer might escalate pages another team member problem well understood typically secondary oncall engineer acts backup primary secondary engineer duties vary rotations secondary duty contact primary pages fall case secondary might another team secondary engineer may may consider interrupts depending duties tickets managed different ways depending sre team primary oncall engineer might work tickets oncall secondary engineer might work tickets oncall team dedicated ticket person oncall tickets might randomly autodistributed among team members team members might expected service tickets ad hoc ongoing operational responsibilities also managed varying ways sometimes oncall engineer work pushes flag flips etc alternately responsibility assigned team members ad hoc oncall engineer might pick lasting responsibility ie multiweek rollout ticket lasts beyond shift week factors determining interrupts handled take step back mechanics operational load managed number metrics factor interrupts handled sre teams google found following metrics useful deciding manage interrupts interrupt slo expected response time number interrupts usually backlogged severity interrupts frequency interrupts number people available handle certain kind interrupt eg teams require certain amount ticket work going oncall might notice metrics suited meeting lowest possible response time without factoring human costs trying take stock human productivity cost difficult imperfect machines humans imperfect machines get bored processors sometimes uis well understood efficient recognizing human element working intended trying work around ameliorate humans work could fill much larger space provided moment basic ideas might useful determining interrupts work cognitive flow state concept flow state widely accepted empirically acknowledged pretty much everyone works software engineering sysadmin sre disciplines require focused periods concentration zone increase productivity also increase artistic scientific creativity achieving state encourages people actually master improve task project working interrupted kick right state interrupt disruptive enough want maximize amount time spent state cognitive flow also apply less creative pursuits skill level required lower essential elements flow still fulfilled clear goals immediate feedback sense control associated time distortion examples include housework driving get zone working lowskill lowdifficulty problems playing repetitive video game easily get highskill highdifficulty problems engineer might face methods arriving cognitive flow state differ outcome essentially cognitive flow state creative engaged zone someone works problem aware comfortable parameters problem feels like fix solve person works intently problem losing track time ignoring interrupts much possible maximizing amount time person spend state desirablethey going produce creative results good work volume happier job unfortunately many people sretype roles spend much time either trying failing get mode getting frustrated never even attempting reach mode instead languishing interrupted state cognitive flow state angry birds people enjoy performing tasks know fact executing tasks one clearest paths cognitive flow sres oncall reach state cognitive flow fulfilling chase causes problems work others improve overall health system tangible way conversely stressedout oncall engineers stress caused either pager volume treating oncall interrupt trying code work projects simultaneously oncall fulltime interrupts engineers exist state constant interruption interruptability working environment extremely stressful hand person concentrating fulltime interrupts interrupts stop interrupts visceral level making incremental improvements system whacking tickets fixing problems outages becomes clear set goals boundaries clear feedback close x bugs stop getting paged left distractions interrupts projects distraction even though interrupts may satisfying use time short term mixed projectoncall environment people ultimately happier balance two types work ideal balance varies engineer engineer important aware engineers may actually know balance best motivates might think know may disagree one thing well might wondering practical implications read thus far following suggestions based worked various sre teams managed google mainly benefit team managers influencers document agnostic personal habitspeople free manage time see fit concentration directing structure team manages interrupts people set failure team function structure distractibility ways engineer may distracted therefore prevented achieving state cognitive flow numerous example consider random sre named fred fred comes work monday morning fred oncall interrupts today fred would clearly like work projects grabs coffee sticks disturb headphones sits desk zone time right except time following things might happen fred team uses automated ticket system randomly assign tickets team ticket gets assigned due today fred colleague oncall receives page component fred expert interrupts ask user fred service raises priority ticket assigned since last week oncall flag rollout rolling weeks assigned fred goes wrong forcing fred drop everything examine rollout roll back change forth user fred service contacts fred ask question fred helpful chap end result even though fred entire calendar day free work projects remains extremely distractible distractions manage closing email turning im taking similar measures distractions caused policy assumptions around interrupts ongoing responsibilities claim level distraction inevitable design assumption correct people hang onto bugs primary contact people also build responsibilities obligations however ways team manage interrupt response people average come work morning feel undistractible polarizing time order limit distractibility try minimize context switches interrupts inevitable however viewing engineer interruptible unit work whose context switches free suboptimal want people happy productive assign cost context switches minute interruption working project entails two context switches realistically interruption results loss couple hours truly productive work avoid constant occurrences productivity loss aim polarized time work styles work period lasting long possible ideally time period week day even halfday may practical strategy also fits complementary concept make time gra polarizing time means person comes work day know project work interrupts polarizing time way means get concentrate longer periods time task hand get stressed roped tasks drag away work supposed seriously tell seriously tell general model presented chapter work specific suggestions components implement piecemeal general suggestions given class interrupt volume interrupts high one person add another person concept obviously applies tickets potentially apply pages toothe oncall start bumping things secondary downgrading pages tickets oncall primary oncall engineer focus solely oncall work pager quiet service tickets interruptbased work abandoned fairly quickly part oncall duties engineer oncall week week written far project work concerned project important let slip week person oncall escalate order assign someone else oncall shift person never expected oncall also make progress projects anything else high context switching cost secondary duties depend onerous duties function secondary back primary case fallthrough maybe safely assume secondary also accomplish project work someone secondary assigned handling tickets consider merging roles secondary expected actually help primary case high pager volume interrupt work aside never run cleanup work ticket count might zero always documentation needs updating configs need cleanup etc future oncall engineers thank means less likely interrupt precious make time tickets currently assign tickets randomly victims team stop extremely disrespectful team time works completely counter principle interruptible much possible tickets fulltime role amount time manageable person happen unenviable position tickets closed primary secondary oncall engineers combined structure ticket rotation two people handling tickets given time spread load across entire team people machines causing context switches impact valuable flow time ongoing responsibilities much possible define roles let anyone team take mantle welldefined procedure performing verifying pushes flag flips reason person shepherd change entire lifetime even stop oncall interrupts define push manager role juggle pushes duration time oncall interrupts formalize handover processit small price pay uninterrupted make time people oncall interrupts sometimes person interrupts team receives interrupt person uniquely qualified handle ideally scenario never happen sometimes work make occurrences rare sometimes people work tickets assigned handle tickets easy way look busy behavior helpful means person less effective skew numbers terms manageable ticket load one person assigned tickets two three people also take stab ticket queue might still unmanageable ticket queue even though realize reducing interrupts team interrupt load may unmanageable requires many team members simultaneously staff interrupts given time number techniques use reduce ticket load overall actually analyze tickets lots ticket rotations oncall rotations function like gauntlet especially true rotations larger teams interrupts every couple months easy run gauntlet heave sigh relief return regular duties successor root causes tickets never investigated rather achieving forward movement team bogged succession people getting annoyed issues handoff tickets well oncall work handoff process maintains shared state ticket handlers responsibility switches even basic introspection root causes interrupts provide good solutions reducing overall rate lots teams conduct oncall handoffs page reviews teams tickets team conduct regular scrub tickets pages examine classes interrupts see identify root cause think root cause fixable reasonable amount time silence interrupts root cause expected fixed provides relief person handling interrupts creates handy deadline enforcement person fixing root cause respect well customers maxim applies user interrupts automated interrupts although principles stand scenarios tickets particularly annoying onerous resolve effectively use policy mitigate burden remember team sets level service provided service ok push back effort onto customers team responsible handling tickets interrupts customers often use policy make work load manageable policy fix temporary permanent depending makes sense fix strike good balance respect customer respect policy powerful tool code example support particularly flaky tool many developer resources small number needy customers use consider options think value time spend interrupts system spending time wisely point get attention need fix root cause problems causing interrupts perhaps component supporting important consider giving pager back deprecating replacing another strategy vein might make sense particular steps interrupt timeconsuming tricky require privileges accomplish consider using policy push request back requestor example people need donate compute resources prepare code config change similar step instruct customer execute step send review remember customer wants certain task accomplished prepared spend effort getting want caveat preceding solutions need find balance respect customer guiding principle constructing strategy dealing customer requests request meaningful rational provide information legwork need order fulfill request return response helpful timely see wikipedia flow http enwikipediaorgwikiflow psychology see http enwikipediaorgwikirunningthegauntlet psychology chapter embedding sre recover operational overload written randall bosetti edited diane bates s standard policy google s sre teams evenly split time projects reactive ops work practice balance upset months time increase daily ticket volume burdensome amount ops work especially dangerous sre team might burn unable make progress project work team must allocate disproportionate amount time resolving tickets cost spending time improving service scalability reliability suffer one way relieve burden temporarily transfer sre overloaded team embedded team sre focuses improving team s practices instead simply helping team empty ticket queue sre observes team s daily routine makes recommendations improve practices consultation gives team fresh perspective routines team members ca nt provide using approach nt necessary transfer one engineer two sres nt necessarily produce better results may actually cause problems team reacts defensively starting first sre team approach outlined chapter help avoid turning operation team solely focused ticket rotation decide embed one reports team take time review sre practices philosophy ben treynor sloss s introduction material monitoring monitoring distributed systems following sections provide guidance sre embedded team phase learn service get context job embedded team articulate processes habits contribute detract service s scalability remind team tickets require sres goal sre model introduce humans complexity added system instead try draw attention healthy work habits reduce time spent tickets important pointing missed opportunities automation simplification service ops mode versus nonlinear scaling term ops mode refers certain method keeping service running various work items increase size service example service needs way increase number configured virtual machines vms grows team ops mode responds greater number administrators managing vms sre instead focuses writing software eliminating scalability concerns number people required run service nt increase function load service teams sliding ops mode might convinced scale nt matter service tiny shadow oncall session determine whether assessment true element scale affects strategy primary service important business actually tiny entailing resources low complexity put focus ways team s current approach prevents improving service s reliability remember job make service work shield development team alerts hand service getting started focus ways prepare team explosive growth requestsecond service turn k requestsecond service year identify largest sources stress sre teams sometimes fall ops mode focus quickly address emergencies instead reduce number emergencies default ops mode usually happens response overwhelming pressure real imagined ve learned enough service ask hard questions design deployment spend time prioritizing various service outages according impact team s stress levels keep mind due team s perspective history small problems outages may produce inordinate amount stress identify kindling identify team s largest existing problems move emergencies waiting happen sometimes impending emergencies come form new subsystem nt designed selfmanaging sources include knowledge gaps large teams people overspecialize without immediate consequence person specializes run risk either broad knowledge need perform oncall support allowing team members ignore critical pieces system services developed sre quietly increasing importance services often nt get careful attention new feature launches re smaller scale implicitly endorsed least one sre strong dependence next big thing people might ignore problems months time believe new solution s horizon obviates temporary fixes common alerts nt diagnosed either dev team sres alerts frequently triaged transient still distract teammates fixing real problems either investigate alerts fully fix alerting rules service subject complaints clients lacks formal slislosla see service level objectives discussion slis slos slas service capacity plan effectively add servers servers running memory last night capacity plans sufficiently forwardlooking system model predicts servers need gb loadtest passes short term revealing gb last run nt necessarily mean system capacity adequate shape postmortems action items rolling back specific changes caused outage example change streaming timeout back seconds instead figure sometimes takes seconds fetch first megabyte promo videos servingcritical component existing sres respond questions saying nt know anything devs give acceptable oncall support component least know consequences breaks urgency needed fix problems phase sharing context scoping dynamics pain points team lay groundwork improvement best practices like postmortems identifying sources toil best address write good postmortem team postmortems offer much insight team s collective reasoning postmortems conducted unhealthy teams often ineffectual team members might consider postmortems punitive even useless might tempted review postmortem archives leave comments improvement nt help team instead exercise might put team defensive instead trying correct previous mistakes take ownership next postmortem outage re embedded nt person oncall team oncall sre write great blameless postmortem document opportunity demonstrate shift toward sre model benefits team making bug fixes permanent permanent bug fixes reduce impact outages team members time mentioned might encounter responses response especially likely team believes postmortem process retaliatory attitude comes subscribing bad apple theory system working fine get rid bad apples mistakes system continue fine bad apple theory demonstrably false shown evidence dek several disciplines including airline safety point falsity effective phrasing postmortem say mistakes inevitable system multiple subtle interactions oncall trust make right decisions right information d like write thinking point time find system misled cognitive demands high sort fires according type two types fires simplifiedforconvenience model fires nt exist cause commonly called ops work toil see eliminating toil fires cause stress andor furious typing actually part job either case team needs build tools control burn sort team fires toil nottoil re finished present list team clearly explain fire either work automated acceptable overhead running service phase driving change team health process s something solve heroic effort ensure team selfregulate help build good mental model ideal sre engagement note humans pretty good homeostasis focus creating restoring right initial conditions teaching small set principles needed make healthy choices start basics teams struggling distinction sre traditional ops model generally unable articulate certain aspects team s code processes culture bother rather trying address issues pointbypoint work forward principles outlined chapters introduction monitoring distributed systems first goal team writing service level objective slo one nt already exist slo important provides quantitative measure impact outages addition important process change could slo probably single important lever moving team reactive ops work healthy longterm sre focus agreement missing advice chapter helpful find team without slos first read service level objectives get tech leads management room start arbitrating get help clearing kindling may strong urge simply fix issues identify please resist urge fix issues bolsters idea making changes people instead take following steps find useful work accomplished one team member clearly explain work addresses issue postmortem permanent way even otherwise healthy teams produce shortsighted action items serve reviewer code changes document revisions repeat two three issues identify additional issue put bug report doc team consult serves dual purposes distributing information encouraging team members write docs often first victim deadline pressure always explain reasoning emphasize good documentation ensures team nt repeat old mistakes slightly new context explain reasoning team recovers momentum grasps basics suggested changes move tackle quotidian decisions originally led ops overload prepare undertaking challenged re lucky challenge along lines explain right middle weekly production meeting re unlucky one demands explanation sidestep problem entirely simply explaining decisions whether someone requests explanation refer basics underscore suggestions helps build team s mental model leave team able predict comment design changelist would nt explain reasoning poorly risk team simply emulate lackadaisical behavior explicit examples thorough explanation decision m pushing back latest release tests bad m pushing back error budget set releases exhausted releases need rollbacksafe slo tight meeting slo requires mean time recovery small indepth diagnosis rollback realistic examples insufficient explanation decision nt think every server generate routing config safe ca nt see decision probably correct reasoning poor poorly explained team ca nt read mind likely might emulate observed poor reasoning instead try nt safe bug code cause correlated failure across service additional code source bugs might slow rollback automation give encounters conflicting deployment like previous example explanation probably correct insufficient instead try re making simplifying assumption changes pass automation something clearly violated rule happens often identify remove sources unorganized change ask leading questions leading questions loaded questions talking sre team try ask questions way encourages people think basic principles s particularly valuable model behavior definition team ops mode rejects sort reasoning constituents ve spent time explaining reasoning various policy questions practice reinforces team s understanding sre philosophy examples leading questions see taskfailures alert fires frequently oncall engineers usually nt anything respond alert impact slo turnup procedure looks pretty complicated know many config files update creating new instance service counterexamples leading questions s old stalled releases frobnitzer many things conclusion following tenets outlined chapter provides sre team following technical possibly quantitative perspective change strong example change looks like logical explanation much folk wisdom used sre core principles needed address novel situations scalable manner final task write afteraction report report reiterate perspective examples explanation also provide action items team ensure exercise ve taught organize report postvitam explaining critical decisions step led success bulk engagement complete embedded assignment concludes remain available design code reviews keep eye team next months confirm re slowly improving capacity planning emergency response rollout processes in contrast postmortem chapter communication collaboration sre written niall murphy alex rodriguez carl crous dario freni dylan curley lorenzo blanco todd underwood edited betsy beyer organizational position sre google interesting effects communicate collaborate begin tremendous diversity sre infrastructural teams service teams horizontal product teams relationships product development teams ranging teams many times size teams roughly size counterparts situations product development team sre teams made people systems engineering architectural skills see hixb software engineering skills project management skills leadership instincts backgrounds kinds industries see lessons learned industries nt one model found variety configurations work flexibility suits ultimately pragmatic nature s also true sre commandandcontrol organization generally owe allegiance least two masters service infrastructure sre teams work closely corresponding product development teams work services infrastructure also obviously work context sre generally service relationship strong since held accountable performance systems despite relationship actual reporting lines sre whole today spend time supporting individual services crossproduction work culture shared values produce strongly homogeneous approaches problems design two preceding facts steered sre organization certain directions comes two crucial dimensions teams operate communications collaboration data flow would apt computing metaphor communications like data must flow around production data also flow around sre teamdata projects state services production state individuals maximum effectiveness team data flow reliable ways one interested party another one way think flow think interface sre team must present teams api like api good design crucial effective operation api wrong painful correct later apiascontract metaphor also relevant collaboration among sre teams sre product development teamsall make progress environment unrelenting change extent collaboration looks quite like collaboration fastmoving company difference mix software engineering skills systems engineering expertise wisdom production experience sre brings bear collaboration best designs best implementations result joint concerns production product met atmosphere mutual respect promise sre organization charged reliability skills product development teams improve things measurably experience suggests simply someone charge reliability without also complete skill set enough communications production meetings although literature running effective meetings abounds kra s difficult find someone s lucky enough useful effective meetings equally true sre however s one kind meeting useful average called production meeting production meetings special kind meeting sre team carefully articulates itselfand invitees state service charge increase general awareness among everyone cares improve operation service general meetings serviceoriented directly status updates individuals goal everyone leave meeting idea s going onthe idea major goal production meetings improve services bringing wisdom production bear services means talk detail operational performance service relate operational performance design configuration implementation make recommendations fix problems connecting performance service design decisions regular meeting immensely powerful feedback loop production meetings usually happen weekly given sre s antipathy pointless meetings frequency seems right time allow enough relevant material accumulate make meeting worthwhile frequent people find excuses attend usually last somewhere minutes less re probably cutting something unnecessarily short probably growing service portfolio re probably getting mired detail ve got much talk shard team service set like meeting production meeting chair many sre teams rotate chair various team members advantage making everyone feel stake service notional ownership issues s true everyone equal levels chairing skill value group ownership large tradeoff temporary suboptimality worthwhile furthermore chance instill chairing skills useful kind incident coordination situations commonly faced sre cases two sre teams meeting video one teams much larger noticed interesting dynamic play recommend placing chair smaller side call default larger side naturally tends quiet bad effects imbalanced team sizes made worse delays inherent video conferencing improve idea technique scientific basis tend work agenda many ways run production meeting attesting diversity sre looks extent s appropriate prescriptive run one meetings however default agenda see example production meeting minutes example might look something like following upcoming production changes changetracking meetings well known throughout industry indeed whole meetings often devoted stopping change however production environment usually default enabling change requires tracking useful set properties change start time duration expected effect nearterm horizon visibility metrics one major ways conduct serviceoriented discussion talking core metrics systems question see service level objectives even systems nt dramatically fail week s common position re looking gradually sharply increasing load throughout year keeping track latency figures cpu utilization figures etc change time incredibly valuable developing feeling performance envelope system teams track resource usage efficiency also useful indicator slower perhaps insidious system changes outages item addresses problems approximately postmortem size indispensable opportunity learning good postmortem analysis discussed postmortem culture learning failure always set juices flowing paging events pages monitoring system relating problems postmortem worthy often nt event outages portion looks larger picture outage section looks tactical view list pages paged happened two implicit questions section alert paged way paged answer last question remove unactionable pages nonpaging events bucket contains three items issue probably paged nt cases probably fix monitoring events trigger page often encounter issue re trying fix something else s related metric re tracking nt got alert issue pageable requires attention lowimpact data corruption slowness nonuserfacing dimension system tracking reactive operational work also appropriate issue pageable require attention alerts removed create extra noise distracts engineers issues merit attention prior action items preceding detailed discussions often lead actions sre needs takefix monitor develop subsystem track improvements would tracked meeting assign action items people track progress s good idea explicit agenda item acts catchall nothing else consistent delivery also wonderful credibility trust builder nt matter delivery done done attendance attendance compulsory members sre team question particularly true team spread across multiple countries andor time zones major opportunity interact group major stakeholders also attend meeting partner product development teams may also attend sre teams shard meeting sreonly matters kept first half practice fine long everyone stated previously leaves idea s going time time representatives sre teams might turn particularly s larger crossteam issue discuss general sre team question plus major teams attend relationship invite product development partners need fix relationship perhaps first step invite representative team find trusted intermediary proxy communication model healthy interactions many reasons teams nt get along wealth writing solve problem information also applicable sre teams important end goal feedback loop operations fulfilled large part value sre team lost occasionally ll many teams busyyetcrucial attendees invite number techniques use handle situations less active services might attended single representative product development team commitment product development team read comment agenda minutes production development team quite large nominate subset representatives busyyetcrucial attendees provide feedback andor steering advance individuals using prefilled agenda technique described next meeting strategies ve discussed common sense serviceoriented twist one unique spin making meetings efficient inclusive use realtime collaborative features google docs many sre teams doc wellknown address anyone engineering access doc enables two great practices prepopulating agenda bottom ideas comments information preparing agenda parallel advance really efficient fully use multipleperson collaboration features enabled product s nothing quite like seeing meeting chair type sentence seeing someone else supply link source material brackets finished typing seeing yet another person tidy spelling grammar original sentence collaboration gets stuff done faster makes people feel like slice team collaboration within sre obviously google multinational organization emergency response pager rotation component role good business reasons distributed organization separated least time zones practical impact distribution fluid definitions team compared example average product development team local teams team site crosscontinental team virtual teams various sizes coherence everything creates cheerfully chaotic mix responsibilities skills opportunities much dynamics could expected pertain sufficiently large company although might particularly intense tech companies given local collaboration faces particular obstacle interesting case collaborationwise crossteam crosssite across virtual team similar pattern distribution also informs sre teams tend organized raison dêtre bringing value technical mastery technical mastery tends hard therefore try find way mastery related subset systems infrastructures order decrease cognitive load specialization one way accomplishing objective ie team x works product specialization good leads higher chances improved technical mastery s also bad leads siloization ignorance broader picture try crisp team charter define team willand importantly wontsupport nt always succeed team composition wide array skill sets sre ranging systems engineering software engineering organization management one thing say collaboration chances successful collaborationand indeed anything elseare improved diversity team s lot evidence suggesting diverse teams simply better teams nel running diverse team implies particular attention communication cognitive biases ca nt cover detail formally sre teams roles tech lead tl manager srm project manager also known pm tpm pgm people operate best roles welldefined responsibilities major benefit make inscope decisions quickly safely others operate best fluid environment shifting responsibilities depending dynamic negotiation general fluid team developed terms capabilities individuals able team adapt new situationsbut cost communicate often less background assumed regardless well roles defined base level tech lead responsible technical direction team lead variety ways everything carefully commenting everyone s code holding quarterly direction presentations building consensus team google tls almost manager s job managers highly technical manager two special responsibilities tl nt performance management function general catchall everything nt handled someone else great tls srms tpms complete set skills cheerfully turn hand organizing project commenting design doc writing code necessary techniques working effectively number ways engineer effectively sre general singleton projects fail unless person particularly gifted problem straightforward accomplish anythingsignificant pretty much need multiple people therefore also need good collaboration skills lots material written topic much literature applicable sre general good sre work calls excellent communication skills re working outside boundary purely local team collaborations outside building effectively working across time zones implies either great written communication lots travel supply inperson experience deferrable ultimately necessary highquality relationship even re great writer time decay email address turn flesh case study collaboration sre viceroy one example successful crosssre collaboration project called viceroy monitoring dashboard framework service current organizational architecture sre end teams producing multiple slightly different copies piece work various reasons monitoring dashboard frameworks particularly fertile ground duplication work incentives led serious litter problem many smoldering abandoned hulks monitoring frameworks lying around pretty simple team rewarded developing solution working outside team boundary hard infrastructure tended provided srewide typically closer toolkit product environment encouraged individual engineers use toolkit make another burning wreck rather fix problem largest number people possible effort would therefore take much longer coming viceroy viceroy different began number teams considering move monarch new monitoring system google sre deeply conservative respect monitoring systems monarch somewhat ironically took longer get traction within sre within nonsre teams one could argue legacy monitoring system borgmon see practical alerting timeseries data room improvement example consoles cumbersome used custom html templating system specialcased full funky edge cases difficult test time monarch matured enough accepted principle replacement legacy system therefore adopted teams across google turned still problem consoles us tried using monarch services soon found fell short console support two main reasons consoles easy set small service nt scale well services complex consoles also nt support legacy monitoring system making transition monarch difficult viable alternative deploying monarch way existed time number teamspecific projects launched since little enough way coordinated development solutions even crossgroup tracking time problem since fixed ended duplicating efforts yet multiple teams spanner ads frontend variety services spun efforts one notable example called consoles course months eventually sanity prevailed engineers teams woke discovered s respective efforts decided sensible thing join forces order create general solution sre thus viceroy project born mid beginning viceroy started gather interest teams yet move legacy system looking put toe water obviously teams larger existing monitoring projects fewer incentives move new system hard teams rationalize jettisoning low maintenance cost existing solution basically worked fine something relatively new unproven would require lots effort make work sheer diversity requirements added reluctance teams even though monitoring console projects shared two main requirements notably support complex curated dashboards support monarch legacy monitoring system project also set technical requirements depended author s preference experience example multiple data sources outside core monitoring systems definition consoles using configuration versus explicit html layout javascript versus full embrace javascript ajax sole use static content consoles cached browser although requirements stickier others overall made merging efforts difficult indeed although consoles team interested seeing project compared viceroy initial examination first half determined fundamental differences two projects significant enough prevent integration largest difficulty viceroy design use much javascript consoles mostly written javascript glimmer hope however two systems number underlying similarities used similar syntaxes html template rendering shared number longterm goals neither team yet begun address example systems wanted cache monitoring data support offline pipeline periodically produce data console use computationally expensive produce demand ended parking unified console discussion however end consoles viceroy developed significantly technical differences narrowed viceroy started using javascript render monitoring graphs two teams met figured integration lot easier integration boiled serving consoles data viceroy server first integrated prototypes completed early proved systems could work well together teams felt comfortable committing joint effort point viceroy already established brand common monitoring solution combined project retained viceroy name developing full functionality took quarters end combined system complete joining forces reaped huge benefits viceroy received host data sources javascript clients access javascript compilation rewritten support separate modules selectively included essential scale system number teams javascript code consoles benefited many improvements actively made viceroy addition cache background data pipeline overall development velocity one solution much larger sum development velocity duplicative projects ultimately common future vision key factor combining projects teams found value expanding development team benefited s contributions momentum end viceroy officially declared general monitoring solution sre perhaps characteristically google declaration nt require teams adopt viceroy rather recommended teams use viceroy instead writing another monitoring console challenges ultimately success viceroy without difficulties many arose due crosssite nature project extended viceroy team established initial coordination among remote team members proved difficult meeting people first time subtle cues writing speaking misinterpreted communication styles vary substantially person person start project team members nt located mountain view also missed impromptu water cooler discussions often happened shortly meetings although communication since improved considerably core viceroy team remained fairly consistent extended team contributors fairly dynamic contributors responsibilities changed time therefore many able dedicate one three months project thus developer contributor pool inherently larger core viceroy team characterized significant amount churn adding new people project required training contributor overall design structure system took time hand sre contributed core functionality viceroy later returned team local expert system unanticipated dissemination local viceroy experts drove usage adoption people joined left team found casual contributions useful costly primary cost dilution ownership features delivered person left features became unsupported time generally dropped furthermore scope viceroy project grew time ambitious goals launch initial scope limited scope grew however struggled deliver core features time improve project management set clearer direction ensure project stayed track finally viceroy team found difficult completely component significant determining contributions distributed sites even best world people generally default path least resistance discuss issues make decisions locally without involving remote owners lead conflict recommendations develop projects crosssite often good reasons cost working across sites higher latency actions communication required benefit isif get mechanics rightmuch higher throughput single site project also fall foul one outside site knowing re costs approaches motivated contributors valuable contributions equally valuable make sure project contributors actually committed nt joining nebulous selfactualization goal wanting earn notch belt attaching name shiny project wanting code new exciting project without committing maintaining project contributors specific goal achieve generally better motivated better maintain contributions projects develop usually grow re always lucky position people local team contribute project therefore think carefully project structure project leaders important provide longterm vision project make sure work aligns vision prioritized correctly also need agreed way making decisions specifically optimize making decisions locally high level agreement trust standard divide conquer strategy applies crosssite projects reduce communication costs primarily splitting project many reasonably sized components possible trying make sure component assigned small group preferably within one site divide components among project subteams establish clear deliverables deadlines try let conway s law distort natural shape software deeply goal project team works best s oriented toward providing functionality solving problem approach ensures individuals working component know expected work complete component fully integrated used within main project obviously usual engineering best practices apply collaborative projects component design documents reviews team way everyone team given opportunity stay abreast changes addition chance influence improve designs writing things one major techniques offset physical andor logical distanceuse standards important coding style guidelines good start re usually quite tactical therefore starting point establishing team norms every time debate around choice make issue argue fully team strict time limit pick solution document move ca nt agree need pick arbitrator everyone respects move forward time ll build collection best practices help new people come speed ultimately s substitute inperson interaction although portion facetoface interaction deferred good use vc good written communication leaders project meet rest team person time budget allows organize team summit members team interact person summit also provides great opportunity hash designs goals situations neutrality important s advantageous hold team summits neutral location individual site home advantage finally use project management style suits project current state even projects ambitious goals start small overhead correspondingly low project grows s appropriate adapt change project managed given sufficient growth full project management necessary collaboration outside sre suggested evolving sre engagement model discusses collaboration product development organization sre really best occurs early design phase ideally line code committed sres best placed make recommendations architecture software behavior quite difficult impossible retrofit voice present room new system designed goes better everyone broadly speaking use objectives key results okr process kla track work service teams collaboration mainstay dotracking new designs making recommendations helping implement seeing production case study migrating dfp f large migration projects existing services quite common google typical examples include porting service components new technology updating components support new data format recent introduction database technologies scale global level spanner cor f shu google undertaken number largescale migration projects involving databases one project migration main database doubleclick publishers dfp mysql f particular chapter s authors charge portion serving system shown figure continually extracts processes data database order generate set indexed files loaded served around world system distributed several datacenters used cpus tb ram index tb data every day figure generic ads serving system migration nontrivial addition migrating new technology database schema significantly refactored simplified thanks ability f store index protocol buffer data table columns goal migrate processing system could produce output perfectly identical existing system allowed us leave serving system untouched perform user s perspective seamless migration added restriction product required complete live migration without disruption service users time order achieve product development team sre team started working closely beginning develop new indexing service main developers product development teams typically familiar business logic bl software also closer contact product managers actual business need component products hand sre teams usually expertise pertaining infrastructure components software eg libraries talk distributed storage systems databases sres often reuse building blocks across different services learning many caveats nuances allow software run scalably reliably time start migration project product development sre knew would collaborate even closely conducting weekly meetings sync project s progress particular case bl changes partially dependent upon infrastructure changes reason project started design new infrastructure sres extensive knowledge domain extracting processing data scale drove design infrastructure changes involved designing extract various tables f filter join data extract data changed opposed entire database sustain loss machines without impacting service ensure resource usage grows linearly amount extracted data capacity planning many similar aspects new proposed infrastructure similar services already extracting processing data f therefore could sure soundness solution reuse parts monitoring tooling proceeding development new infrastructure two sres produced detailed design document product development sre teams thoroughly reviewed document tweaking solution handle edge cases eventually agreed design plan plan clearly identified kind changes new infrastructure would bring bl example designed new infrastructure extract changed data instead repeatedly extracting entire database bl take account new approach early defined new interfaces infrastructure bl allowed product development team work independently bl changes similarly product development team kept sre informed bl changes interacted eg bl changes dependent infrastructure coordination structure allowed us know changes happening handle quickly correctly later phases project sres began deploying new service testing environment resembled project s eventual finished production environment step essential measure expected behavior servicein particular performance resource utilizationwhile development bl still underway product development team used testing environment perform validation new service index ads produced old service running production match perfectly index produced new service running testing environment suspected validation process highlighted discrepancies old new services due edge cases new data format product development team able resolve iteratively ad debugged cause difference fixed bl produced bad output meantime sre team began preparing production environment allocating necessary resources different datacenter setting processes monitoring rules training engineers designated oncall service sre team also set basic release process included validation task usually completed product development team release engineers specific case completed sres speed migration service ready sres prepared rollout plan collaboration product development team launched new service launch successful proceeded smoothly without visible user impact conclusion given globally distributed nature sre teams effective communication always high priority sre chapter discussed tools techniques sre teams use maintain effective relationships among team various partner teams collaboration sre teams challenges potentially great rewards including common approaches platforms solving problems letting us focus solving difficult problems and know culture beats strategy every time mer the larger team generally tends unintentionally talk smaller team s difficult control distracting side conversations etc in particular case road hell indeed paved javascript that software structure communications structure organization produces https enwikipediaorgwikiconway slaw softwaresee doubleclick publishers tool publishers manage ads served websites apps chapter evolving sre engagement model written acacio cruz ashish bhambhani edited betsy beyer tim harvey sre engagement ve discussed rest book happens sre already charge service services begin lifecycle enjoying sre support needs process evaluating service making sure merits sre support negotiating improve deficits bar sre support actually instituting sre support call process onboarding environment surrounded lot existing services varying states perfection sre team probably running prioritized queue onboardings quite team finished taking highestvalue targets although common completely reasonable way dealing fait accompli environment actually least two better ways bringing wisdom production sre support services old new alike first case software engineeringwhere earlier bug found cheaper fixthe earlier sre team consultation happens better service quicker feel benefit sre engaged earliest stages design time onboard lowered service reliable gate usually nt spend time unwinding suboptimal design implementation another way perhaps best shortcircuit process specially created systems lots individual variations end arriving sre s door provide product development platform srevalidated infrastructure upon build systems platform double benefit reliable scalable avoids certain classes cognitive load problems entirely addressing common infrastructure practices allows product development teams focus innovation application layer mostly belongs following sections ll spend time looking models turn beginning classic one prrdriven model prr model typical initial step sre engagement production readiness review prr process identifies reliability needs service based specific details prr sres seek apply ve learned experienced ensure reliability service operating production prr considered prerequisite sre team accept responsibility managing production aspects service figure illustrates lifecycle typical service production readiness review started point service lifecycle stages sre engagement applied expanded time chapter describes simple prr model discusses modification extended engagement model frameworks sre platform structure allowed sre scale engagement process impact figure typical service lifecycle sre engagement model sre seeks production responsibility important services make concrete contributions reliability sre concerned several aspects service collectively referred production aspects include following system architecture interservice dependencies instrumentation metrics monitoring emergency response capacity planning change management performance availability latency efficiency sres engage service aim improve along axes makes managing production service easier alternative support google services receive close sre engagement couple factors play many services nt need high reliability availability support provided means design number development teams request sre support exceeds available bandwidth sre teams see introduction sre ca nt provide fullfledged support provides options making improvements production documentation consultation documentation development guides available internal technologies clients widely used systems google s production guide documents production best practices services determined experiences sre development teams alike developers implement solutions recommendations documentation improve services consultation developers may also seek sre consulting discuss specific services problem areas launch coordination engineering lce team see reliable product launches scale spends majority time consulting development teams sre teams nt specifically dedicated launch consultations also engage consultation development teams new service new feature implemented developers usually consult sre advice preparing launch phase launch consultation usually involves one two sres spending hours studying design implementation high level sre consultants meet development team provide advice risky areas need attention discuss wellknown patterns solutions incorporated improve service production advice may come production guide mentioned earlier consultation sessions necessarily broad scope s possible gain deep understanding given system limited time available development teams consultation sufficient services grown orders magnitude since launched require time understand feasible documentation consultation services upon many services subsequently come rely upon host significantly traffic many different clients types services may grown point begin encounter significant difficulties production simultaneously becoming important users cases longterm sre engagement becomes necessary ensure properly maintained production grow production readiness reviews simple prr model development team requests sre take production management service sre gauges importance service availability sre teams service merits sre support sre team development organization agree staffing levels facilitate support sre initiates production readiness review development team objectives production readiness review follows verify service meets accepted standards production setup operational readiness service owners prepared work sre take advantage sre expertise improve reliability service production minimize number severity incidents might expected prr targets aspects production sre cares sufficient improvements made service deemed ready sre support sre team assumes production responsibilities brings us production readiness review process three different related engagement models simple prr model early engagement model frameworks sre platform discussed turn first describe simple prr model usually targeted service already launched taken sre team prr follows several phases much like development lifecycle although may proceed independently parallel development lifecycle engagement sre leadership first decides sre team good fit taking service usually one three sres selected selfnominated conduct prr process small group initiates discussion development team discussion covers matters establishing slosla service planning potentially disruptive design changes required improve reliability planning training schedules goal arrive common agreement process end goals outcomes necessary sre team engage development team service analysis analysis first large segment work phase sre reviewers learn service begin analyzing production shortcomings aim gauge maturity service along various axes concern sre also examine service s design implementation check follows production best practices usually sre team establishes maintains prr checklist explicitly analysis phase checklist specific service generally based domain expertise experience related similar systems best practices production guide sre team may also consult teams experience certain components dependencies service examples checklist items include updates service impact unreasonably large percentage system service connect appropriate serving instance dependencies example enduser requests service depend system designed batchprocessing use case service request sufficiently high network qualityofservice talking critical remote service service report errors central logging systems analysis report exceptional conditions result degraded responses failures end users uservisible request failures well instrumented monitored suitable alerting configured checklist may also include operational standards best practices followed specific sre team example perfectly functional service configuration nt follow sre team s gold standard might refactored work better sre tools scalably managing configurations sres also look recent incidents postmortems service well followup tasks incidents evaluation gauges demands emergency response service availability wellestablished operational controls improvements refactoring analysis phase leads identification recommended improvements service next phase proceeds follows improvements prioritized based upon importance service reliability priorities discussed negotiated development team plan execution agreed upon sre product development teams participate assist refactoring parts service implementing additional features phase typically varies duration amount effort much time effort phase involve depends upon availability engineering time refactoring maturity complexity service start review myriad factors training responsibility managing service production generally assumed entire sre team ensure team prepared sre reviewers led prr take ownership training team includes documentation necessary support service typically help participation development team engineers organize series training sessions exercises instruction include design overviews deep dives various request flows system description production setup handson exercises various aspects system operations training concluded sre team prepared manage service onboarding training phase unblocks onboarding service sre team involves progressive transfer responsibilities ownership various production aspects service including parts operations change management process access rights forth sre team continues focus various areas production mentioned earlier complete transition development team must available back advise sre team period time settles managing production service relationship becomes basis ongoing work teams continuous improvement active services continuously change response new demands conditions including user requests new features evolving system dependencies technology upgrades addition factors sre team must maintain service reliability standards face changes driving continuous improvement responsible sre team naturally learns service course operating service reviewing new changes responding incidents especially conducting postmortemsroot cause analyses expertise shared development team suggestions proposals changes service whenever new features components dependencies may added service lessons managing service also contributed best practices documented production guide elsewhere engaging shakespeare initially developers shakespeare service responsible product including carrying pager emergency response however growing use service growth revenue coming service sre support became desirable product already launched sre conducted production readiness review one things found dashboards completely covering metrics defined slo needed fixed issues filed fixed sre took pager service though two developers oncall rotation well developers participating weekly oncall meeting discussing last week s problems handle upcoming largescale maintenance cluster turndowns also future plans service discussed sres make sure new launches go flawlessly though murphy s law always looking opportunities spoil evolving simple prr model early engagement evolving simple prr model early engagement thus far ve discussed production readiness review s used simple prr model limited services already entered launch phase several limitations costs associated model example additional communication teams increase process overhead development team cognitive burden sre reviewers right sre reviewers must available capable managing time priorities regards existing engagements work done sres must highly visible sufficiently reviewed development team ensure effective knowledge sharing sres essentially work part development team rather external unit however main limitations prr model stem fact service launched serving scale sre engagement starts late development lifecycle prr occurred earlier service lifecycle sre s opportunity remedy potential issues service would markedly increased result success sre engagement future success service would likely improve resulting drawbacks pose significant challenge success sre engagement future success service candidates early engagement early engagement model introduces sre earlier development lifecycle order achieve significant additional advantages applying early engagement model requires identifying importance andor business value service early development lifecycle determining service sufficient scale complexity benefit sre expertise applicable services often following characteristics service implements significant new functionality part existing system already managed sre service significant rewrite alternative existing system targeting use cases development team sought sre advice approached sre takeover upon launch early engagement model essentially immerses sres development process sre s focus remains though means achieve better production service different sre participates design later phases eventually taking service time build phase model based active collaboration development sre teams benefits early engagement model early engagement model entail certain risks challenges discussed previously additional sre expertise collaboration entire lifecycle product creates significant benefits compared engagement initiated later service lifecycle design phase sre collaboration design phase prevent variety problems incidents occurring later production design decisions reversed rectified later development lifecycle changes come high cost terms effort complexity best production incidents never happen occasionally difficult tradeoffs lead selection lessthanideal design participation design phase means sres aware front tradeoffs part decision pick lessthanideal option early sre involvement aims minimize future disputes design choices service production build implementation build phase addresses production aspects instrumentation metrics operational emergency controls resource usage efficiency phase sre influence improve implementation recommending specific existing libraries components helping build certain controls system sre participation stage helps enable ease operations future allows sre gain operational experience advance launch launch sre also help implement widely used launch patterns controls example sre might help implement dark launch setup part traffic existing users sent new service addition sent live production service responses new service dark since thrown away actually shown users practices dark launches allow team gain operational insight resolve issues without impacting existing users reduce risk encountering issues launch smooth launch immensely helpful keeping operational burden low maintaining development momentum launch disruptions around launch easily result emergency changes source code production disrupt development team s work future features postlaunch stable system launch time generally leads fewer conflicting priorities development team terms choosing improving service reliability versus adding new features later phases service lessons earlier phases better inform refactoring redesign extended involvement sre team ready take new service much sooner possible simple prr model longer closer engagement sre development teams also creates collaborative relationship sustained long term positive crossteam relationship fosters mutual feeling solidarity helps sre establish ownership production responsibility disengaging service sometimes service nt warrant fullfledged sre team managementthis determination might made postlaunch sre might engage service never officially take positive outcome service engineered reliable low maintenance therefore remain development team also possible sre engages early service fails meet levels usage projected cases sre effort spent simply part overall business risk comes new projects small cost relative success projects meet expected scale sre team reassigned lessons learned incorporated engagement process evolving services development frameworks sre platform early engagement model made strides evolving sre engagement beyond simple prr model applied services already launched however still progress made scaling sre engagement next level designing reliability lessons learned time sre engagement model described thus far produced several distinct patterns onboarding service required two three sres typically lasted two three quarters lead times prr relatively high quarters away effort level required proportional number services review constrained insufficient number sres available conduct prrs conditions led serialization service takeovers strict service prioritization due differing software practices across services production feature implemented differently meet prrdriven standards features usually reimplemented specifically service best small subset services sharing code reimplementations waste engineering effort one canonical example implementation functionally similar logging frameworks repeatedly language different services nt implement coding structure review common service issues outages revealed certain patterns way easily replicate fixes improvements across services typical examples included service overload situations data hotspotting sre software engineering contributions often local service thus building generic solutions reused difficult consequence easy way implement new lessons individual sre teams learned best practices across services already onboarded external factors affecting sre external factors traditionally pressured sre organization resources several ways google increasingly following industry trend moving toward microservices result number requests sre support cardinality services support increased service base fixed operational cost even simple services demand staffing microservices also imply expectation lower lead time deployment possible previous prr model lead time months hiring experienced qualified sres difficult costly despite enormous effort recruiting organization never enough sres support services need expertise sres hired training also lengthier process typical development engineers finally sre organization responsible serving needs large growing number development teams already enjoy direct sre support mandate calls extending sre support model far beyond original concept engagement model toward structural solution frameworks effectively respond conditions became necessary develop model allowed following principles codified best practices ability commit works well production code services simply use code become production ready design reusable solutions common easily shareable implementations techniques used mitigate scalability reliability issues common production platform common control surface uniform sets interfaces production facilities uniform sets operational controls uniform monitoring logging configuration services easier automation smarter systems common control surface enables automation smart systems level possible example sres readily receive single view relevant information outage rather hand collecting analyzing mostly raw data disparate sources logs monitoring data based upon principles set sresupported platform service frameworks created one environment support java c go services built using frameworks share implementations designed work sresupported platform maintained sre development teams main shift brought frameworks enable product development teams design applications using framework solution built blessed sre opposed either retrofitting application sre specifications fact retrofitting sres support service markedly different google services application typically comprises business logic turn depends various infrastructure components sre production concerns largely focused infrastructurerelated parts service service frameworks implement infrastructure code standardized fashion address various production concerns concern encapsulated one framework modules provides cohesive solution problem domain infrastructure dependency framework modules address various sre concerns enumerated earlier instrumentation metrics request logging control systems involving traffic load management sre builds framework modules implement canonical solutions concerned production area result development teams focus business logic framework already takes care correct infrastructure use framework essentially prescriptive implementation using set software components canonical way combining components framework also expose features control various components cohesive manner example framework might provide following business logic organized welldefined semantic components referenced using standard terms standard dimensions monitoring instrumentation standard format request debugging logs standard configuration format managing load shedding capacity single server determination overload use semantically consistent measure feedback various control systems frameworks provide multiple upfront gains consistency efficiency free developers glue together configure individual components ad hoc servicespecific manner eversoslightly incompatible ways manually reviewed sres drive single reusable solution production concerns across services means framework users end common implementation minimal configuration differences google supports several major languages application development frameworks implemented across languages different implementations framework say c versus java ca nt share code goal expose api behavior configuration controls identical functionality therefore development teams choose language platform fits needs experience sres still expect familiar behavior production standard tools manage service new service management benefits structural approach founded service frameworks common production platform control surface provided host new benefits significantly lower operational overhead production platform built top frameworks stronger conventions significantly reduced operational overhead following reasons supports strong conformance tests coding structure dependencies tests coding style guides functionality also improves user data privacy testing security conformance features builtin service deployment monitoring automation services facilitates easier management large numbers services especially microservices growing number enables much faster deployment idea graduate fully deployed srelevel production quality matter days universal support design constant growth number services google means services neither warrant sre engagement maintained sres regardless services nt receive full sre support built use production features developed maintained sres practice effectively breaks sre staffing barrier enabling sresupported production standards tools teams improves overall service quality across google furthermore services implemented frameworks automatically benefit improvements made time frameworks modules faster lower overhead engagements frameworks approach results faster prr execution rely upon builtin service features part framework implementation faster service onboarding usually accomplished single sre one quarter less cognitive burden sre teams managing services built using frameworks properties allow sre teams lower assessment qualification effort service onboarding maintaining high bar service production quality new engagement model based shared responsibility original sre engagement model presented two options either full sre support approximately sre engagement production platform common service structure conventions software infrastructure made possible sre team provide support platform infrastructure development teams provide oncall support functional issues servicethat bugs application code model sres assume responsibility development maintenance large parts service software infrastructure particularly control systems load shedding overload automation traffic management logging monitoring model represents significant departure way service management originally conceived two major ways entails new relationship model interaction sre development teams new staffing model sresupported service management part v conclusions covered much ground terms sre works google principles practices ve developed might applied organizations field seems appropriate turn view lessons learned industries examine sre s practices compare industries reliability critically important finally google s vp site reliability engineering benjamin lutch writes sre s evolution course career conclusion examining sre lens observations aviation industry chapter lessons learned industries written jennifer petoff edited betsy beyer deep dive sre culture practices google naturally leads question industries manage businesses reliability compiling book google sre created opportunity speak number google s engineers previous work experiences variety highreliability fields order address following comparative questions principles used site reliability engineering also important outside google industries tackle requirements high reliability markedly different ways industries also adhere sre principles principles manifested similarities differences implementation principles across industries factors drive similarities differences implementation google tech industry learn comparisons number principles fundamental site reliability engineering google discussed throughout text simplify comparison best practices industries distilled concepts four key themes preparedness disaster testing postmortem culture automation reduced operational overhead structured rational decision making chapter introduces industries profiled industry veterans interviewed define key sre themes discuss themes implemented google give examples principles reveal industries comparative purposes conclude insights discussion patterns antipatterns discovered meet industry veterans peter dahl principal engineer google previously worked defense contractor several highreliability systems including many airborne wheeled vehicle gps inertial guidance systems consequences lapse reliability systems include vehicle malfunction loss financial consequences associated failure mike doherty site reliability engineer google worked lifeguard lifeguard trainer decade canada reliability absolutely essential nature field lives line every day erik gross currently software engineer google joining company spent seven years designing algorithms code lasers systems used perform refractive eye surgery eg lasik highstakes highreliability field many lessons relevant reliability face government regulations human risk learned technology received fda approval gradually improved finally became ubiquitous gus hartmann kevin greer experience telecommunications industry including maintaining e emergency response system kevin currently software engineer google chrome team gus systems engineer google s corporate engineering team user expectations telecom industry demand high reliability implications lapse service range user inconvenience due system outage fatalities e goes ron heiby technical program manager site reliability engineering google ron experience development cell phones medical devices automotive industry cases worked interface components industries example device allow ekg readings ambulances transmitted digital wireless phone network industries impact reliability issue range harm business incurred equipment recalls indirectly impacting life health eg people getting medical attention need ekg communicate hospital adrian hilton launch coordination engineer google previously worked uk usa military aircraft naval avionics aircraft stores management systems uk railway signaling systems reliability critical space impact incidents ranges multimilliondollar loss equipment injuries fatalities eddie kennedy project manager global customer experience team google mechanical engineer training eddie spent six years working six sigma black belt process engineer manufacturing facility makes synthetic diamonds industry characterized relentless focus safety extremes temperature pressure demands process pose high level danger workers daily basis john li currently site reliability engineer google john previously worked systems administrator software developer proprietary trading company finance industry reliability issues financial sector taken quite seriously lead serious fiscal consequences dan sheridan site reliability engineer google joining company worked safety consultant civil nuclear industry uk reliability important nuclear industry incident serious repercussions outages incur millions day lost revenue risks workers community even dire dictating zero tolerance failure nuclear infrastructure designed series failsafes halt operations incident magnitude reached jeff stevenson currently hardware operations manager google past experience nuclear engineer us navy submarine reliability stakes nuclear navy highproblems arise case incidents range damaged equipment longstanding environmental impact potential loss life matthew toia site reliability manager focused storage systems prior google worked software development deployment air traffic control software systems effects incidents industry range inconveniences passengers airlines eg delayed flights diverted planes potential loss life event crash defense depth key strategy avoiding catastrophic failures ve met experts gained highlevel understanding reliability important respective former fields ll delve four key themes reliability preparedness disaster testing hope strategy rallying cry sre team google sums mean preparedness disaster testing sre culture forever vigilant constantly questioning could go wrong action take address issues lead outage data loss annual disaster recovery testing dirt drills seek address questions headon kri dirt exercises sres push production systems limit inflict actual outages order ensure systems react way think determine unexpected weaknesses figure ways make systems robust order prevent uncontrolled outages several strategies testing disaster readiness ensuring preparedness industries emerged conversations strategies included following relentless organizational focus safety attention detail swing capacity simulations live drills training certification obsessive focus detailed requirements gathering design defense depth relentless organizational focus safety principle particularly important industrial engineering context according eddie kennedy worked manufacturing floor workers faced safety hazards every management meeting started discussion safety manufacturing industry prepares unexpected establishing highly defined processes strictly followed every level organization critical employees take safety seriously workers feel empowered speak anything seems amiss case nuclear power military aircraft railway signaling industries safety standards software well detailed eg uk defence standard iec iec us dobc do levels reliability systems clearly identified eg safety integrity level sil aim specifying acceptable approaches delivering product attention detail time spent us navy jeff stevenson recalls acute awareness lack diligence executing small tasks example lube oil maintenance could lead major submarine failure small oversight mistake big effects systems highly interconnected accident one area impact multiple related components nuclear navy focuses routine maintenance ensure small issues nt snowball swing capacity system utilization telecom industry highly unpredictable absolute capacity strained unforeseeable events natural disasters well large predictable events like olympics according gus hartmann industry deals incidents deploying swing capacity form sow switch wheels mobile telco office excess capacity rolled emergency anticipation known event likely overload system capacity issues veer unexpected matters unrelated absolute capacity well example celebrity s private phone number leaked thousands fans simultaneously attempted call telecom system exhibited symptoms similar ddos massive routing error simulations live drills google s disaster recovery tests lot common simulations live drills key focus many established industries researched potential consequences system outage determine whether using simulation live drill appropriate example matthew toia points aviation industry ca nt perform live test production without putting equipment passengers risk instead employ extremely realistic simulators live data feeds control rooms equipment modeled tiniest details ensure realistic experience without putting real people risk gus hartmann reports telecom industry typically focuses live drills centered surviving hurricanes weather emergencies modeling led creation weatherproof facilities generators inside building capable outlasting storm us nuclear navy uses mixture thought exercises live drills according jeff stevenson live drills involve actually breaking real stuff control parameters live drills carried religiously every week two three days per week nuclear navy thought exercises useful sufficient prepare actual incidents responses must practiced forgotten according mike doherty lifeguards face disaster testing exercises akin mystery shopper experience typically facility manager works child incognito lifeguard training stage mock drowning scenarios conducted realistic possible lifeguards nt able differentiate real staged emergencies training certification interviews suggest training certification particularly important lives stake example mike doherty described lifeguards complete rigorous training certification addition periodic recertification process courses include fitness components eg lifeguard must able hold someone heavier shoulders water technical components like first aid cpr operational elements eg lifeguard enters water team members respond every facility also sitespecific training lifeguarding pool markedly different lifeguarding lakeside beach ocean focus detailed requirements gathering design engineers interviewed discussed importance detailed requirements gathering design docs practice particularly important working medical devices many cases actual use maintenance equipment nt fall within purview product designers thus usage maintenance requirements must gathered sources example according erik gross laser eye surgery machines designed foolproof possible thus soliciting requirements surgeons actually use machines technicians responsible maintaining particularly important another example former defense contractor peter dahl described detailed design culture creating new defense system commonly entailed entire year design followed three weeks writing code actualize design examples markedly different google s launch iterate culture promotes much faster rate change calculated risk industries eg medical industry military previously discussed different pressures risk appetites requirements processes much informed circumstances defense depth breadth nuclear power industry defense depth key element preparedness iaea nuclear reactors feature redundancy systems implement design methodology mandates fallback systems behind primary systems case failure system designed multiple layers protection including final physical barrier radioactive release around plant defense depth particularly important nuclear industry due zero tolerance failures incidents postmortem culture corrective preventative action capa wellknown concept improving reliability focuses systematic investigation root causes identified issues risks order prevent recurrence principle embodied sre s strong culture blameless postmortems something goes wrong given scale complexity rapid rate change google something inevitably go wrong s important evaluate following happened effectiveness response would differently next time actions taken make sure particular incident nt happen exercise undertaken without pointing fingers individual instead assigning blame far important figure went wrong organization rally ensure nt happen dwelling might caused outage counterproductive postmortems conducted incidents published across sre teams benefit lessons learned interviews uncovered many industries perform version postmortem although many use specific moniker obvious reasons motivation behind exercises appears main differentiator among industry practices many industries heavily regulated held accountable specific government authorities something goes wrong regulation especially ingrained stakes failure high eg lives stake relevant government agencies include fcc telecommunications faa aviation osha manufacturing chemical industries fda medical devices various national competent authorities eu nuclear power transportation industries also heavily regulated safety considerations another motivating factor behind postmortems manufacturing chemical industries risk injury death everpresent due nature conditions required produce final product high temperature pressure toxicity corrosivity name example alcoa features noteworthy safety culture former ceo paul oneill required staff notify within hours injury lost worker day even distributed home phone number workers factory floor could personally alert safety concerns stakes high manufacturing chemical industries even near misses when given event could caused serious harm are carefully scrutinized scenarios function type preemptive postmortem according vm brasseur talk given yapc na multiple near misses every disaster business crisis typically re ignored time occur latent error plus enabling condition equals things working quite way planned bra near misses effectively disasters waiting happen example scenarios worker nt follow standard operating procedure employee jumps way last second avoid splash spill staircase nt cleaned represent near misses opportunities learn improve next time employee company might lucky united kingdom s chirp confidential reporting programme aviation maritime seeks raise awareness incidents across industry providing central reporting point aviation maritime personnel report near misses confidentially reports analyses near misses published periodic newsletters lifeguarding deeply embedded culture postincident analysis action planning mike doherty quips lifeguard s feet go water paperwork detailed writeup required incident pool beach case serious incidents team collectively examines incident end end discussing went right went wrong operational changes made based findings training often scheduled help people build confidence around ability handle similar incident future cases particularly shocking traumatic incidents counselor brought site help staff cope psychological aftermath lifeguards may well prepared happened practice might feel like nt done adequate job similar google lifeguarding embraces culture blameless incident analysis incidents chaotic many factors contribute given incident field s helpful place blame single individual automating away repetitive work operational overhead core google s site reliability engineers software engineers low tolerance repetitive reactive work strongly ingrained culture avoid repeating operation nt add value service task automated away would run system repetitive work low value automation lowers operational overhead frees time engineers proactively assess improve services support industries surveyed mixed terms embraced automation certain industries trusted humans machines tenure industry veteran us nuclear navy eschewed automation favor series interlocks administrative procedures example according jeff stevenson operating valve required operator supervisor crew member phone engineering watch officer tasked monitoring response action taken operations manual due concern automated system might spot problem human would definitely notice operations submarine ruled trusted human decision chaina series people rather one individual nuclear navy also concerned automation computers move rapidly capable committing large irreparable mistake dealing nuclear reactors slow steady methodical approach important accomplishing task quickly according john li proprietary trading industry become increasingly cautious application automation recent years experience shown incorrectly configured automation inflict significant damage incur great deal financial loss short period time example knight capital group encountered software glitch led loss m hours similarly us stock market experienced flash crash ultimately blamed rogue trader attempting manipulate market automated means market quick recover flash crash resulted loss magnitude trillions dollars minutes computers execute tasks quickly speed negative tasks configured incorrectly contrast companies embrace automation precisely computers act quickly people according eddie kennedy efficiency monetary savings key manufacturing industry automation provides means accomplish tasks efficiently costeffectively furthermore automation generally reliable repeatable work conducted manually humans means produces higherquality standards tighter tolerances dan sheridan discussed automation deployed uk nuclear industry rule thumb dictates plant required respond given situation less minutes response must automated matt toia s experience aviation industry applies automation selectively example operational failover performed automatically comes certain tasks industry trusts automation s verified human industry employs good deal automatic monitoring actual airtrafficcontrolsystem implementations must manually inspected humans according erik gross automation quite effective reducing user error laser eye surgery lasik surgery performed doctor measures patient using refractive eye test originally doctor would type numbers press button laser would go work correcting patient s vision however data entry errors could big issue process also entailed possibility mixing patient data jumbling numbers left right eye automation greatly lessens chance humans make mistake impacts someone s vision computerized sanity check manually entered data first major automated improvement human operator inputs measurements outside expected range automation promptly prominently flags case unusual automated improvements followed development iris photographed preliminary refractive eye test s time perform surgery iris patient automatically matched iris photo thus eliminating possibility mixing patient data automated solution implemented entire class medical errors disappeared structured rational decision making google general site reliability engineering particular data critical team aspires make decisions structured rational way ensuring basis decision agreed upon advance rather justified ex post facto inputs decision clear assumptions explicitly stated datadriven decisions win decisions based feelings hunches opinion senior employee room google sre operates baseline assumption everyone team best interests service s users heart figure proceed based data available decisions informed rather prescriptive made without deference personal opinionseven mostsenior person room eric schmidt jonathan rosenberg dub hippo highestpaid person s opinion sch decision making different industries varies widely learned industries use approach ai nt broke nt fix itever industries featuring systems whose design entailed much thought effort often characterized reluctance change underlying technology example telecom industry still uses longdistance switches implemented s rely technology developed decades ago switches pretty much bulletproof massively redundant according gus hartmann reported dan sheridan nuclear industry similarly slow change decisions underpinned thought works nt change many industries heavily focus playbooks procedures rather openended problem solving every humanly conceivable scenario captured checklist binder something goes wrong resource authoritative source react prescriptive approach works industries evolve develop relatively slowly scenarios could go wrong constantly evolving due system updates changes approach also common industries skill level workers may limited best way make sure people respond appropriately emergency provide simple clear set instructions industries also take clear datadriven approach decision making eddie kennedy s experience research manufacturing environments characterized rigorous experimentation culture relies heavily formulating testing hypotheses industries regularly conduct controlled experiments make sure given change yields expected result statistically significant level nothing unexpected occurs changes implemented data yielded experiment supports decision finally industries like proprietary trading divide decision making better manage risk according john li industry features enforcement team separate traders ensure undue risks nt taken pursuit achieving profit enforcement team responsible monitoring events floor halting trading events spin hand system abnormality occurs enforcement team s first response shut system put john li nt trading nt losing money nt making money either least nt losing money enforcement team bring system back despite excruciating delay might seem traders missing potentially profitable opportunity conclusions many principles core site reliability engineering google evident across wide range industries lessons already learned wellestablished industries likely inspired practices use google today main takeaway crossindustry survey many parts software business google higher appetite velocity players industries ability move change quickly must weighed differing implications failure nuclear aviation medical industries example people could injured even die event outage failure stakes high conservative approach achieving high reliability warranted google constantly walk tightrope user expectations high reliability versus lasersharp focus rapid change innovation google incredibly serious reliability must adapt approaches high rate change discussed earlier chapters many software businesses search make conscious decisions reliable reliable enough really google flexibility software products services operate environment lives directly risk something goes wrong therefore re able use tools error budgets motivation error budgets means fund culture innovation calculated risk taking essence google adapted known reliability principles many cases developed honed industries create unique reliability culture one addresses complicated equation balances scale complexity velocity high reliability e enhanced emergency response line us leverages location data electrocardiogram readings https enwikipediaorgwikielectrocardiography https enwikipediaorgwikisafetyintegritylevel https enwikipediaorgwikicorrectiveandpreventiveaction https enwikipediaorgwikicompetentauthority http ehstodaycomsafetynsconeillexemplifiessafetyleadership see facts section b discussion knight power peg software sec regulators blame computer algorithm stock market flash crash computerworld http wwwcomputerworldcomarticlefinancialitregulatorsblamecomputeralgorithmforstockmarketflashcrashhtml chapter conclusion written benjamin lutchedited betsy beyer read book enormous pride time began working excite early s group sort neanderthal sre group dubbed software operations spent career fumbling process building systems light experiences years tech industry amazing see idea sre took root google evolved quickly sre grown hundred engineers joined google people today spread dozen sites running think interesting computing infrastructure planet enabled sre organization google evolve past decade maintain massive infrastructure intelligent efficient scalable way think key overwhelming success sre nature principles operates sre teams constructed engineers divide time two equally important types work sres staff oncall shifts entail putting hands around systems observing systems break understanding challenges best scale also time reflect decide build order make systems easier manage essence pleasure playing roles pilot engineerdesigner experiences running massive computing infrastructure codified actual code packaged discrete product solutions easily usable sre teams ultimately anyone google even outside googlethink google cloud wants use improve upon experience accumulated systems built approach building team system ideally foundation set rules axioms general enough immediately useful remain relevant future much ben treynor sloss outlined book introduction represents flexible mostly futureproof set responsibilities remain spoton years conceived despite changes growth google infrastructure sre team undergone sre grown noticed couple different dynamics play first consistent nature sre primary responsibilities concerns time systems might times larger faster ultimately still need remain reliable flexible easy manage emergency well monitored capacity planned time typical activities undertaken sre evolve necessity google services sre competencies mature example goal build dashboard machines might instead automate discovery dashboard building alerting fleet tens thousands machines trenches sre past decade analogy sre thinks complex systems aircraft industry approached plane flight useful conceptualizing sre evolved matured time stakes failure two industries different certain core similarities hold true imagine wanted fly two cities hundred years ago airplane probably single engine two lucky bags cargo pilot pilot also filled role mechanic possibly additionally acted cargo loader unloader cockpit room pilot lucky copilotnavigator little plane would bounce runway good weather everything went well slowly climb way skies eventually touch another city maybe hundred miles away failure plane systems catastrophic unheard pilot climb cockpit perform repairs inflight systems fed cockpit essential simple fragile likely redundant fastforward hundred years huge sitting tarmac hundreds passengers loading floors tons cargo simultaneously loaded hold plane chockfull reliable redundant systems model safety reliability fact actually safer air ground car plane take dotted line runway one continent land easily dotted line another runway miles away right schedulewithin minutes forecasted landing time take look cockpit find two pilots every element flight experiencesafety capacity speed reliabilityscaled beautifully still two pilots answer question great parallel google approaches enormous fantastically complex systems sre runs interfaces plane operating systems well thought approachable enough learning pilot normal conditions insurmountable task yet interfaces also provide enough flexibility people operating sufficiently trained responses emergencies robust quick cockpit designed people understand complex systems present humans way consumable scalable systems underlying cockpit properties discussed book availability performance optimization change management monitoring alerting capacity planning emergency response ultimately sre goal follow similar course sre team compact possible operate high level abstraction relying upon lots backup systems failsafes thoughtful apis communicate systems time sre team also comprehensive knowledge systemshow operate fail respond failuresthat comes operating daytoday vice president site reliability engineering google inc appendix availability table availability generally calculated based long service unavailable period assuming planned downtime table indicates much downtime permitted reach given availability level table availability table availability level allowed unavailability window per year per quarter per month per week per day per hour days days days hours hours minutes days days days hours hours minutes days hours hours hours minutes seconds days hours hours minutes minutes seconds hours hours minutes minutes minutes seconds hours hours minutes minutes seconds seconds minutes minutes minutes seconds seconds seconds minutes minutes seconds seconds seconds seconds using aggregate unavailability metric ie x operations failed useful focusing outage lengths services may partially availablefor instance due multiple replicas unavailableand services whose load varies course day week rather remaining constant see equations timebased availability aggregate availability embracing risk calculations appendix b collection best practices production services written ben treynor slossedited betsy beyer fail sanely sanitize validate configuration inputs respond implausible inputs continuing operate previous state alerting receipt bad input bad input often falls one categories incorrect data validate syntax possible semantics watch empty data partial truncated data eg alert configuration n smaller previous version delayed data may invalidate current data due timeouts alert well data expected expire fail way preserves function possibly expense overly permissive overly simplistic found generally safer systems continue functioning previous configuration await human approval using new perhaps invalid data examples google global dns load latencybalancing system received empty dns entry file result file permissions accepted empty file served nxdomain six minutes google properties response system performs number sanity checks new configurations including confirming presence virtual ips googlecom continue serving previous dns entries receives new file passes input checks incorrect valid data led google marking entire web containing malware may configuration file containing list suspect urls replaced single forward slash character matched urls checks dramatic changes file size checks see whether configuration matching sites believed unlikely contain malware would prevented reaching production progressive rollouts nonemergency rollouts must proceed stages configuration binary changes introduce risk mitigate risk applying change small fractions traffic capacity one time size service rollout well risk profile inform percentages production capacity rollout pushed appropriate time frame stages also good idea perform different stages different geographies order detect problems related diurnal traffic cycles geographical traffic mix differences rollouts supervised ensure nothing unexpected occurring rollout must monitored either engineer performing rollout stage orpreferablya demonstrably reliable monitoring system unexpected behavior detected roll back first diagnose afterward order minimize mean time recovery define slos like user measure availability performance terms matter end user see service level objectives discussion example measuring error rates latency gmail client rather server resulted substantial reduction assessment gmail availability prompted changes gmail client server code result gmail went available available years error budgets error budgets balance reliability pace innovation error budgets see motivation error budgets define acceptable level failure service period often use month budget simply minus service slo instance service availability target budget unavailability long service spent error budget month background rate errors plus downtime development team free within reason launch new features updates error budget spent service freezes changes except urgent security bug fixes addressing cause increased errors either service earned back room budget month resets mature services slo greater quarterly rather monthly budget reset appropriate amount allowable downtime small error budgets eliminate structural tension might otherwise develop sre product development teams giving common datadriven mechanism assessing launch risk also give sre product development teams common goal developing practices technology allow faster innovation launches without blowing budget monitoring monitoring may three output types pages human must something tickets human must something within days logging one need look output immediately available later analysis needed important enough bother human either require immediate action ie page treated bug entered bugtracking system putting alerts email hoping someone read notice important ones moral equivalent piping devnull eventually ignored history demonstrates strategy attractive nuisance work relies eternal human vigilance inevitable outage thus severe happens postmortems postmortems see postmortem culture learning failure blameless focus process technology people assume people involved incident intelligent well intentioned making best choices could given information available time follows fix people must instead fix environment eg improving system design avoid entire classes problems making appropriate information easily available automatically validating operational decisions make difficult put systems dangerous states capacity planning provision handle simultaneous planned unplanned outage without making user experience unacceptable results n configuration peak traffic handled n instances possibly degraded mode largest instances unavailable validate prior demand forecasts reality consistently match divergence implies unstable forecasting inefficient provisioning risk capacity shortfall use load testing rather tradition establish resourcetocapacity ratio cluster x machines could handle queries per second three months ago still given changes system mistake dayone load steadystate load launches often attract traffic also time especially want put product best foot forward see reliable product launches scale launch coordination checklist overloads failure services produce reasonable suboptimal results overloaded example google search search smaller fraction index stop serving features like instant continue provide good quality web search results overloaded search sre tests web search clusters beyond rated capacity ensure perform acceptably overloaded traffic times load high enough even degraded responses expensive queries practice graceful load shedding using wellbehaved queuing dynamic timeouts see handling overload techniques include answering requests significant delay tarpitting choosing consistent subset clients receive errors preserving good user experience remainder retries amplify low error rates higher levels traffic leading cascading failures see addressing cascading failures respond cascading failures dropping fraction traffic including retries upstream system total load exceeds total capacity every client makes rpc must implement exponential backoff jitter retries dampen error amplification mobile clients especially troublesome may millions updating code fix behavior takes significant amount timepossibly weeksand requires users install updates sre teams sre teams spend time operational work see eliminating toil operational overflow directed product development team many services also include product developers oncall rotation ticket handling even currently overflow provides incentives design systems minimize eliminate operational toil along ensuring product developers touch operational side service regular production meeting sres development team see communication collaboration sre also helpful found least eight people need part oncall team order avoid fatigue allow sustainable staffing low turnover preferably oncall two wellseparated geographic locations eg california ireland provide better quality life avoiding nighttime pages case six people site minimum team size expect handle two events per oncall shift eg per hours takes time respond fix outages start postmortem file resulting bugs frequent events may degrade quality response suggest something wrong least one system design monitoring sensitivity response postmortem bugs ironically implement best practices sre team may eventually end practice responding incidents due infrequency making long outage short one practice handling hypothetical outages see disaster role playing routinely improve incidenthandling documentation process appendix c example incident state document shakespeare sonnet overload incident management info http incidentmanagementcheatsheet communications lead keep summary updated summary shakespeare search service cascading failure due newly discovered sonnet search index status active incident command post shakespeare irc command hierarchy responders current incident commander jennifer operations lead docbrown planning lead jennifer communications lead jennifer next incident commander determined update least every four hours handoff comms lead role detailed status last updated utc jennifer exit criteria new sonnet added shakespeare search corpus todo within availability latency ile ms slos minutes todo todo list bugs filed run mapreduce job reindex shakespeare corpus done borrow emergency resources bring extra capacity done enable flux capacitor balance load clusters bug todo incident timeline recent first times utc utc jennifer increasing serving capacity globally x utc jennifer directing traffic usa sacrificial cluster draining traffic clusters recover cascading failure spinning tasks mapreduce index job complete awaiting bigtable replication clusters utc martym adding new sonnet shakespeare corpus starting index mapreduce utc martym obtains text newly discovered sonnet shakespearediscuss mailing list utc docbrown incident declared due cascading failure utc docbrown pager storm manyhttps clusters appendix example postmortem shakespeare sonnet postmortem incident date authors jennifer martym agoogler status complete action items progress summary shakespeare search minutes period high interest shakespeare due discovery new sonnet impact estimated b queries lost revenue impact root causes cascading failure due combination exceptionally high load resource leak searches failed due terms shakespeare corpus newly discovered sonnet used word never appeared one shakespeare works happened term users searched normal circumstances rate task failures due resource leaks low enough unnoticed trigger latent bug triggered sudden increase traffic resolution directed traffic sacrificial cluster added x capacity mitigate cascading failure updated index deployed resolving interaction latent bug maintaining extra capacity surge public interest new sonnet passes resource leak identified fix deployed detection borgmon detected high level http s paged oncall action items action item update playbook instructions responding type mitigate owner jennifer bug na done mitigate jennifer na done use flux capacitor balance load prevent clusters martym bug todo schedule cascading failure test process next dirt docbrown na todo investigate running index mrfusion prevent continuously jennifer bug todo plug file descriptor leak search prevent ranking subsystem agoogler bug done add load shedding capabilities prevent shakespeare search agoogler bug todo build regression tests ensure servers respond prevent sanely queries death clarac bug todo deploy updated search ranking prevent subsystem prod jennifer na done responding cascading failure freeze production due error budget exhaustion seek exception due grotesque unbelievable bizarre unprecedented circumstances docbrown na todo lessons learned went well monitoring quickly alerted us high rate reaching http s rapidly distributed updated shakespeare corpus clusters went wrong practice responding cascading failure exceeded availability error budget several orders magnitude due exceptional surge traffic essentially resulted failures got lucky mailing list shakespeare aficionados copy new sonnet available server logs stack traces pointing file descriptor exhaustion cause crash queryofdeath resolved pushing new index containing popular search term timeline times utc news reports new shakespearean sonnet discovered delorean glove compartment traffic shakespeare search increases x post rshakespeare points shakespeare search engine place find new sonnet except sonnet yet outage begins search backends start melting load docbrown receives pager storm manyhttps clusters traffic shakespeare search failing see http monitor docbrown starts investigating finds backend crash rate high incident begins docbrown declares incident due cascading failure coordination shakespeare names jennifer incident commander someone coincidentally sends email shakespearediscuss sonnet discovery happens top martym inbox jennifer notifies shakespeareincidents list incident martym tracks text new sonnet looks documentation corpus update docbrown finds crash symptoms identical across tasks clusters investigating cause based application logs martym finds documentation starts prep work corpus update martym adds sonnet shakespeare known works starts indexing job docbrown contacts clarac agoogler shakespeare dev team help examining codebase possible causes clarac finds smoking gun logs pointing file descriptor exhaustion confirms code leak exists term corpus searched martym index mapreduce job completes jennifer docbrown decide increase instance count enough drop load instances able appreciable work dying restarted docbrown load balances traffic usa cluster permitting instance count increase clusters without servers failing immediately martym starts replicating new index clusters docbrown starts x instance count increase jennifer changes load balancing increase traffic nonsacrificial clusters tasks nonsacrificial clusters start failing symptoms found orderofmagnitude error whiteboard calculations instance count increase jennifer reverts load balancing resacrifice usa cluster preparation additional global x instance count increase total x initial capacity outage mitigated updated index replicated clusters docbrown starts second wave instance count increase x initial capacity jennifer reinstates load balancing across clusters traffic nonsacrificial clusters http rates nominal rates task failures intermittent low levels jennifer balances traffic across nonsacrificial clusters nonsacrificial clusters http rates remain within slo task failures observed traffic balanced across nonsacrificial clusters traffic balanced across nonsacrificial clusters outage ends traffic balanced across clusters incident ends reached exit criterion minutes nominal performance supporting information monitoring dashboard http monitorshakespeare endtimet duration impact effect users revenue etc an explanation circumstances incident happened often helpful use technique whys ohn understand contributing factors kneejerk ais often turn extreme costly implement judgment may needed rescope larger context risk overoptimizing particular issue adding specific monitoringalerting reliable mechanisms like unit tests catch problems much earlier development process this section really near misses eg goat teleporter available emergency use animals despite lack certification a screenplay incident use incident timeline incident management document start filling postmortem timeline supplement relevant entries useful information links logs screenshots graphs irc logs im logs etc appendix e launch coordination checklist google original launch coordination checklist circa slightly abridged brevity architecture architecture sketch types servers types requests clients programmatic client requests machines datacenters machines bandwidth datacenters n redundancy network qos new domain names dns load balancing volume estimates capacity performance http traffic bandwidth estimates launch spike traffic mix months load test endtoend test capacity per datacenter max latency impact services care storage capacity system reliability failover happens machine dies rack fails cluster goes offline network fails two datacenters type server talks servers backends detect backends die die terminate restart without affecting clients users load balancing ratelimiting timeout retry error handling behavior data backuprestore disaster recovery monitoring server management monitoring internal state monitoring endtoend behavior managing alerts monitoring monitoring financially important alerts logs tips running servers within cluster environment crash mail servers sending email alerts server code security security design review security code audit spam risk authentication ssl prelaunch visibilityaccess control various types blacklists automation manual tasks methods change control update servers data configs release process repeatable builds canaries live traffic staged rollouts growth issues spare capacity x growth growth alerts scalability bottlenecks linear scaling scaling hardware changes needed caching data shardingresharding external dependencies thirdparty systems monitoring networking traffic volume launch spikes graceful degradation avoid accidentally overrunning thirdparty services playing nice syndicated partners mail systems services within google schedule rollout planning hard deadlines external events mondays fridays standard operating procedures service services appendix f example production meeting minutes date attendees agoogler clarac docbrown jennifer martym announcements major outage blew error budget previous action item review certify goat teleporter use cattle bug nonlinearities mass acceleration predictable able target accurately days outage review new sonnet outage b queries lost due cascading failure interaction latent bug leaked file descriptor searches results new sonnet corpus unprecedented unexpected traffic volume file descriptor leak bug fixed bug deployed prod looking using flux capacitor load balancing bug using load shedding bug prevent recurrence annihilated availability error budget pushes prod frozen month unless docbrown obtain exception grounds event bizarre unforeseeable consensus exception unlikely paging events annotationconsistencytooeventual paged times week likely due crossregional replication delay bigtables investigation still ongoing see bug fix expected soon raise acceptable consistency threshold reduce unactionable alerts nonpaging events none monitoring changes andor silences annotationconsistencytooeventual acceptable delay threshold raised s s see bug todo martym planned production changes usa cluster going offline maintenance response required traffic automatically route clusters region resources borrowed resources respond sonnet incident spin additional server instances return resources next week utilization cpu ram disk last week key service metrics ok ile latency ms ms slo target trailing days bad availability slo target trailing days discussion project updates project molière launching two weeks new action items todo martym raise annotationconsistencytooeventual threshold todo docbrown return instance count normal return resources bibliography ada bram adams stephany bellomo christian bird tamara marshallkeim foutse khomh kim moir practice future release engineering roundtable three release engineers ieee software vol marchapril pp agu m k aguilera stumbling consensus research misunderstandings issues replication lecture notes computer science all j allspaw j robbins web operations keeping data time reilly all j allspaw blameless postmortems culture blog post all j allspaw tradeoffs pressure heuristics observations teams resolving internet service outages msc thesis lund university ana s anantharaju automating web application security testing blog post july ana r ananatharayan et al photon faulttolerant scalable joining continuous data streams sigmod and a andrieux k czajkowski a dan et al web services agreement specification wsagreement september bai p bailis a ghodsi eventual consistency today limitations extensions beyond acm queue vol bai l bainbridge ironies automation automatica vol november bak j baker et al megastore providing scalable highly available storage interactive services proceedings conference innovative data system research bar l a barroso warehousescale computing entering teenage decade talk th annual symposium computer architecture video available online bar l a barroso j clidaras u hölzle datacenter computer introduction design warehousescale machines second edition morgan claypool ben c bennett a tseitlin chaos monkey released wild blog post july bla m bland goto fail heartbleed unit testing culture blog post june boc l bock work rules twelve books bol w j bolosky d bradshaw r b haagens n p kusters p li paxos replicated state machines basis highperformance data store proc nsdi boy p g boysen culture foundation balanced accountability patient safety ochsner journal fall bra vm brasseur failure happens benefit yapc bre e brewer lessons giantscale services ieee internet computing vol july august bre e brewer cap twelve years later rules changed computer vol february bro m brooker exponential backoff jitter aws architecture blog march bro f p brooks jr silver bulletessence accidents software engineering mythical manmonth boston addisonwesley pp bru j brutlag speed matters google research blog june bul g m bull dartmouth timesharing system ellis horwood bur m burgess principles network system administration wiley bur m burrows chubby lock service looselycoupled distributed systems osdi seventh symposium operating system design implementation november bur b burns b grant d oppenheimer e brewer j wilkes borg omega kubernetes acm queue vol cas m castro b liskov practical byzantine fault tolerance proc osdi cha c chambers a raniwala f perry s adams r henry r bradshaw n weizenbaum flumejava easy efficient dataparallel pipelines acm sigplan conference programming language design implementation cha t d chandra s toueg unreliable failure detectors reliable distributed systems j acm cha t chandra r griesemer j redstone paxos made live engineering perspective podc th acm symposium principles distributed computing cha f chang et al bigtable distributed storage system structured data osdi seventh symposium operating system design implementation november chr g p chrousous stress disorders stress system nature reviews endocrinology vol clos c clos study nonblocking switching networks bell system technical journal vol con c contavalli w van der gaast d lawrence w kumari client subnet dns queries ietf internetdraft con m e conway design separable transitiondiagram compiler commun acm july con p conway preservation digital world report published council library information resources coo r i cook complex systems fail web operations reilly cor j c corbett et al spanner google globallydistributed database osdi tenth symposium operating system design implementation october cra j cranmer visualizing code coverage blog post march dea j dean l a barroso tail scale communications acm vol dea j dean s ghemawat mapreduce simplified data processing large clusters osdi sixth symposium operating system design implementation december dea j dean software engineering advice building largescale distributed systems stanford cs class lecture spring dek s dekker reconstructing human contributions accidents new view error performance journal safety research vol dek s dekker field guide understanding human error rd edition ashgate dic c dickson embracing continuous release reduced change complexity presentation usenix release engineering summit west video available online dur j durmer d dinges neurocognitive consequences sleep deprivation seminars neurology vol eis d e eisenbud et al maglev fast reliable software network load balancer nsdi th usenix symposium networked systems design implementation march ere j r erenkrantz release management within open source projects proceedings rd workshop open source software engineering portland oregon may fis m j fischer n a lynch m s paterson impossibility distributed consensus one faulty process j acm fit b w fitzpatrick b collinssussman team geek software developer guide working well others reilly flo s floyd v jacobson synchronization periodic routing messages ieeeacm transactions networking vol issue april pp for d ford et al availability globally distributed storage systems proceedings th usenix symposium operating systems design implementation fox fox e brewer harvest yield scalable tolerant systems proceedings th workshop hot topics operating systems rio rico arizona march fow m fowler gui architectures blog post gal j gall systemantics systems really work fail st ed pocket gal j gall systems bible beginner guide systems large small rd ed general systemantics pressliberty gaw a gawande checklist manifesto get things right henry holt company ghe s ghemawat h gobioff st leung google file system th acm symposium operating systems principles october gil s gilbert n lynch brewer conjecture feasibility consistent available partitiontolerant web services acm sigact news vol gla r glass facts fallacies software engineering addisonwesley professional gol w golab et al eventually consistent expecting acm queue vol gra p graham maker schedule manager schedule blog post july gup a gupta j shute highavailability massive scale building google data infrastructure ads workshop business intelligence real time enterprise ham j hamilton designing deploying internetscale services proceedings st large installation system administration conference november han s hanks t li d farinacci p traina generic routing encapsulation ipv networks ietf informational rfc hic m hickins tape rescues google lost email scare digits wall street journal march hixa d hixson capacity planning login vol february hixb d hixson systems engineering side site reliability engineering login vol june hod j hodges notes distributed systems young bloods blog post january hol l holmwood applying cardiac alarm management techniques oncall blog post august hum j humble c read d north deployment production line proceedings ieee agile conference july hum j humble d farley continuous delivery reliable software releases build test deployment automation addisonwesley hun p hunt m konar f p junqueira b reed zookeeper waitfree coordination internetscale systems usenix atc iaea international atomic energy agency safety nuclear power plants design ssr jai s jain et al b experience globallydeployed software defined wan sigcomm jon c jones t underwood s nukala hiring site reliability engineers login vol june jun f junqueira y mao k marzullo classic paxos vs fast paxos caveat emptor proc hotdep jun f p junqueira b c reid m serafini zab highperformance broadcast primarybackup systems dependable systems networks dsn ieeeifip st international conference jun kah d kahneman thinking fast slow farrar straus giroux kar d karger et al consistent hashing random trees distributed caching protocols relieving hot spots world wide web proc stoc th annual acm symposium theory computing kem c kemper build cloud build system works google engineering tools blog post august ken s kendrick takes us login vol october kinc kincaid jason tmobile sidekick disaster danger servers crashed backup techcrunch np oct web jan http techcrunchcomtmobilesidekickdisastermicrosoftsserverscrashedandtheydonthaveabackup kin k kingsbury trouble timestamps blog post kir j kirsch y amir paxos system builders overview proc ladis kla r klau google sets goals okrs blog post october kle d v klein forensic analysis distributed twostage webbased spam attack proceedings th large installation system administration conference december kle d v klein d m betser m g monroe making push green reality login vol october kra t krattenmaker make every meeting matter harvard business review february kre j kreps getting real distributed system reliability blog post march kri k krishan weathering unexpected communications acm vol november kum a kumar et al bwe flexible hierarchical bandwidth allocation wan distributed computing sigcomm lam l lamport parttime parliament acm transactions computer systems may lam l lamport paxos made simple acm sigact news december lam l lamport fast paxos distributed computing october lim t a limoncelli s r chalup c j hogan practice cloud system administration designing operating large distributed systems volume addisonwesley loo j loomis make failure beautiful art science postmortems web operations reilly lu h lu et al existential consistency measuring understanding consistency facebook sosp mao y mao f p junqueira k marzullo mencius building efficient replicated state machines wans osdi mas a h maslow theory human motivation psychological review mau b maurer fail scale acm queue vol may m mayer site may harm computer every search result blog post january mci m d mcilroy research unix reader annotated excerpts programmer manual mcn d mcnutt maintaining consistency massively parallel environment presentation usenix configuration management summit video available online mcna d mcnutt accelerating path dev devops login vol april mcnb d mcnutt commandments release engineering presentation nd international workshop release engineering april mcnc d mcnutt distributing software massively parallel environment presentation usenix lisa video available online mic microsoft technet snmp last modified march https technetmicrosoftcomenuslibrarycc vws aspx mea d meadows thinking systems chelsea green men p menage adding generic process containers linux kernel proc ottawa linux symposium mer n merchant culture trumps strategy every time harvard business review march moc p mockapetris domain names implementation specification ietf internet standard mol c moler matrix computation distributed memory multiprocessors hypercube multiprocessors mora i moraru d g andersen m kaminsky egalitarian paxos carnegie mellon university parallel data lab technical report cmupdl mor i moraru d g andersen m kaminsky paxos quorum leases fast reads without sacrificing writes proc socc morb j d morgenthaler m gridnev r sauciuc s bhansali searching build debt experiences managing technical debt google proceedings rd int l workshop managing technical debt nar c narla d salas hermetic servers blog post nel b nelson data diversity communications acm vol nic k nichols v jacobson controlling queue delay acm queue vol oco p connor a kleyner practical reliability engineering th edition wiley ohn t ohno toyota production system beyond largescale production productivity press ong d ongaro j ousterhout search understandable consensus algorithm extended version pen d peng f dabek largescale incremental processing using distributed transactions notifications proc th usenix symposium operating system design implementation november per c perrow normal accidents living highrisk technologies princeton university press per a r perry engineering reliability web sites google sre proc linuxworld pik r pike s dorward r griesemer s quinlan interpreting data parallel analysis sawzall scientific programming journal vol pot r potvin j levenberg motivation monolithic codebase google stores billions lines code single repository communications acm vol video available youtube roo j j rooney l n vanden heuvel root cause analysis beginners quality progress july sai a de saint exupéry terre des hommes paris le livre de poche translation lewis galantière wind sand stars sam r r sambasivan r fonseca i shafer g r ganger want trace distributed system key design insights years practical experience carnegie mellon university parallel data lab technical report cmupdl san n santos a schiper tuning paxos highthroughput batching pipelining th int l conf distributed computing networking sar n b sarter d d woods c e billings automation surprises handbook human factors ergonomics nd edition g salvendy ed wiley sch e schmidt j rosenberg a eagle google works grand central publishing sch b schwartz factors impact availability visualized blog post december sch f b schneider implementing faulttolerant services using state machine approach tutorial acm computing surveys vol sec securities exchange commission order matter knight capital americas llc file sha g shao f berman r wolski masterslave computing grid heterogeneous computing workshop shu j shute et al f distributed sql database scales proc vldb sig b h sigelman et al dapper largescale distributed systems tracing infrastructure google technical report sin a singh et al jupiter rising decade clos topologies centralized control google datacenter network sigcomm skel m skelton operability improve developers write draft run book blog post october slo b treynor sloss gmail back soon everyone blog post february tat s tatham report bugs effectively ver a verma l pedrosa m r korupolu d oppenheimer e tune j wilkes largescale cluster management google borg proceedings european conference computer systems wal d r wallace r u fujii software verification validation overview ieee software vol may pp war r ward b beyer beyondcorp new approach enterprise security login vol december whi j a whittaker j arbon j carollo google tests software addisonwesley woo wood predicting software reliability computer vol wria h k wright release engineering processes faults failures section phd thesis university texas austin wrib h k wright d e perry release engineering practices pitfalls proceedings th international conference software engineering icse ieee pp wri h k wright d jasper m klimek c carruth z wan largescale automated refactoring using clangmr proceedings th international conference software maintenance icsm ieee pp yor n york build cloud accessing source code google engineering tools blog post june zoo zookeeper project apache foundation zookeeper recipes solutions zookeeper documentation